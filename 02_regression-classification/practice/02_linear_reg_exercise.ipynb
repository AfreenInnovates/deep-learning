{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1\n",
        "\n",
        "Questions taken from: https://www.learnpytorch.io/01_pytorch_workflow/#exercises"
      ],
      "metadata": {
        "id": "F85PH7TfQq1q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data preparation\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4T0-HDfMSFU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "jHNkQR7FRflN"
      },
      "execution_count": 3306,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3307,
      "metadata": {
        "id": "XKPIVGuqQo0v"
      },
      "outputs": [],
      "source": [
        "weight = 0.3\n",
        "bias = 0.9"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.arange(1, 101, dtype=torch.float32).unsqueeze(dim=1)"
      ],
      "metadata": {
        "id": "3QzJ1rpCReav"
      },
      "execution_count": 3308,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhPvGAqiRjvj",
        "outputId": "d5b168af-0b91-4357-c73c-f753f04f30b4"
      },
      "execution_count": 3309,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  1.],\n",
              "        [  2.],\n",
              "        [  3.],\n",
              "        [  4.],\n",
              "        [  5.],\n",
              "        [  6.],\n",
              "        [  7.],\n",
              "        [  8.],\n",
              "        [  9.],\n",
              "        [ 10.],\n",
              "        [ 11.],\n",
              "        [ 12.],\n",
              "        [ 13.],\n",
              "        [ 14.],\n",
              "        [ 15.],\n",
              "        [ 16.],\n",
              "        [ 17.],\n",
              "        [ 18.],\n",
              "        [ 19.],\n",
              "        [ 20.],\n",
              "        [ 21.],\n",
              "        [ 22.],\n",
              "        [ 23.],\n",
              "        [ 24.],\n",
              "        [ 25.],\n",
              "        [ 26.],\n",
              "        [ 27.],\n",
              "        [ 28.],\n",
              "        [ 29.],\n",
              "        [ 30.],\n",
              "        [ 31.],\n",
              "        [ 32.],\n",
              "        [ 33.],\n",
              "        [ 34.],\n",
              "        [ 35.],\n",
              "        [ 36.],\n",
              "        [ 37.],\n",
              "        [ 38.],\n",
              "        [ 39.],\n",
              "        [ 40.],\n",
              "        [ 41.],\n",
              "        [ 42.],\n",
              "        [ 43.],\n",
              "        [ 44.],\n",
              "        [ 45.],\n",
              "        [ 46.],\n",
              "        [ 47.],\n",
              "        [ 48.],\n",
              "        [ 49.],\n",
              "        [ 50.],\n",
              "        [ 51.],\n",
              "        [ 52.],\n",
              "        [ 53.],\n",
              "        [ 54.],\n",
              "        [ 55.],\n",
              "        [ 56.],\n",
              "        [ 57.],\n",
              "        [ 58.],\n",
              "        [ 59.],\n",
              "        [ 60.],\n",
              "        [ 61.],\n",
              "        [ 62.],\n",
              "        [ 63.],\n",
              "        [ 64.],\n",
              "        [ 65.],\n",
              "        [ 66.],\n",
              "        [ 67.],\n",
              "        [ 68.],\n",
              "        [ 69.],\n",
              "        [ 70.],\n",
              "        [ 71.],\n",
              "        [ 72.],\n",
              "        [ 73.],\n",
              "        [ 74.],\n",
              "        [ 75.],\n",
              "        [ 76.],\n",
              "        [ 77.],\n",
              "        [ 78.],\n",
              "        [ 79.],\n",
              "        [ 80.],\n",
              "        [ 81.],\n",
              "        [ 82.],\n",
              "        [ 83.],\n",
              "        [ 84.],\n",
              "        [ 85.],\n",
              "        [ 86.],\n",
              "        [ 87.],\n",
              "        [ 88.],\n",
              "        [ 89.],\n",
              "        [ 90.],\n",
              "        [ 91.],\n",
              "        [ 92.],\n",
              "        [ 93.],\n",
              "        [ 94.],\n",
              "        [ 95.],\n",
              "        [ 96.],\n",
              "        [ 97.],\n",
              "        [ 98.],\n",
              "        [ 99.],\n",
              "        [100.]])"
            ]
          },
          "metadata": {},
          "execution_count": 3309
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = (X * weight) + bias"
      ],
      "metadata": {
        "id": "5Aj_wLXSRj4r"
      },
      "execution_count": 3310,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klRymAwfRnV7",
        "outputId": "046e057b-9ef1-45c9-efb0-9ac5f47c8491"
      },
      "execution_count": 3311,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.2000],\n",
              "        [ 1.5000],\n",
              "        [ 1.8000],\n",
              "        [ 2.1000],\n",
              "        [ 2.4000],\n",
              "        [ 2.7000],\n",
              "        [ 3.0000],\n",
              "        [ 3.3000],\n",
              "        [ 3.6000],\n",
              "        [ 3.9000],\n",
              "        [ 4.2000],\n",
              "        [ 4.5000],\n",
              "        [ 4.8000],\n",
              "        [ 5.1000],\n",
              "        [ 5.4000],\n",
              "        [ 5.7000],\n",
              "        [ 6.0000],\n",
              "        [ 6.3000],\n",
              "        [ 6.6000],\n",
              "        [ 6.9000],\n",
              "        [ 7.2000],\n",
              "        [ 7.5000],\n",
              "        [ 7.8000],\n",
              "        [ 8.1000],\n",
              "        [ 8.4000],\n",
              "        [ 8.7000],\n",
              "        [ 9.0000],\n",
              "        [ 9.3000],\n",
              "        [ 9.6000],\n",
              "        [ 9.9000],\n",
              "        [10.2000],\n",
              "        [10.5000],\n",
              "        [10.8000],\n",
              "        [11.1000],\n",
              "        [11.4000],\n",
              "        [11.7000],\n",
              "        [12.0000],\n",
              "        [12.3000],\n",
              "        [12.6000],\n",
              "        [12.9000],\n",
              "        [13.2000],\n",
              "        [13.5000],\n",
              "        [13.8000],\n",
              "        [14.1000],\n",
              "        [14.4000],\n",
              "        [14.7000],\n",
              "        [15.0000],\n",
              "        [15.3000],\n",
              "        [15.6000],\n",
              "        [15.9000],\n",
              "        [16.2000],\n",
              "        [16.5000],\n",
              "        [16.8000],\n",
              "        [17.1000],\n",
              "        [17.4000],\n",
              "        [17.7000],\n",
              "        [18.0000],\n",
              "        [18.3000],\n",
              "        [18.6000],\n",
              "        [18.9000],\n",
              "        [19.2000],\n",
              "        [19.5000],\n",
              "        [19.8000],\n",
              "        [20.1000],\n",
              "        [20.4000],\n",
              "        [20.7000],\n",
              "        [21.0000],\n",
              "        [21.3000],\n",
              "        [21.6000],\n",
              "        [21.9000],\n",
              "        [22.2000],\n",
              "        [22.5000],\n",
              "        [22.8000],\n",
              "        [23.1000],\n",
              "        [23.4000],\n",
              "        [23.7000],\n",
              "        [24.0000],\n",
              "        [24.3000],\n",
              "        [24.6000],\n",
              "        [24.9000],\n",
              "        [25.2000],\n",
              "        [25.5000],\n",
              "        [25.8000],\n",
              "        [26.1000],\n",
              "        [26.4000],\n",
              "        [26.7000],\n",
              "        [27.0000],\n",
              "        [27.3000],\n",
              "        [27.6000],\n",
              "        [27.9000],\n",
              "        [28.2000],\n",
              "        [28.5000],\n",
              "        [28.8000],\n",
              "        [29.1000],\n",
              "        [29.4000],\n",
              "        [29.7000],\n",
              "        [30.0000],\n",
              "        [30.3000],\n",
              "        [30.6000],\n",
              "        [30.9000]])"
            ]
          },
          "metadata": {},
          "execution_count": 3311
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_min, X_max = X.min(), X.max()\n",
        "X = X / 100.0"
      ],
      "metadata": {
        "id": "piJBGcNNoTOD"
      },
      "execution_count": 3312,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = (X * weight) + bias"
      ],
      "metadata": {
        "id": "WePEyySeo4MQ"
      },
      "execution_count": 3313,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[0] * weight + bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwT1ZiwvSATj",
        "outputId": "5a54c0a5-90e0-4b7b-cd8a-b53a4317112a"
      },
      "execution_count": 3314,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.9030])"
            ]
          },
          "metadata": {},
          "execution_count": 3314
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "uzYMfOOrRvlh"
      },
      "execution_count": 3315,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(X, y, s = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "id": "XBLuSbz3RyCF",
        "outputId": "b870cbc8-bdad-4819-814d-6b7c56d9e64c"
      },
      "execution_count": 3316,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x781d2765be90>"
            ]
          },
          "metadata": {},
          "execution_count": 3316
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.8 * len(X))\n",
        "train_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1yqGHsIRza2",
        "outputId": "9d28ce56-e211-4ed4-889f-f98b199cf721"
      },
      "execution_count": 3317,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80"
            ]
          },
          "metadata": {},
          "execution_count": 3317
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = X[:train_size], y[:train_size]\n",
        "X_test, y_test = X[train_size:], y[train_size:]"
      ],
      "metadata": {
        "id": "ngccxlY3SOG-"
      },
      "execution_count": 3318,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train), len(X_test), len(y_train), len(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjEGYo9sSVtU",
        "outputId": "d5585f60-2a7b-4e60-d06b-5e9b7b66b51e"
      },
      "execution_count": 3319,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(80, 20, 80, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 3319
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_data(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, y_preds=None):\n",
        "\n",
        "  plt.figure(figsize=(10, 8))\n",
        "\n",
        "  plt.scatter(X_train, y_train, s=5, c=\"b\", label=\"Train data\", )\n",
        "  plt.scatter(X_test, y_test, s=5, c=\"r\", label=\"Test data\")\n",
        "\n",
        "  if y_preds != None:\n",
        "    plt.scatter(X_test, y_preds, s=5, c=\"#000000\", label=\"Predictions on test data\")\n",
        "\n",
        "  plt.legend(prop={\"size\": 16})"
      ],
      "metadata": {
        "id": "jKX3iaJdSZHY"
      },
      "execution_count": 3320,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "MCMa0-HPTAgI",
        "outputId": "552b01d2-245b-4dfd-c97f-fcf411e78673"
      },
      "execution_count": 3321,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Building model"
      ],
      "metadata": {
        "id": "SvymmMI1T5Qo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegressionModel(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.weight = torch.nn.Parameter(torch.randn(1, dtype=torch.float), requires_grad=True)\n",
        "    self.bias = torch.nn.Parameter(torch.randn(1, dtype=torch.float), requires_grad=True)\n",
        "\n",
        "  def forward(self, X : torch.Tensor) -> torch.Tensor:\n",
        "    return (self.weight * X) + self.bias"
      ],
      "metadata": {
        "id": "4l8DO9IST8vi"
      },
      "execution_count": 3322,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearRegressionModel()"
      ],
      "metadata": {
        "id": "BpJ28ElxVmGF"
      },
      "execution_count": 3323,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azYjXnegV1x1",
        "outputId": "a468c924-19ad-450d-eab9-3438ba97c418"
      },
      "execution_count": 3324,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegressionModel()"
            ]
          },
          "metadata": {},
          "execution_count": 3324
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5t6F5C4HV6bq",
        "outputId": "0fdff8a5-5da6-499e-d9e9-ef4011f38cd4"
      },
      "execution_count": 3325,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([0.3367], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([0.1288], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 3325
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTXoNkpwV7qa",
        "outputId": "7a425e8c-fc6e-4bc8-ea8d-95e61350d555"
      },
      "execution_count": 3326,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weight', tensor([0.3367])), ('bias', tensor([0.1288]))])"
            ]
          },
          "metadata": {},
          "execution_count": 3326
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = torch.nn.L1Loss()\n",
        "optimizer = torch.optim.SGD(lr=1e-3, params=model.parameters())"
      ],
      "metadata": {
        "id": "fcZ4o7cDcT7c"
      },
      "execution_count": 3327,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Training model"
      ],
      "metadata": {
        "id": "9JDLDJkMWPCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### steps of model training"
      ],
      "metadata": {
        "id": "dkiG7WyIWi0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds = model(X_test)"
      ],
      "metadata": {
        "id": "iQRrPHr0WkdQ"
      },
      "execution_count": 3328,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sHzpcg0Wobt",
        "outputId": "6649cfd7-a32e-4336-c339-f2a594cc9a4e"
      },
      "execution_count": 3329,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4015],\n",
              "        [0.4049],\n",
              "        [0.4083],\n",
              "        [0.4116],\n",
              "        [0.4150],\n",
              "        [0.4184],\n",
              "        [0.4217],\n",
              "        [0.4251],\n",
              "        [0.4285],\n",
              "        [0.4318],\n",
              "        [0.4352],\n",
              "        [0.4386],\n",
              "        [0.4419],\n",
              "        [0.4453],\n",
              "        [0.4487],\n",
              "        [0.4520],\n",
              "        [0.4554],\n",
              "        [0.4588],\n",
              "        [0.4621],\n",
              "        [0.4655]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 3329
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = loss_fn(y_preds, y_test)"
      ],
      "metadata": {
        "id": "7OfzS-w5WowF"
      },
      "execution_count": 3330,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMeDH4TVWrup",
        "outputId": "45f1ecfe-2de5-4412-8dbb-69646c7786a8"
      },
      "execution_count": 3331,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.7380, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 3331
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMNPToI1Wt7H",
        "outputId": "43884575-c6e7-4205-e91f-57db62396e0b"
      },
      "execution_count": 3332,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGD (\n",
              "Parameter Group 0\n",
              "    dampening: 0\n",
              "    differentiable: False\n",
              "    foreach: None\n",
              "    fused: None\n",
              "    lr: 0.001\n",
              "    maximize: False\n",
              "    momentum: 0\n",
              "    nesterov: False\n",
              "    weight_decay: 0\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3332
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "okRVvNaKWyaR"
      },
      "execution_count": 3333,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRgfmRF2W6Qp",
        "outputId": "ffe79535-fa86-490b-936e-3daa3f1b0f6a"
      },
      "execution_count": 3334,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGD (\n",
              "Parameter Group 0\n",
              "    dampening: 0\n",
              "    differentiable: False\n",
              "    foreach: None\n",
              "    fused: None\n",
              "    lr: 0.001\n",
              "    maximize: False\n",
              "    momentum: 0\n",
              "    nesterov: False\n",
              "    weight_decay: 0\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3334
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "metadata": {
        "id": "T8VkDNZJW6rN"
      },
      "execution_count": 3335,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2NBr0hlW8pE",
        "outputId": "b8597fed-2540-4d10-d85a-305f18f7bf28"
      },
      "execution_count": 3336,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.7380, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 3336
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9RiVP4IW-MU",
        "outputId": "5736693b-997d-4728-dc3c-7a23ecc1127c"
      },
      "execution_count": 3337,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGD (\n",
              "Parameter Group 0\n",
              "    dampening: 0\n",
              "    differentiable: False\n",
              "    foreach: None\n",
              "    fused: None\n",
              "    lr: 0.001\n",
              "    maximize: False\n",
              "    momentum: 0\n",
              "    nesterov: False\n",
              "    weight_decay: 0\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3337
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.step()"
      ],
      "metadata": {
        "id": "xkmd5YIuW_l_"
      },
      "execution_count": 3338,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUcHcTMHXALe",
        "outputId": "41396fc8-4088-48d9-e616-5d949251d185"
      },
      "execution_count": 3339,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGD (\n",
              "Parameter Group 0\n",
              "    dampening: 0\n",
              "    differentiable: False\n",
              "    foreach: None\n",
              "    fused: None\n",
              "    lr: 0.001\n",
              "    maximize: False\n",
              "    momentum: 0\n",
              "    nesterov: False\n",
              "    weight_decay: 0\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3339
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NtCzbN6XAhz",
        "outputId": "396c8c37-b8fe-4ffb-ce55-56437b3d68eb"
      },
      "execution_count": 3340,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegressionModel()"
            ]
          },
          "metadata": {},
          "execution_count": 3340
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds = model(X_test)\n",
        "test_loss = loss_fn(test_preds, y_test)"
      ],
      "metadata": {
        "id": "w1nRVMuXXDII"
      },
      "execution_count": 3341,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds, test_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNj4oa_IXJ2l",
        "outputId": "0adf0952-0865-4115-f1fe-9a522a4d6d56"
      },
      "execution_count": 3342,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.4033],\n",
              "         [0.4066],\n",
              "         [0.4100],\n",
              "         [0.4134],\n",
              "         [0.4168],\n",
              "         [0.4201],\n",
              "         [0.4235],\n",
              "         [0.4269],\n",
              "         [0.4303],\n",
              "         [0.4336],\n",
              "         [0.4370],\n",
              "         [0.4404],\n",
              "         [0.4438],\n",
              "         [0.4471],\n",
              "         [0.4505],\n",
              "         [0.4539],\n",
              "         [0.4573],\n",
              "         [0.4607],\n",
              "         [0.4640],\n",
              "         [0.4674]], grad_fn=<AddBackward0>),\n",
              " tensor(0.7362, grad_fn=<MeanBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 3342
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss.item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htalTUfpXKki",
        "outputId": "acd92274-e3d6-4d46-801d-d917a07a78c4"
      },
      "execution_count": 3343,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7361667156219482"
            ]
          },
          "metadata": {},
          "execution_count": 3343
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 2500\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "train_loss_vals = []\n",
        "test_loss_vals = []\n",
        "epoch_counts = []\n",
        "\n",
        "for i in range(epochs):\n",
        "  model.train()\n",
        "\n",
        "  y_preds = model(X_train)\n",
        "\n",
        "  training_loss = loss_fn(y_preds, y_train)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  training_loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    preds = model(X_test)\n",
        "    test_loss = loss_fn(preds, y_test)\n",
        "\n",
        "  train_loss_vals.append(training_loss.item())\n",
        "  test_loss_vals.append(test_loss.item())\n",
        "  epoch_counts.append(i)\n",
        "\n",
        "  print(f\"Epoch : {i + 1} \\t Train loss : {training_loss.item()} \\t Test loss : {test_loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUPAbygAXN1Q",
        "outputId": "f0a8037d-683c-4900-c81c-0bae7a87a3cf"
      },
      "execution_count": 3344,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 1 \t Train loss : 0.7549644708633423 \t Test loss : 0.7348002195358276\n",
            "Epoch : 2 \t Train loss : 0.7538003921508789 \t Test loss : 0.7334336638450623\n",
            "Epoch : 3 \t Train loss : 0.7526363730430603 \t Test loss : 0.7320671677589417\n",
            "Epoch : 4 \t Train loss : 0.7514723539352417 \t Test loss : 0.7307006120681763\n",
            "Epoch : 5 \t Train loss : 0.7503082752227783 \t Test loss : 0.7293341159820557\n",
            "Epoch : 6 \t Train loss : 0.7491443157196045 \t Test loss : 0.7279675602912903\n",
            "Epoch : 7 \t Train loss : 0.7479802370071411 \t Test loss : 0.7266010046005249\n",
            "Epoch : 8 \t Train loss : 0.7468162178993225 \t Test loss : 0.7252343893051147\n",
            "Epoch : 9 \t Train loss : 0.7456521987915039 \t Test loss : 0.7238678932189941\n",
            "Epoch : 10 \t Train loss : 0.7444881200790405 \t Test loss : 0.7225013375282288\n",
            "Epoch : 11 \t Train loss : 0.7433241605758667 \t Test loss : 0.7211348414421082\n",
            "Epoch : 12 \t Train loss : 0.7421600222587585 \t Test loss : 0.7197682857513428\n",
            "Epoch : 13 \t Train loss : 0.7409960627555847 \t Test loss : 0.7184017300605774\n",
            "Epoch : 14 \t Train loss : 0.7398320436477661 \t Test loss : 0.7170352339744568\n",
            "Epoch : 15 \t Train loss : 0.7386679649353027 \t Test loss : 0.7156686782836914\n",
            "Epoch : 16 \t Train loss : 0.7375038862228394 \t Test loss : 0.7143021821975708\n",
            "Epoch : 17 \t Train loss : 0.7363399267196655 \t Test loss : 0.7129356265068054\n",
            "Epoch : 18 \t Train loss : 0.7351757884025574 \t Test loss : 0.7115691304206848\n",
            "Epoch : 19 \t Train loss : 0.7340118288993835 \t Test loss : 0.7102025151252747\n",
            "Epoch : 20 \t Train loss : 0.7328478097915649 \t Test loss : 0.7088359594345093\n",
            "Epoch : 21 \t Train loss : 0.7316837310791016 \t Test loss : 0.7074694037437439\n",
            "Epoch : 22 \t Train loss : 0.7305197715759277 \t Test loss : 0.7061029672622681\n",
            "Epoch : 23 \t Train loss : 0.7293556928634644 \t Test loss : 0.7047363519668579\n",
            "Epoch : 24 \t Train loss : 0.7281917333602905 \t Test loss : 0.7033697962760925\n",
            "Epoch : 25 \t Train loss : 0.7270277142524719 \t Test loss : 0.7020033001899719\n",
            "Epoch : 26 \t Train loss : 0.7258636355400085 \t Test loss : 0.7006367444992065\n",
            "Epoch : 27 \t Train loss : 0.7246996164321899 \t Test loss : 0.6992701888084412\n",
            "Epoch : 28 \t Train loss : 0.7235356569290161 \t Test loss : 0.6979036927223206\n",
            "Epoch : 29 \t Train loss : 0.7223714590072632 \t Test loss : 0.6965370774269104\n",
            "Epoch : 30 \t Train loss : 0.7212074995040894 \t Test loss : 0.6951705813407898\n",
            "Epoch : 31 \t Train loss : 0.7200435400009155 \t Test loss : 0.6938040256500244\n",
            "Epoch : 32 \t Train loss : 0.7188794016838074 \t Test loss : 0.6924375295639038\n",
            "Epoch : 33 \t Train loss : 0.7177153825759888 \t Test loss : 0.6910709142684937\n",
            "Epoch : 34 \t Train loss : 0.7165514230728149 \t Test loss : 0.6897043585777283\n",
            "Epoch : 35 \t Train loss : 0.7153873443603516 \t Test loss : 0.6883379220962524\n",
            "Epoch : 36 \t Train loss : 0.7142232656478882 \t Test loss : 0.6869713068008423\n",
            "Epoch : 37 \t Train loss : 0.7130593061447144 \t Test loss : 0.6856047511100769\n",
            "Epoch : 38 \t Train loss : 0.7118952870368958 \t Test loss : 0.6842382550239563\n",
            "Epoch : 39 \t Train loss : 0.7107312679290771 \t Test loss : 0.6828716993331909\n",
            "Epoch : 40 \t Train loss : 0.7095671892166138 \t Test loss : 0.6815052032470703\n",
            "Epoch : 41 \t Train loss : 0.7084031105041504 \t Test loss : 0.6801386475563049\n",
            "Epoch : 42 \t Train loss : 0.7072391510009766 \t Test loss : 0.6787721514701843\n",
            "Epoch : 43 \t Train loss : 0.7060750722885132 \t Test loss : 0.677405595779419\n",
            "Epoch : 44 \t Train loss : 0.7049110531806946 \t Test loss : 0.6760390996932983\n",
            "Epoch : 45 \t Train loss : 0.703747034072876 \t Test loss : 0.674672544002533\n",
            "Epoch : 46 \t Train loss : 0.7025829553604126 \t Test loss : 0.6733059287071228\n",
            "Epoch : 47 \t Train loss : 0.7014189958572388 \t Test loss : 0.6719393730163574\n",
            "Epoch : 48 \t Train loss : 0.7002549171447754 \t Test loss : 0.6705728769302368\n",
            "Epoch : 49 \t Train loss : 0.6990908980369568 \t Test loss : 0.6692063212394714\n",
            "Epoch : 50 \t Train loss : 0.6979268193244934 \t Test loss : 0.667839765548706\n",
            "Epoch : 51 \t Train loss : 0.6967628002166748 \t Test loss : 0.6664732694625854\n",
            "Epoch : 52 \t Train loss : 0.6955987811088562 \t Test loss : 0.6651067733764648\n",
            "Epoch : 53 \t Train loss : 0.6944347620010376 \t Test loss : 0.6637401580810547\n",
            "Epoch : 54 \t Train loss : 0.6932708024978638 \t Test loss : 0.6623736619949341\n",
            "Epoch : 55 \t Train loss : 0.6921066641807556 \t Test loss : 0.6610070466995239\n",
            "Epoch : 56 \t Train loss : 0.6909425854682922 \t Test loss : 0.6596406102180481\n",
            "Epoch : 57 \t Train loss : 0.6897786259651184 \t Test loss : 0.6582740545272827\n",
            "Epoch : 58 \t Train loss : 0.6886146068572998 \t Test loss : 0.6569074392318726\n",
            "Epoch : 59 \t Train loss : 0.6874505281448364 \t Test loss : 0.6555410027503967\n",
            "Epoch : 60 \t Train loss : 0.6862865090370178 \t Test loss : 0.6541743874549866\n",
            "Epoch : 61 \t Train loss : 0.6851224899291992 \t Test loss : 0.652807891368866\n",
            "Epoch : 62 \t Train loss : 0.6839584708213806 \t Test loss : 0.6514413356781006\n",
            "Epoch : 63 \t Train loss : 0.682794451713562 \t Test loss : 0.6500747799873352\n",
            "Epoch : 64 \t Train loss : 0.6816304326057434 \t Test loss : 0.6487082839012146\n",
            "Epoch : 65 \t Train loss : 0.6804662942886353 \t Test loss : 0.6473417282104492\n",
            "Epoch : 66 \t Train loss : 0.6793023347854614 \t Test loss : 0.6459752321243286\n",
            "Epoch : 67 \t Train loss : 0.6781383752822876 \t Test loss : 0.6446086168289185\n",
            "Epoch : 68 \t Train loss : 0.6769742369651794 \t Test loss : 0.6432420611381531\n",
            "Epoch : 69 \t Train loss : 0.6758102178573608 \t Test loss : 0.6418755650520325\n",
            "Epoch : 70 \t Train loss : 0.674646258354187 \t Test loss : 0.6405089497566223\n",
            "Epoch : 71 \t Train loss : 0.6734821200370789 \t Test loss : 0.6391425132751465\n",
            "Epoch : 72 \t Train loss : 0.6723181009292603 \t Test loss : 0.6377759575843811\n",
            "Epoch : 73 \t Train loss : 0.6711540818214417 \t Test loss : 0.6364094018936157\n",
            "Epoch : 74 \t Train loss : 0.669990062713623 \t Test loss : 0.6350429058074951\n",
            "Epoch : 75 \t Train loss : 0.6688259840011597 \t Test loss : 0.6336763501167297\n",
            "Epoch : 76 \t Train loss : 0.6676619648933411 \t Test loss : 0.6323097944259644\n",
            "Epoch : 77 \t Train loss : 0.666498064994812 \t Test loss : 0.630943238735199\n",
            "Epoch : 78 \t Train loss : 0.6653338670730591 \t Test loss : 0.6295766830444336\n",
            "Epoch : 79 \t Train loss : 0.6641699075698853 \t Test loss : 0.6282101273536682\n",
            "Epoch : 80 \t Train loss : 0.6630058884620667 \t Test loss : 0.6268435716629028\n",
            "Epoch : 81 \t Train loss : 0.6618417501449585 \t Test loss : 0.625477135181427\n",
            "Epoch : 82 \t Train loss : 0.6606777906417847 \t Test loss : 0.6241105198860168\n",
            "Epoch : 83 \t Train loss : 0.6595138311386108 \t Test loss : 0.6227440237998962\n",
            "Epoch : 84 \t Train loss : 0.6583497524261475 \t Test loss : 0.6213774681091309\n",
            "Epoch : 85 \t Train loss : 0.6571857333183289 \t Test loss : 0.6200109720230103\n",
            "Epoch : 86 \t Train loss : 0.6560215950012207 \t Test loss : 0.6186444163322449\n",
            "Epoch : 87 \t Train loss : 0.6548576951026917 \t Test loss : 0.6172778606414795\n",
            "Epoch : 88 \t Train loss : 0.6536935567855835 \t Test loss : 0.6159113645553589\n",
            "Epoch : 89 \t Train loss : 0.6525295972824097 \t Test loss : 0.6145447492599487\n",
            "Epoch : 90 \t Train loss : 0.6513656377792358 \t Test loss : 0.6131781935691833\n",
            "Epoch : 91 \t Train loss : 0.6502014994621277 \t Test loss : 0.6118116974830627\n",
            "Epoch : 92 \t Train loss : 0.6490374803543091 \t Test loss : 0.6104451417922974\n",
            "Epoch : 93 \t Train loss : 0.6478734612464905 \t Test loss : 0.609078586101532\n",
            "Epoch : 94 \t Train loss : 0.6467095017433167 \t Test loss : 0.6077120304107666\n",
            "Epoch : 95 \t Train loss : 0.6455453634262085 \t Test loss : 0.606345534324646\n",
            "Epoch : 96 \t Train loss : 0.6443814039230347 \t Test loss : 0.6049789786338806\n",
            "Epoch : 97 \t Train loss : 0.6432173252105713 \t Test loss : 0.60361248254776\n",
            "Epoch : 98 \t Train loss : 0.6420532464981079 \t Test loss : 0.6022459268569946\n",
            "Epoch : 99 \t Train loss : 0.6408892869949341 \t Test loss : 0.6008793711662292\n",
            "Epoch : 100 \t Train loss : 0.6397252678871155 \t Test loss : 0.5995128154754639\n",
            "Epoch : 101 \t Train loss : 0.6385611891746521 \t Test loss : 0.5981463193893433\n",
            "Epoch : 102 \t Train loss : 0.6373971700668335 \t Test loss : 0.5967798233032227\n",
            "Epoch : 103 \t Train loss : 0.6362331509590149 \t Test loss : 0.5954132080078125\n",
            "Epoch : 104 \t Train loss : 0.6350691318511963 \t Test loss : 0.5940467119216919\n",
            "Epoch : 105 \t Train loss : 0.6339050531387329 \t Test loss : 0.5926801562309265\n",
            "Epoch : 106 \t Train loss : 0.6327410936355591 \t Test loss : 0.5913136005401611\n",
            "Epoch : 107 \t Train loss : 0.6315770149230957 \t Test loss : 0.5899470448493958\n",
            "Epoch : 108 \t Train loss : 0.6304129362106323 \t Test loss : 0.5885804891586304\n",
            "Epoch : 109 \t Train loss : 0.6292489767074585 \t Test loss : 0.5872139930725098\n",
            "Epoch : 110 \t Train loss : 0.6280848979949951 \t Test loss : 0.5858474373817444\n",
            "Epoch : 111 \t Train loss : 0.6269208788871765 \t Test loss : 0.5844809412956238\n",
            "Epoch : 112 \t Train loss : 0.6257568597793579 \t Test loss : 0.5831143260002136\n",
            "Epoch : 113 \t Train loss : 0.6245928406715393 \t Test loss : 0.5817478895187378\n",
            "Epoch : 114 \t Train loss : 0.6234288215637207 \t Test loss : 0.5803812742233276\n",
            "Epoch : 115 \t Train loss : 0.6222647428512573 \t Test loss : 0.5790147185325623\n",
            "Epoch : 116 \t Train loss : 0.621100664138794 \t Test loss : 0.5776482224464417\n",
            "Epoch : 117 \t Train loss : 0.6199367046356201 \t Test loss : 0.5762816667556763\n",
            "Epoch : 118 \t Train loss : 0.6187726259231567 \t Test loss : 0.5749150514602661\n",
            "Epoch : 119 \t Train loss : 0.6176086664199829 \t Test loss : 0.5735486149787903\n",
            "Epoch : 120 \t Train loss : 0.6164445877075195 \t Test loss : 0.5721820592880249\n",
            "Epoch : 121 \t Train loss : 0.6152805089950562 \t Test loss : 0.5708155632019043\n",
            "Epoch : 122 \t Train loss : 0.6141165494918823 \t Test loss : 0.5694490075111389\n",
            "Epoch : 123 \t Train loss : 0.612952470779419 \t Test loss : 0.5680825114250183\n",
            "Epoch : 124 \t Train loss : 0.6117885112762451 \t Test loss : 0.5667159557342529\n",
            "Epoch : 125 \t Train loss : 0.6106244325637817 \t Test loss : 0.5653494596481323\n",
            "Epoch : 126 \t Train loss : 0.6094604730606079 \t Test loss : 0.5639828443527222\n",
            "Epoch : 127 \t Train loss : 0.6082964539527893 \t Test loss : 0.5626164078712463\n",
            "Epoch : 128 \t Train loss : 0.6071323156356812 \t Test loss : 0.561249852180481\n",
            "Epoch : 129 \t Train loss : 0.6059683561325073 \t Test loss : 0.5598832964897156\n",
            "Epoch : 130 \t Train loss : 0.6048043966293335 \t Test loss : 0.558516800403595\n",
            "Epoch : 131 \t Train loss : 0.6036403179168701 \t Test loss : 0.5571502447128296\n",
            "Epoch : 132 \t Train loss : 0.6024764180183411 \t Test loss : 0.5557838082313538\n",
            "Epoch : 133 \t Train loss : 0.6013123989105225 \t Test loss : 0.5544172525405884\n",
            "Epoch : 134 \t Train loss : 0.6001483201980591 \t Test loss : 0.553050696849823\n",
            "Epoch : 135 \t Train loss : 0.5989843606948853 \t Test loss : 0.5516841411590576\n",
            "Epoch : 136 \t Train loss : 0.5978202819824219 \t Test loss : 0.550317645072937\n",
            "Epoch : 137 \t Train loss : 0.5966562032699585 \t Test loss : 0.5489511489868164\n",
            "Epoch : 138 \t Train loss : 0.5954922437667847 \t Test loss : 0.5475846529006958\n",
            "Epoch : 139 \t Train loss : 0.5943282246589661 \t Test loss : 0.5462180972099304\n",
            "Epoch : 140 \t Train loss : 0.5931642055511475 \t Test loss : 0.5448516011238098\n",
            "Epoch : 141 \t Train loss : 0.5920001864433289 \t Test loss : 0.5434851050376892\n",
            "Epoch : 142 \t Train loss : 0.5908361673355103 \t Test loss : 0.542118489742279\n",
            "Epoch : 143 \t Train loss : 0.5896721482276917 \t Test loss : 0.5407519340515137\n",
            "Epoch : 144 \t Train loss : 0.588508129119873 \t Test loss : 0.5393854379653931\n",
            "Epoch : 145 \t Train loss : 0.5873441100120544 \t Test loss : 0.5380190014839172\n",
            "Epoch : 146 \t Train loss : 0.5861800909042358 \t Test loss : 0.5366524457931519\n",
            "Epoch : 147 \t Train loss : 0.5850160717964172 \t Test loss : 0.5352858304977417\n",
            "Epoch : 148 \t Train loss : 0.5838520526885986 \t Test loss : 0.5339193940162659\n",
            "Epoch : 149 \t Train loss : 0.5826880931854248 \t Test loss : 0.5325527787208557\n",
            "Epoch : 150 \t Train loss : 0.5815240144729614 \t Test loss : 0.5311863422393799\n",
            "Epoch : 151 \t Train loss : 0.5803600549697876 \t Test loss : 0.5298197269439697\n",
            "Epoch : 152 \t Train loss : 0.5791959762573242 \t Test loss : 0.5284532904624939\n",
            "Epoch : 153 \t Train loss : 0.5780320167541504 \t Test loss : 0.5270867347717285\n",
            "Epoch : 154 \t Train loss : 0.5768679976463318 \t Test loss : 0.5257202386856079\n",
            "Epoch : 155 \t Train loss : 0.5757039189338684 \t Test loss : 0.5243537425994873\n",
            "Epoch : 156 \t Train loss : 0.5745399594306946 \t Test loss : 0.5229871869087219\n",
            "Epoch : 157 \t Train loss : 0.573375940322876 \t Test loss : 0.5216206908226013\n",
            "Epoch : 158 \t Train loss : 0.5722119212150574 \t Test loss : 0.5202541351318359\n",
            "Epoch : 159 \t Train loss : 0.571047842502594 \t Test loss : 0.5188875794410706\n",
            "Epoch : 160 \t Train loss : 0.5698838233947754 \t Test loss : 0.51752108335495\n",
            "Epoch : 161 \t Train loss : 0.5687198638916016 \t Test loss : 0.5161545276641846\n",
            "Epoch : 162 \t Train loss : 0.5675557851791382 \t Test loss : 0.5147879719734192\n",
            "Epoch : 163 \t Train loss : 0.5663918256759644 \t Test loss : 0.5134214162826538\n",
            "Epoch : 164 \t Train loss : 0.5652278065681458 \t Test loss : 0.512054979801178\n",
            "Epoch : 165 \t Train loss : 0.5640638470649719 \t Test loss : 0.5106884241104126\n",
            "Epoch : 166 \t Train loss : 0.5628997087478638 \t Test loss : 0.509321928024292\n",
            "Epoch : 167 \t Train loss : 0.5617357492446899 \t Test loss : 0.5079553723335266\n",
            "Epoch : 168 \t Train loss : 0.5605716705322266 \t Test loss : 0.506588876247406\n",
            "Epoch : 169 \t Train loss : 0.5594077110290527 \t Test loss : 0.5052223205566406\n",
            "Epoch : 170 \t Train loss : 0.5582436919212341 \t Test loss : 0.50385582447052\n",
            "Epoch : 171 \t Train loss : 0.5570796728134155 \t Test loss : 0.5024893283843994\n",
            "Epoch : 172 \t Train loss : 0.5559157133102417 \t Test loss : 0.5011228322982788\n",
            "Epoch : 173 \t Train loss : 0.5547515749931335 \t Test loss : 0.4997562766075134\n",
            "Epoch : 174 \t Train loss : 0.5535876750946045 \t Test loss : 0.4983897805213928\n",
            "Epoch : 175 \t Train loss : 0.5524235963821411 \t Test loss : 0.49702319502830505\n",
            "Epoch : 176 \t Train loss : 0.5512596368789673 \t Test loss : 0.49565666913986206\n",
            "Epoch : 177 \t Train loss : 0.5500955581665039 \t Test loss : 0.49429017305374146\n",
            "Epoch : 178 \t Train loss : 0.5489314794540405 \t Test loss : 0.49292364716529846\n",
            "Epoch : 179 \t Train loss : 0.5477675199508667 \t Test loss : 0.49155712127685547\n",
            "Epoch : 180 \t Train loss : 0.5466034412384033 \t Test loss : 0.4901905953884125\n",
            "Epoch : 181 \t Train loss : 0.5454395413398743 \t Test loss : 0.4888240694999695\n",
            "Epoch : 182 \t Train loss : 0.5442754030227661 \t Test loss : 0.4874575138092041\n",
            "Epoch : 183 \t Train loss : 0.5431114435195923 \t Test loss : 0.4860909879207611\n",
            "Epoch : 184 \t Train loss : 0.5419474840164185 \t Test loss : 0.4847244620323181\n",
            "Epoch : 185 \t Train loss : 0.5407834053039551 \t Test loss : 0.48335790634155273\n",
            "Epoch : 186 \t Train loss : 0.5396193861961365 \t Test loss : 0.4819914698600769\n",
            "Epoch : 187 \t Train loss : 0.5384553670883179 \t Test loss : 0.4806249141693115\n",
            "Epoch : 188 \t Train loss : 0.537291407585144 \t Test loss : 0.47925838828086853\n",
            "Epoch : 189 \t Train loss : 0.5361273884773254 \t Test loss : 0.47789183259010315\n",
            "Epoch : 190 \t Train loss : 0.5349633097648621 \t Test loss : 0.47652536630630493\n",
            "Epoch : 191 \t Train loss : 0.5337992906570435 \t Test loss : 0.47515878081321716\n",
            "Epoch : 192 \t Train loss : 0.5326353311538696 \t Test loss : 0.47379231452941895\n",
            "Epoch : 193 \t Train loss : 0.5314712524414062 \t Test loss : 0.47242575883865356\n",
            "Epoch : 194 \t Train loss : 0.5303072333335876 \t Test loss : 0.47105926275253296\n",
            "Epoch : 195 \t Train loss : 0.5291432738304138 \t Test loss : 0.46969276666641235\n",
            "Epoch : 196 \t Train loss : 0.5279792547225952 \t Test loss : 0.46832624077796936\n",
            "Epoch : 197 \t Train loss : 0.5268152356147766 \t Test loss : 0.4669596552848816\n",
            "Epoch : 198 \t Train loss : 0.5256511569023132 \t Test loss : 0.465593159198761\n",
            "Epoch : 199 \t Train loss : 0.5244871973991394 \t Test loss : 0.464226633310318\n",
            "Epoch : 200 \t Train loss : 0.5233231782913208 \t Test loss : 0.462860107421875\n",
            "Epoch : 201 \t Train loss : 0.5221590995788574 \t Test loss : 0.461493581533432\n",
            "Epoch : 202 \t Train loss : 0.5209951400756836 \t Test loss : 0.460127055644989\n",
            "Epoch : 203 \t Train loss : 0.5198310613632202 \t Test loss : 0.4587605595588684\n",
            "Epoch : 204 \t Train loss : 0.5186671018600464 \t Test loss : 0.45739397406578064\n",
            "Epoch : 205 \t Train loss : 0.517503023147583 \t Test loss : 0.4560275077819824\n",
            "Epoch : 206 \t Train loss : 0.516339123249054 \t Test loss : 0.45466098189353943\n",
            "Epoch : 207 \t Train loss : 0.5151749849319458 \t Test loss : 0.45329442620277405\n",
            "Epoch : 208 \t Train loss : 0.514011025428772 \t Test loss : 0.45192790031433105\n",
            "Epoch : 209 \t Train loss : 0.5128470659255981 \t Test loss : 0.4505613446235657\n",
            "Epoch : 210 \t Train loss : 0.51168292760849 \t Test loss : 0.44919484853744507\n",
            "Epoch : 211 \t Train loss : 0.5105189681053162 \t Test loss : 0.44782835245132446\n",
            "Epoch : 212 \t Train loss : 0.5093549489974976 \t Test loss : 0.44646185636520386\n",
            "Epoch : 213 \t Train loss : 0.5081909894943237 \t Test loss : 0.44509536027908325\n",
            "Epoch : 214 \t Train loss : 0.5070269107818604 \t Test loss : 0.4437287747859955\n",
            "Epoch : 215 \t Train loss : 0.5058628916740417 \t Test loss : 0.4423622190952301\n",
            "Epoch : 216 \t Train loss : 0.5046988725662231 \t Test loss : 0.4409957528114319\n",
            "Epoch : 217 \t Train loss : 0.5035348534584045 \t Test loss : 0.4396291673183441\n",
            "Epoch : 218 \t Train loss : 0.5023708343505859 \t Test loss : 0.4382626414299011\n",
            "Epoch : 219 \t Train loss : 0.5012068152427673 \t Test loss : 0.4368961453437805\n",
            "Epoch : 220 \t Train loss : 0.5000427961349487 \t Test loss : 0.4355296194553375\n",
            "Epoch : 221 \t Train loss : 0.4988787770271301 \t Test loss : 0.4341631531715393\n",
            "Epoch : 222 \t Train loss : 0.4977148175239563 \t Test loss : 0.43279656767845154\n",
            "Epoch : 223 \t Train loss : 0.4965507388114929 \t Test loss : 0.43143001198768616\n",
            "Epoch : 224 \t Train loss : 0.4953867793083191 \t Test loss : 0.43006354570388794\n",
            "Epoch : 225 \t Train loss : 0.4942227005958557 \t Test loss : 0.42869701981544495\n",
            "Epoch : 226 \t Train loss : 0.4930586814880371 \t Test loss : 0.4273304343223572\n",
            "Epoch : 227 \t Train loss : 0.4918947219848633 \t Test loss : 0.42596396803855896\n",
            "Epoch : 228 \t Train loss : 0.4907306134700775 \t Test loss : 0.42459744215011597\n",
            "Epoch : 229 \t Train loss : 0.4895666539669037 \t Test loss : 0.4232308864593506\n",
            "Epoch : 230 \t Train loss : 0.4884026050567627 \t Test loss : 0.4218643307685852\n",
            "Epoch : 231 \t Train loss : 0.4872385859489441 \t Test loss : 0.4204978048801422\n",
            "Epoch : 232 \t Train loss : 0.4860745966434479 \t Test loss : 0.419131338596344\n",
            "Epoch : 233 \t Train loss : 0.4849105775356293 \t Test loss : 0.417764812707901\n",
            "Epoch : 234 \t Train loss : 0.48374658823013306 \t Test loss : 0.416398286819458\n",
            "Epoch : 235 \t Train loss : 0.4825824797153473 \t Test loss : 0.415031760931015\n",
            "Epoch : 236 \t Train loss : 0.48141852021217346 \t Test loss : 0.41366520524024963\n",
            "Epoch : 237 \t Train loss : 0.48025450110435486 \t Test loss : 0.41229867935180664\n",
            "Epoch : 238 \t Train loss : 0.47909054160118103 \t Test loss : 0.41093215346336365\n",
            "Epoch : 239 \t Train loss : 0.47792649269104004 \t Test loss : 0.40956562757492065\n",
            "Epoch : 240 \t Train loss : 0.47676247358322144 \t Test loss : 0.40819913148880005\n",
            "Epoch : 241 \t Train loss : 0.4755984842777252 \t Test loss : 0.40683260560035706\n",
            "Epoch : 242 \t Train loss : 0.47443443536758423 \t Test loss : 0.40546607971191406\n",
            "Epoch : 243 \t Train loss : 0.47327035665512085 \t Test loss : 0.40409955382347107\n",
            "Epoch : 244 \t Train loss : 0.472106397151947 \t Test loss : 0.4027330279350281\n",
            "Epoch : 245 \t Train loss : 0.47094234824180603 \t Test loss : 0.40136653184890747\n",
            "Epoch : 246 \t Train loss : 0.46977829933166504 \t Test loss : 0.3999999761581421\n",
            "Epoch : 247 \t Train loss : 0.46861428022384644 \t Test loss : 0.3986334502696991\n",
            "Epoch : 248 \t Train loss : 0.4674503207206726 \t Test loss : 0.3972669243812561\n",
            "Epoch : 249 \t Train loss : 0.4662862718105316 \t Test loss : 0.3959004282951355\n",
            "Epoch : 250 \t Train loss : 0.4651222825050354 \t Test loss : 0.3945338726043701\n",
            "Epoch : 251 \t Train loss : 0.463958203792572 \t Test loss : 0.3931673467159271\n",
            "Epoch : 252 \t Train loss : 0.4627942442893982 \t Test loss : 0.39180082082748413\n",
            "Epoch : 253 \t Train loss : 0.4616301953792572 \t Test loss : 0.3904343247413635\n",
            "Epoch : 254 \t Train loss : 0.460466206073761 \t Test loss : 0.38906779885292053\n",
            "Epoch : 255 \t Train loss : 0.4593021869659424 \t Test loss : 0.38770127296447754\n",
            "Epoch : 256 \t Train loss : 0.4581380784511566 \t Test loss : 0.38633471727371216\n",
            "Epoch : 257 \t Train loss : 0.4569741189479828 \t Test loss : 0.38496822118759155\n",
            "Epoch : 258 \t Train loss : 0.4558101296424866 \t Test loss : 0.38360169529914856\n",
            "Epoch : 259 \t Train loss : 0.4546460509300232 \t Test loss : 0.3822351396083832\n",
            "Epoch : 260 \t Train loss : 0.45348209142684937 \t Test loss : 0.3808686137199402\n",
            "Epoch : 261 \t Train loss : 0.45231813192367554 \t Test loss : 0.37950214743614197\n",
            "Epoch : 262 \t Train loss : 0.45115405321121216 \t Test loss : 0.3781355917453766\n",
            "Epoch : 263 \t Train loss : 0.44999009370803833 \t Test loss : 0.3767690360546112\n",
            "Epoch : 264 \t Train loss : 0.44882601499557495 \t Test loss : 0.3754025399684906\n",
            "Epoch : 265 \t Train loss : 0.44766196608543396 \t Test loss : 0.3740360140800476\n",
            "Epoch : 266 \t Train loss : 0.44649800658226013 \t Test loss : 0.372669517993927\n",
            "Epoch : 267 \t Train loss : 0.44533395767211914 \t Test loss : 0.3713029623031616\n",
            "Epoch : 268 \t Train loss : 0.44416993856430054 \t Test loss : 0.36993643641471863\n",
            "Epoch : 269 \t Train loss : 0.4430059492588043 \t Test loss : 0.36856991052627563\n",
            "Epoch : 270 \t Train loss : 0.4418419301509857 \t Test loss : 0.36720338463783264\n",
            "Epoch : 271 \t Train loss : 0.4406778812408447 \t Test loss : 0.36583685874938965\n",
            "Epoch : 272 \t Train loss : 0.4395138621330261 \t Test loss : 0.36447030305862427\n",
            "Epoch : 273 \t Train loss : 0.4383498728275299 \t Test loss : 0.36310380697250366\n",
            "Epoch : 274 \t Train loss : 0.4371858537197113 \t Test loss : 0.36173728108406067\n",
            "Epoch : 275 \t Train loss : 0.4360218048095703 \t Test loss : 0.36037078499794006\n",
            "Epoch : 276 \t Train loss : 0.4348578453063965 \t Test loss : 0.3590042293071747\n",
            "Epoch : 277 \t Train loss : 0.4336937963962555 \t Test loss : 0.3576377034187317\n",
            "Epoch : 278 \t Train loss : 0.4325297474861145 \t Test loss : 0.3562712073326111\n",
            "Epoch : 279 \t Train loss : 0.4313657283782959 \t Test loss : 0.3549047112464905\n",
            "Epoch : 280 \t Train loss : 0.4302017092704773 \t Test loss : 0.3535381853580475\n",
            "Epoch : 281 \t Train loss : 0.4290377199649811 \t Test loss : 0.3521716594696045\n",
            "Epoch : 282 \t Train loss : 0.42787376046180725 \t Test loss : 0.3508051037788391\n",
            "Epoch : 283 \t Train loss : 0.4267096519470215 \t Test loss : 0.3494386076927185\n",
            "Epoch : 284 \t Train loss : 0.42554569244384766 \t Test loss : 0.34807202219963074\n",
            "Epoch : 285 \t Train loss : 0.42438164353370667 \t Test loss : 0.34670549631118774\n",
            "Epoch : 286 \t Train loss : 0.42321762442588806 \t Test loss : 0.34533900022506714\n",
            "Epoch : 287 \t Train loss : 0.42205363512039185 \t Test loss : 0.34397247433662415\n",
            "Epoch : 288 \t Train loss : 0.42088955640792847 \t Test loss : 0.3426060080528259\n",
            "Epoch : 289 \t Train loss : 0.41972559690475464 \t Test loss : 0.34123945236206055\n",
            "Epoch : 290 \t Train loss : 0.41856154799461365 \t Test loss : 0.33987292647361755\n",
            "Epoch : 291 \t Train loss : 0.4173975884914398 \t Test loss : 0.3385063707828522\n",
            "Epoch : 292 \t Train loss : 0.41623353958129883 \t Test loss : 0.3371398150920868\n",
            "Epoch : 293 \t Train loss : 0.41506949067115784 \t Test loss : 0.3357733190059662\n",
            "Epoch : 294 \t Train loss : 0.413905531167984 \t Test loss : 0.3344067931175232\n",
            "Epoch : 295 \t Train loss : 0.412741482257843 \t Test loss : 0.3330402970314026\n",
            "Epoch : 296 \t Train loss : 0.4115775227546692 \t Test loss : 0.3316737711429596\n",
            "Epoch : 297 \t Train loss : 0.4104134142398834 \t Test loss : 0.3303072452545166\n",
            "Epoch : 298 \t Train loss : 0.4092493951320648 \t Test loss : 0.3289407193660736\n",
            "Epoch : 299 \t Train loss : 0.4080854058265686 \t Test loss : 0.3275741934776306\n",
            "Epoch : 300 \t Train loss : 0.4069214463233948 \t Test loss : 0.32620769739151\n",
            "Epoch : 301 \t Train loss : 0.4057573676109314 \t Test loss : 0.3248412013053894\n",
            "Epoch : 302 \t Train loss : 0.4045933783054352 \t Test loss : 0.32347458600997925\n",
            "Epoch : 303 \t Train loss : 0.4034293591976166 \t Test loss : 0.32210806012153625\n",
            "Epoch : 304 \t Train loss : 0.4022653102874756 \t Test loss : 0.32074156403541565\n",
            "Epoch : 305 \t Train loss : 0.401101291179657 \t Test loss : 0.31937503814697266\n",
            "Epoch : 306 \t Train loss : 0.39993733167648315 \t Test loss : 0.31800854206085205\n",
            "Epoch : 307 \t Train loss : 0.39877328276634216 \t Test loss : 0.31664198637008667\n",
            "Epoch : 308 \t Train loss : 0.39760923385620117 \t Test loss : 0.31527549028396606\n",
            "Epoch : 309 \t Train loss : 0.39644521474838257 \t Test loss : 0.3139089345932007\n",
            "Epoch : 310 \t Train loss : 0.39528122544288635 \t Test loss : 0.3125423789024353\n",
            "Epoch : 311 \t Train loss : 0.39411720633506775 \t Test loss : 0.3111758828163147\n",
            "Epoch : 312 \t Train loss : 0.39295318722724915 \t Test loss : 0.3098093569278717\n",
            "Epoch : 313 \t Train loss : 0.39178916811943054 \t Test loss : 0.3084428608417511\n",
            "Epoch : 314 \t Train loss : 0.39062511920928955 \t Test loss : 0.3070763349533081\n",
            "Epoch : 315 \t Train loss : 0.38946112990379333 \t Test loss : 0.3057098090648651\n",
            "Epoch : 316 \t Train loss : 0.38829708099365234 \t Test loss : 0.3043432831764221\n",
            "Epoch : 317 \t Train loss : 0.38713306188583374 \t Test loss : 0.3029767870903015\n",
            "Epoch : 318 \t Train loss : 0.3859690725803375 \t Test loss : 0.30161020159721375\n",
            "Epoch : 319 \t Train loss : 0.38480502367019653 \t Test loss : 0.30024367570877075\n",
            "Epoch : 320 \t Train loss : 0.3836410641670227 \t Test loss : 0.29887717962265015\n",
            "Epoch : 321 \t Train loss : 0.3824770152568817 \t Test loss : 0.29751068353652954\n",
            "Epoch : 322 \t Train loss : 0.3813130259513855 \t Test loss : 0.2961440980434418\n",
            "Epoch : 323 \t Train loss : 0.3801489770412445 \t Test loss : 0.2947775721549988\n",
            "Epoch : 324 \t Train loss : 0.3789849579334259 \t Test loss : 0.2934110760688782\n",
            "Epoch : 325 \t Train loss : 0.3778209090232849 \t Test loss : 0.2920445501804352\n",
            "Epoch : 326 \t Train loss : 0.3766569495201111 \t Test loss : 0.2906780242919922\n",
            "Epoch : 327 \t Train loss : 0.3754929304122925 \t Test loss : 0.2893114686012268\n",
            "Epoch : 328 \t Train loss : 0.3743289113044739 \t Test loss : 0.2879449725151062\n",
            "Epoch : 329 \t Train loss : 0.3731648921966553 \t Test loss : 0.2865784764289856\n",
            "Epoch : 330 \t Train loss : 0.37200087308883667 \t Test loss : 0.2852119505405426\n",
            "Epoch : 331 \t Train loss : 0.3708368241786957 \t Test loss : 0.28384536504745483\n",
            "Epoch : 332 \t Train loss : 0.36967283487319946 \t Test loss : 0.28247886896133423\n",
            "Epoch : 333 \t Train loss : 0.36850881576538086 \t Test loss : 0.28111234307289124\n",
            "Epoch : 334 \t Train loss : 0.36734479665756226 \t Test loss : 0.27974584698677063\n",
            "Epoch : 335 \t Train loss : 0.36618077754974365 \t Test loss : 0.27837932109832764\n",
            "Epoch : 336 \t Train loss : 0.36501675844192505 \t Test loss : 0.27701276540756226\n",
            "Epoch : 337 \t Train loss : 0.36385273933410645 \t Test loss : 0.27564626932144165\n",
            "Epoch : 338 \t Train loss : 0.36268875002861023 \t Test loss : 0.27427974343299866\n",
            "Epoch : 339 \t Train loss : 0.3615247309207916 \t Test loss : 0.27291321754455566\n",
            "Epoch : 340 \t Train loss : 0.36036068201065063 \t Test loss : 0.27154669165611267\n",
            "Epoch : 341 \t Train loss : 0.35919666290283203 \t Test loss : 0.2701801657676697\n",
            "Epoch : 342 \t Train loss : 0.3580326437950134 \t Test loss : 0.2688136398792267\n",
            "Epoch : 343 \t Train loss : 0.3568686246871948 \t Test loss : 0.2674471139907837\n",
            "Epoch : 344 \t Train loss : 0.3557046055793762 \t Test loss : 0.2660805583000183\n",
            "Epoch : 345 \t Train loss : 0.3545405864715576 \t Test loss : 0.2647140622138977\n",
            "Epoch : 346 \t Train loss : 0.353376567363739 \t Test loss : 0.2633475363254547\n",
            "Epoch : 347 \t Train loss : 0.3522125780582428 \t Test loss : 0.2619810104370117\n",
            "Epoch : 348 \t Train loss : 0.3510485589504242 \t Test loss : 0.2606144845485687\n",
            "Epoch : 349 \t Train loss : 0.3498845398426056 \t Test loss : 0.25924795866012573\n",
            "Epoch : 350 \t Train loss : 0.3487204909324646 \t Test loss : 0.2578814625740051\n",
            "Epoch : 351 \t Train loss : 0.347556471824646 \t Test loss : 0.25651493668556213\n",
            "Epoch : 352 \t Train loss : 0.3463924527168274 \t Test loss : 0.25514838099479675\n",
            "Epoch : 353 \t Train loss : 0.3452284634113312 \t Test loss : 0.25378188490867615\n",
            "Epoch : 354 \t Train loss : 0.3440644443035126 \t Test loss : 0.25241532921791077\n",
            "Epoch : 355 \t Train loss : 0.34290045499801636 \t Test loss : 0.2510488033294678\n",
            "Epoch : 356 \t Train loss : 0.34173640608787537 \t Test loss : 0.24968227744102478\n",
            "Epoch : 357 \t Train loss : 0.34057238698005676 \t Test loss : 0.24831578135490417\n",
            "Epoch : 358 \t Train loss : 0.33940833806991577 \t Test loss : 0.24694927036762238\n",
            "Epoch : 359 \t Train loss : 0.33824434876441956 \t Test loss : 0.2455827295780182\n",
            "Epoch : 360 \t Train loss : 0.33708032965660095 \t Test loss : 0.2442162036895752\n",
            "Epoch : 361 \t Train loss : 0.33591631054878235 \t Test loss : 0.2428496778011322\n",
            "Epoch : 362 \t Train loss : 0.33475232124328613 \t Test loss : 0.2414831668138504\n",
            "Epoch : 363 \t Train loss : 0.33358824253082275 \t Test loss : 0.24011662602424622\n",
            "Epoch : 364 \t Train loss : 0.33242425322532654 \t Test loss : 0.23875010013580322\n",
            "Epoch : 365 \t Train loss : 0.3312602639198303 \t Test loss : 0.23738357424736023\n",
            "Epoch : 366 \t Train loss : 0.3300962448120117 \t Test loss : 0.23601703345775604\n",
            "Epoch : 367 \t Train loss : 0.3289322257041931 \t Test loss : 0.23465053737163544\n",
            "Epoch : 368 \t Train loss : 0.3277681767940521 \t Test loss : 0.23328399658203125\n",
            "Epoch : 369 \t Train loss : 0.3266041576862335 \t Test loss : 0.23191750049591064\n",
            "Epoch : 370 \t Train loss : 0.3254401683807373 \t Test loss : 0.23055095970630646\n",
            "Epoch : 371 \t Train loss : 0.3242761492729187 \t Test loss : 0.22918438911437988\n",
            "Epoch : 372 \t Train loss : 0.3231120705604553 \t Test loss : 0.22781789302825928\n",
            "Epoch : 373 \t Train loss : 0.3219480812549591 \t Test loss : 0.2264513522386551\n",
            "Epoch : 374 \t Train loss : 0.3207840323448181 \t Test loss : 0.2250848263502121\n",
            "Epoch : 375 \t Train loss : 0.3196200430393219 \t Test loss : 0.2237183153629303\n",
            "Epoch : 376 \t Train loss : 0.3184560537338257 \t Test loss : 0.2223517894744873\n",
            "Epoch : 377 \t Train loss : 0.3172920346260071 \t Test loss : 0.2209852635860443\n",
            "Epoch : 378 \t Train loss : 0.3161279857158661 \t Test loss : 0.21961872279644012\n",
            "Epoch : 379 \t Train loss : 0.3149639666080475 \t Test loss : 0.21825222671031952\n",
            "Epoch : 380 \t Train loss : 0.3137999176979065 \t Test loss : 0.21688565611839294\n",
            "Epoch : 381 \t Train loss : 0.3126359283924103 \t Test loss : 0.21551914513111115\n",
            "Epoch : 382 \t Train loss : 0.3114719092845917 \t Test loss : 0.21415260434150696\n",
            "Epoch : 383 \t Train loss : 0.31030789017677307 \t Test loss : 0.21278610825538635\n",
            "Epoch : 384 \t Train loss : 0.30914390087127686 \t Test loss : 0.21141958236694336\n",
            "Epoch : 385 \t Train loss : 0.30797988176345825 \t Test loss : 0.21005305647850037\n",
            "Epoch : 386 \t Train loss : 0.30681586265563965 \t Test loss : 0.20868656039237976\n",
            "Epoch : 387 \t Train loss : 0.30565181374549866 \t Test loss : 0.20732000470161438\n",
            "Epoch : 388 \t Train loss : 0.30448779463768005 \t Test loss : 0.2059534788131714\n",
            "Epoch : 389 \t Train loss : 0.30332377552986145 \t Test loss : 0.2045869529247284\n",
            "Epoch : 390 \t Train loss : 0.30215978622436523 \t Test loss : 0.203220397233963\n",
            "Epoch : 391 \t Train loss : 0.30099576711654663 \t Test loss : 0.2018539011478424\n",
            "Epoch : 392 \t Train loss : 0.29983171820640564 \t Test loss : 0.20048737525939941\n",
            "Epoch : 393 \t Train loss : 0.2986677289009094 \t Test loss : 0.19912084937095642\n",
            "Epoch : 394 \t Train loss : 0.2975037097930908 \t Test loss : 0.19775430858135223\n",
            "Epoch : 395 \t Train loss : 0.2963396906852722 \t Test loss : 0.19638781249523163\n",
            "Epoch : 396 \t Train loss : 0.2951756715774536 \t Test loss : 0.19502130150794983\n",
            "Epoch : 397 \t Train loss : 0.294011652469635 \t Test loss : 0.19365474581718445\n",
            "Epoch : 398 \t Train loss : 0.2928476333618164 \t Test loss : 0.19228821992874146\n",
            "Epoch : 399 \t Train loss : 0.2916836142539978 \t Test loss : 0.19092172384262085\n",
            "Epoch : 400 \t Train loss : 0.2905195951461792 \t Test loss : 0.18955519795417786\n",
            "Epoch : 401 \t Train loss : 0.2893555760383606 \t Test loss : 0.18818870186805725\n",
            "Epoch : 402 \t Train loss : 0.28819161653518677 \t Test loss : 0.18682217597961426\n",
            "Epoch : 403 \t Train loss : 0.2870275378227234 \t Test loss : 0.18545565009117126\n",
            "Epoch : 404 \t Train loss : 0.2858635187149048 \t Test loss : 0.18408913910388947\n",
            "Epoch : 405 \t Train loss : 0.2846994996070862 \t Test loss : 0.18272261321544647\n",
            "Epoch : 406 \t Train loss : 0.28353551030158997 \t Test loss : 0.18135607242584229\n",
            "Epoch : 407 \t Train loss : 0.282371461391449 \t Test loss : 0.1799895465373993\n",
            "Epoch : 408 \t Train loss : 0.28120747208595276 \t Test loss : 0.1786230355501175\n",
            "Epoch : 409 \t Train loss : 0.28004348278045654 \t Test loss : 0.1772564947605133\n",
            "Epoch : 410 \t Train loss : 0.27887946367263794 \t Test loss : 0.1758899837732315\n",
            "Epoch : 411 \t Train loss : 0.27771544456481934 \t Test loss : 0.17452344298362732\n",
            "Epoch : 412 \t Train loss : 0.27655139565467834 \t Test loss : 0.17315691709518433\n",
            "Epoch : 413 \t Train loss : 0.27538737654685974 \t Test loss : 0.17179039120674133\n",
            "Epoch : 414 \t Train loss : 0.2742233872413635 \t Test loss : 0.17042389512062073\n",
            "Epoch : 415 \t Train loss : 0.27305933833122253 \t Test loss : 0.16905733942985535\n",
            "Epoch : 416 \t Train loss : 0.2718953490257263 \t Test loss : 0.16769082844257355\n",
            "Epoch : 417 \t Train loss : 0.2707313001155853 \t Test loss : 0.16632430255413055\n",
            "Epoch : 418 \t Train loss : 0.2695673108100891 \t Test loss : 0.16495779156684875\n",
            "Epoch : 419 \t Train loss : 0.2684032917022705 \t Test loss : 0.16359125077724457\n",
            "Epoch : 420 \t Train loss : 0.2672392725944519 \t Test loss : 0.16222473978996277\n",
            "Epoch : 421 \t Train loss : 0.2660752236843109 \t Test loss : 0.16085822880268097\n",
            "Epoch : 422 \t Train loss : 0.2649112343788147 \t Test loss : 0.15949168801307678\n",
            "Epoch : 423 \t Train loss : 0.2637471854686737 \t Test loss : 0.1581251323223114\n",
            "Epoch : 424 \t Train loss : 0.2625831961631775 \t Test loss : 0.1567586362361908\n",
            "Epoch : 425 \t Train loss : 0.2614191770553589 \t Test loss : 0.1553920954465866\n",
            "Epoch : 426 \t Train loss : 0.2602551579475403 \t Test loss : 0.1540255844593048\n",
            "Epoch : 427 \t Train loss : 0.2590911388397217 \t Test loss : 0.15265905857086182\n",
            "Epoch : 428 \t Train loss : 0.2579271197319031 \t Test loss : 0.15129253268241882\n",
            "Epoch : 429 \t Train loss : 0.2567631006240845 \t Test loss : 0.14992602169513702\n",
            "Epoch : 430 \t Train loss : 0.25559908151626587 \t Test loss : 0.14855948090553284\n",
            "Epoch : 431 \t Train loss : 0.25443509221076965 \t Test loss : 0.14719296991825104\n",
            "Epoch : 432 \t Train loss : 0.25327104330062866 \t Test loss : 0.14582644402980804\n",
            "Epoch : 433 \t Train loss : 0.25210705399513245 \t Test loss : 0.14445990324020386\n",
            "Epoch : 434 \t Train loss : 0.25094300508499146 \t Test loss : 0.14309337735176086\n",
            "Epoch : 435 \t Train loss : 0.24977898597717285 \t Test loss : 0.14172688126564026\n",
            "Epoch : 436 \t Train loss : 0.24861495196819305 \t Test loss : 0.14036032557487488\n",
            "Epoch : 437 \t Train loss : 0.24745094776153564 \t Test loss : 0.13899381458759308\n",
            "Epoch : 438 \t Train loss : 0.24628691375255585 \t Test loss : 0.13762728869915009\n",
            "Epoch : 439 \t Train loss : 0.24512290954589844 \t Test loss : 0.1362607628107071\n",
            "Epoch : 440 \t Train loss : 0.24395892024040222 \t Test loss : 0.1348942369222641\n",
            "Epoch : 441 \t Train loss : 0.24279490113258362 \t Test loss : 0.1335277259349823\n",
            "Epoch : 442 \t Train loss : 0.24163088202476501 \t Test loss : 0.1321611851453781\n",
            "Epoch : 443 \t Train loss : 0.24046683311462402 \t Test loss : 0.1307946741580963\n",
            "Epoch : 444 \t Train loss : 0.2393028289079666 \t Test loss : 0.12942811846733093\n",
            "Epoch : 445 \t Train loss : 0.2381388247013092 \t Test loss : 0.12806160748004913\n",
            "Epoch : 446 \t Train loss : 0.2369747906923294 \t Test loss : 0.12669511139392853\n",
            "Epoch : 447 \t Train loss : 0.2358108013868332 \t Test loss : 0.12532857060432434\n",
            "Epoch : 448 \t Train loss : 0.2346467673778534 \t Test loss : 0.12396204471588135\n",
            "Epoch : 449 \t Train loss : 0.2334827482700348 \t Test loss : 0.12259554862976074\n",
            "Epoch : 450 \t Train loss : 0.2323187291622162 \t Test loss : 0.12122900784015656\n",
            "Epoch : 451 \t Train loss : 0.23115472495555878 \t Test loss : 0.11986247450113297\n",
            "Epoch : 452 \t Train loss : 0.22999069094657898 \t Test loss : 0.11849595606327057\n",
            "Epoch : 453 \t Train loss : 0.228826642036438 \t Test loss : 0.11712943017482758\n",
            "Epoch : 454 \t Train loss : 0.22766265273094177 \t Test loss : 0.11576290428638458\n",
            "Epoch : 455 \t Train loss : 0.22649864852428436 \t Test loss : 0.11439637839794159\n",
            "Epoch : 456 \t Train loss : 0.22533461451530457 \t Test loss : 0.1130298599600792\n",
            "Epoch : 457 \t Train loss : 0.22417061030864716 \t Test loss : 0.1116633415222168\n",
            "Epoch : 458 \t Train loss : 0.22300660610198975 \t Test loss : 0.1102968081831932\n",
            "Epoch : 459 \t Train loss : 0.22184257209300995 \t Test loss : 0.10893027484416962\n",
            "Epoch : 460 \t Train loss : 0.22067853808403015 \t Test loss : 0.10756375640630722\n",
            "Epoch : 461 \t Train loss : 0.21951453387737274 \t Test loss : 0.10619726032018661\n",
            "Epoch : 462 \t Train loss : 0.21835052967071533 \t Test loss : 0.10483076423406601\n",
            "Epoch : 463 \t Train loss : 0.21718649566173553 \t Test loss : 0.10346418619155884\n",
            "Epoch : 464 \t Train loss : 0.21602249145507812 \t Test loss : 0.10209767520427704\n",
            "Epoch : 465 \t Train loss : 0.21485845744609833 \t Test loss : 0.10073113441467285\n",
            "Epoch : 466 \t Train loss : 0.2136944830417633 \t Test loss : 0.09936460852622986\n",
            "Epoch : 467 \t Train loss : 0.21253041923046112 \t Test loss : 0.09799809008836746\n",
            "Epoch : 468 \t Train loss : 0.2113664150238037 \t Test loss : 0.09663156419992447\n",
            "Epoch : 469 \t Train loss : 0.2102023810148239 \t Test loss : 0.09526503831148148\n",
            "Epoch : 470 \t Train loss : 0.2090383768081665 \t Test loss : 0.09389851987361908\n",
            "Epoch : 471 \t Train loss : 0.2078743726015091 \t Test loss : 0.0925319716334343\n",
            "Epoch : 472 \t Train loss : 0.2067103385925293 \t Test loss : 0.0911654606461525\n",
            "Epoch : 473 \t Train loss : 0.2055463343858719 \t Test loss : 0.0897989273071289\n",
            "Epoch : 474 \t Train loss : 0.2043823003768921 \t Test loss : 0.08843240886926651\n",
            "Epoch : 475 \t Train loss : 0.20321829617023468 \t Test loss : 0.08706588298082352\n",
            "Epoch : 476 \t Train loss : 0.20205426216125488 \t Test loss : 0.08569936454296112\n",
            "Epoch : 477 \t Train loss : 0.20089025795459747 \t Test loss : 0.08433283865451813\n",
            "Epoch : 478 \t Train loss : 0.19972625374794006 \t Test loss : 0.08296632021665573\n",
            "Epoch : 479 \t Train loss : 0.19856221973896027 \t Test loss : 0.08159979432821274\n",
            "Epoch : 480 \t Train loss : 0.19739820063114166 \t Test loss : 0.08023326843976974\n",
            "Epoch : 481 \t Train loss : 0.19623419642448425 \t Test loss : 0.07886673510074615\n",
            "Epoch : 482 \t Train loss : 0.19507017731666565 \t Test loss : 0.07750021666288376\n",
            "Epoch : 483 \t Train loss : 0.19390615820884705 \t Test loss : 0.07613370567560196\n",
            "Epoch : 484 \t Train loss : 0.19274213910102844 \t Test loss : 0.07476717978715897\n",
            "Epoch : 485 \t Train loss : 0.19157810509204865 \t Test loss : 0.07340064644813538\n",
            "Epoch : 486 \t Train loss : 0.19041410088539124 \t Test loss : 0.07203412055969238\n",
            "Epoch : 487 \t Train loss : 0.18925008177757263 \t Test loss : 0.07066760212182999\n",
            "Epoch : 488 \t Train loss : 0.18808606266975403 \t Test loss : 0.069301076233387\n",
            "Epoch : 489 \t Train loss : 0.18692204356193542 \t Test loss : 0.0679345652461052\n",
            "Epoch : 490 \t Train loss : 0.18575800955295563 \t Test loss : 0.0665680319070816\n",
            "Epoch : 491 \t Train loss : 0.18459400534629822 \t Test loss : 0.06520148366689682\n",
            "Epoch : 492 \t Train loss : 0.18342998623847961 \t Test loss : 0.06383497267961502\n",
            "Epoch : 493 \t Train loss : 0.182265967130661 \t Test loss : 0.06246844679117203\n",
            "Epoch : 494 \t Train loss : 0.1811019480228424 \t Test loss : 0.06110192462801933\n",
            "Epoch : 495 \t Train loss : 0.1799379289150238 \t Test loss : 0.05973540619015694\n",
            "Epoch : 496 \t Train loss : 0.1787739247083664 \t Test loss : 0.05836887285113335\n",
            "Epoch : 497 \t Train loss : 0.1776098906993866 \t Test loss : 0.05700235441327095\n",
            "Epoch : 498 \t Train loss : 0.176445871591568 \t Test loss : 0.05563581734895706\n",
            "Epoch : 499 \t Train loss : 0.17528186738491058 \t Test loss : 0.05426930636167526\n",
            "Epoch : 500 \t Train loss : 0.17411784827709198 \t Test loss : 0.05290278047323227\n",
            "Epoch : 501 \t Train loss : 0.17295384407043457 \t Test loss : 0.051536254584789276\n",
            "Epoch : 502 \t Train loss : 0.17178979516029358 \t Test loss : 0.05016972869634628\n",
            "Epoch : 503 \t Train loss : 0.17062579095363617 \t Test loss : 0.04880319908261299\n",
            "Epoch : 504 \t Train loss : 0.16946180164813995 \t Test loss : 0.04743669554591179\n",
            "Epoch : 505 \t Train loss : 0.16829776763916016 \t Test loss : 0.0460701584815979\n",
            "Epoch : 506 \t Train loss : 0.16713373363018036 \t Test loss : 0.04470362514257431\n",
            "Epoch : 507 \t Train loss : 0.16596972942352295 \t Test loss : 0.04333709925413132\n",
            "Epoch : 508 \t Train loss : 0.16480572521686554 \t Test loss : 0.04197058081626892\n",
            "Epoch : 509 \t Train loss : 0.16364167630672455 \t Test loss : 0.04060406610369682\n",
            "Epoch : 510 \t Train loss : 0.16247767210006714 \t Test loss : 0.03923754021525383\n",
            "Epoch : 511 \t Train loss : 0.16131366789340973 \t Test loss : 0.03787101432681084\n",
            "Epoch : 512 \t Train loss : 0.16014964878559113 \t Test loss : 0.036504488438367844\n",
            "Epoch : 513 \t Train loss : 0.15898559987545013 \t Test loss : 0.03513795882463455\n",
            "Epoch : 514 \t Train loss : 0.15782159566879272 \t Test loss : 0.03377143666148186\n",
            "Epoch : 515 \t Train loss : 0.15665757656097412 \t Test loss : 0.03240492194890976\n",
            "Epoch : 516 \t Train loss : 0.1554935723543167 \t Test loss : 0.03103838488459587\n",
            "Epoch : 517 \t Train loss : 0.15432953834533691 \t Test loss : 0.029671858996152878\n",
            "Epoch : 518 \t Train loss : 0.1531655341386795 \t Test loss : 0.028305333107709885\n",
            "Epoch : 519 \t Train loss : 0.1520015150308609 \t Test loss : 0.02693880721926689\n",
            "Epoch : 520 \t Train loss : 0.1508375108242035 \t Test loss : 0.02557229995727539\n",
            "Epoch : 521 \t Train loss : 0.1496734619140625 \t Test loss : 0.024205762892961502\n",
            "Epoch : 522 \t Train loss : 0.14850947260856628 \t Test loss : 0.022920925170183182\n",
            "Epoch : 523 \t Train loss : 0.14734545350074768 \t Test loss : 0.02169489860534668\n",
            "Epoch : 524 \t Train loss : 0.14618143439292908 \t Test loss : 0.020581740885972977\n",
            "Epoch : 525 \t Train loss : 0.14501741528511047 \t Test loss : 0.019498556852340698\n",
            "Epoch : 526 \t Train loss : 0.14385338127613068 \t Test loss : 0.01855231449007988\n",
            "Epoch : 527 \t Train loss : 0.14268937706947327 \t Test loss : 0.017637163400650024\n",
            "Epoch : 528 \t Train loss : 0.14152537286281586 \t Test loss : 0.016830217093229294\n",
            "Epoch : 529 \t Train loss : 0.14036133885383606 \t Test loss : 0.01608107052743435\n",
            "Epoch : 530 \t Train loss : 0.13919731974601746 \t Test loss : 0.015412991866469383\n",
            "Epoch : 531 \t Train loss : 0.13803330063819885 \t Test loss : 0.014827823266386986\n",
            "Epoch : 532 \t Train loss : 0.13686928153038025 \t Test loss : 0.014298224821686745\n",
            "Epoch : 533 \t Train loss : 0.13570526242256165 \t Test loss : 0.013875025324523449\n",
            "Epoch : 534 \t Train loss : 0.13454124331474304 \t Test loss : 0.013483494520187378\n",
            "Epoch : 535 \t Train loss : 0.13337722420692444 \t Test loss : 0.013220220804214478\n",
            "Epoch : 536 \t Train loss : 0.13221322000026703 \t Test loss : 0.012977582402527332\n",
            "Epoch : 537 \t Train loss : 0.13104920089244843 \t Test loss : 0.01286097802221775\n",
            "Epoch : 538 \t Train loss : 0.12988518178462982 \t Test loss : 0.012774634175002575\n",
            "Epoch : 539 \t Train loss : 0.12872114777565002 \t Test loss : 0.012794864363968372\n",
            "Epoch : 540 \t Train loss : 0.12755714356899261 \t Test loss : 0.012862777337431908\n",
            "Epoch : 541 \t Train loss : 0.126393124461174 \t Test loss : 0.01301949005573988\n",
            "Epoch : 542 \t Train loss : 0.1252290904521942 \t Test loss : 0.013239646330475807\n",
            "Epoch : 543 \t Train loss : 0.124065101146698 \t Test loss : 0.013532370328903198\n",
            "Epoch : 544 \t Train loss : 0.1229010671377182 \t Test loss : 0.013902759179472923\n",
            "Epoch : 545 \t Train loss : 0.1217370480298996 \t Test loss : 0.014331132173538208\n",
            "Epoch : 546 \t Train loss : 0.120573028922081 \t Test loss : 0.014849722385406494\n",
            "Epoch : 547 \t Train loss : 0.11940902471542358 \t Test loss : 0.01541333831846714\n",
            "Epoch : 548 \t Train loss : 0.11824500560760498 \t Test loss : 0.01607804372906685\n",
            "Epoch : 549 \t Train loss : 0.11708100140094757 \t Test loss : 0.01677648350596428\n",
            "Epoch : 550 \t Train loss : 0.11591696739196777 \t Test loss : 0.01758534274995327\n",
            "Epoch : 551 \t Train loss : 0.11475296318531036 \t Test loss : 0.01841823384165764\n",
            "Epoch : 552 \t Train loss : 0.11358892917633057 \t Test loss : 0.01936919614672661\n",
            "Epoch : 553 \t Train loss : 0.11242491006851196 \t Test loss : 0.0203360915184021\n",
            "Epoch : 554 \t Train loss : 0.11126089096069336 \t Test loss : 0.021427154541015625\n",
            "Epoch : 555 \t Train loss : 0.11009688675403595 \t Test loss : 0.02252766489982605\n",
            "Epoch : 556 \t Train loss : 0.10893286764621735 \t Test loss : 0.023756777867674828\n",
            "Epoch : 557 \t Train loss : 0.10776884853839874 \t Test loss : 0.02499048039317131\n",
            "Epoch : 558 \t Train loss : 0.10660482943058014 \t Test loss : 0.02635563537478447\n",
            "Epoch : 559 \t Train loss : 0.10544081032276154 \t Test loss : 0.027722150087356567\n",
            "Epoch : 560 \t Train loss : 0.1042768582701683 \t Test loss : 0.02904559299349785\n",
            "Epoch : 561 \t Train loss : 0.1031779870390892 \t Test loss : 0.03036900795996189\n",
            "Epoch : 562 \t Train loss : 0.10207913815975189 \t Test loss : 0.03169243782758713\n",
            "Epoch : 563 \t Train loss : 0.1010102853178978 \t Test loss : 0.032972972840070724\n",
            "Epoch : 564 \t Train loss : 0.0999743863940239 \t Test loss : 0.03425350785255432\n",
            "Epoch : 565 \t Train loss : 0.09896395355463028 \t Test loss : 0.03549143671989441\n",
            "Epoch : 566 \t Train loss : 0.09798877686262131 \t Test loss : 0.036729346960783005\n",
            "Epoch : 567 \t Train loss : 0.09703213721513748 \t Test loss : 0.03792481869459152\n",
            "Epoch : 568 \t Train loss : 0.09611555933952332 \t Test loss : 0.03912029415369034\n",
            "Epoch : 569 \t Train loss : 0.09520827233791351 \t Test loss : 0.04027355834841728\n",
            "Epoch : 570 \t Train loss : 0.09434810280799866 \t Test loss : 0.04142685607075691\n",
            "Epoch : 571 \t Train loss : 0.0934879332780838 \t Test loss : 0.042580146342515945\n",
            "Epoch : 572 \t Train loss : 0.09265317022800446 \t Test loss : 0.04369146749377251\n",
            "Epoch : 573 \t Train loss : 0.09184727072715759 \t Test loss : 0.044802773743867874\n",
            "Epoch : 574 \t Train loss : 0.09105296432971954 \t Test loss : 0.04587234929203987\n",
            "Epoch : 575 \t Train loss : 0.09029930830001831 \t Test loss : 0.04694189503788948\n",
            "Epoch : 576 \t Train loss : 0.08954566717147827 \t Test loss : 0.048011451959609985\n",
            "Epoch : 577 \t Train loss : 0.08881306648254395 \t Test loss : 0.04903955012559891\n",
            "Epoch : 578 \t Train loss : 0.08810955286026001 \t Test loss : 0.05006762221455574\n",
            "Epoch : 579 \t Train loss : 0.08740878105163574 \t Test loss : 0.05105438083410263\n",
            "Epoch : 580 \t Train loss : 0.08675341308116913 \t Test loss : 0.052041150629520416\n",
            "Epoch : 581 \t Train loss : 0.08609805256128311 \t Test loss : 0.05302790552377701\n",
            "Epoch : 582 \t Train loss : 0.0854484885931015 \t Test loss : 0.05397360771894455\n",
            "Epoch : 583 \t Train loss : 0.08483923971652985 \t Test loss : 0.05491930991411209\n",
            "Epoch : 584 \t Train loss : 0.0842299610376358 \t Test loss : 0.055865027010440826\n",
            "Epoch : 585 \t Train loss : 0.0836264118552208 \t Test loss : 0.05676983669400215\n",
            "Epoch : 586 \t Train loss : 0.0830613225698471 \t Test loss : 0.05767464637756348\n",
            "Epoch : 587 \t Train loss : 0.08249623328447342 \t Test loss : 0.058579493314027786\n",
            "Epoch : 588 \t Train loss : 0.08193367719650269 \t Test loss : 0.05944370478391647\n",
            "Epoch : 589 \t Train loss : 0.0814107283949852 \t Test loss : 0.06030797213315964\n",
            "Epoch : 590 \t Train loss : 0.0808877944946289 \t Test loss : 0.061172209680080414\n",
            "Epoch : 591 \t Train loss : 0.08036484569311142 \t Test loss : 0.06203646585345268\n",
            "Epoch : 592 \t Train loss : 0.07985880225896835 \t Test loss : 0.06286037713289261\n",
            "Epoch : 593 \t Train loss : 0.07937610149383545 \t Test loss : 0.06368426978588104\n",
            "Epoch : 594 \t Train loss : 0.07889338582754135 \t Test loss : 0.06450818479061127\n",
            "Epoch : 595 \t Train loss : 0.0784182921051979 \t Test loss : 0.0652918741106987\n",
            "Epoch : 596 \t Train loss : 0.07797396928071976 \t Test loss : 0.06607560068368912\n",
            "Epoch : 597 \t Train loss : 0.07752964645624161 \t Test loss : 0.06685929745435715\n",
            "Epoch : 598 \t Train loss : 0.07708531618118286 \t Test loss : 0.06764301657676697\n",
            "Epoch : 599 \t Train loss : 0.07665504515171051 \t Test loss : 0.06838681548833847\n",
            "Epoch : 600 \t Train loss : 0.0762471929192543 \t Test loss : 0.06913061439990997\n",
            "Epoch : 601 \t Train loss : 0.0758393257856369 \t Test loss : 0.06987441331148148\n",
            "Epoch : 602 \t Train loss : 0.0754314661026001 \t Test loss : 0.07061819732189178\n",
            "Epoch : 603 \t Train loss : 0.07504023611545563 \t Test loss : 0.0713222399353981\n",
            "Epoch : 604 \t Train loss : 0.07466702163219452 \t Test loss : 0.07202626764774323\n",
            "Epoch : 605 \t Train loss : 0.07429381459951401 \t Test loss : 0.07273031026124954\n",
            "Epoch : 606 \t Train loss : 0.0739206075668335 \t Test loss : 0.07343433797359467\n",
            "Epoch : 607 \t Train loss : 0.07356280833482742 \t Test loss : 0.07409892976284027\n",
            "Epoch : 608 \t Train loss : 0.07322237640619278 \t Test loss : 0.07476354390382767\n",
            "Epoch : 609 \t Train loss : 0.07288193702697754 \t Test loss : 0.07542814314365387\n",
            "Epoch : 610 \t Train loss : 0.07254151254892349 \t Test loss : 0.07609274983406067\n",
            "Epoch : 611 \t Train loss : 0.07221153378486633 \t Test loss : 0.07671807706356049\n",
            "Epoch : 612 \t Train loss : 0.07190211862325668 \t Test loss : 0.0773434191942215\n",
            "Epoch : 613 \t Train loss : 0.07159270346164703 \t Test loss : 0.07796874642372131\n",
            "Epoch : 614 \t Train loss : 0.07128328084945679 \t Test loss : 0.07859407365322113\n",
            "Epoch : 615 \t Train loss : 0.07097567617893219 \t Test loss : 0.07918041199445724\n",
            "Epoch : 616 \t Train loss : 0.07069545984268188 \t Test loss : 0.07976673543453217\n",
            "Epoch : 617 \t Train loss : 0.07041524350643158 \t Test loss : 0.08035305142402649\n",
            "Epoch : 618 \t Train loss : 0.07013503462076187 \t Test loss : 0.0809393897652626\n",
            "Epoch : 619 \t Train loss : 0.06985482573509216 \t Test loss : 0.08152568340301514\n",
            "Epoch : 620 \t Train loss : 0.06957834213972092 \t Test loss : 0.08207327127456665\n",
            "Epoch : 621 \t Train loss : 0.06932556629180908 \t Test loss : 0.08262081444263458\n",
            "Epoch : 622 \t Train loss : 0.06907279789447784 \t Test loss : 0.08316835761070251\n",
            "Epoch : 623 \t Train loss : 0.06882002204656601 \t Test loss : 0.08371590822935104\n",
            "Epoch : 624 \t Train loss : 0.06856725364923477 \t Test loss : 0.08426346629858017\n",
            "Epoch : 625 \t Train loss : 0.06831569224596024 \t Test loss : 0.08477237075567245\n",
            "Epoch : 626 \t Train loss : 0.06808866560459137 \t Test loss : 0.08528131246566772\n",
            "Epoch : 627 \t Train loss : 0.0678616464138031 \t Test loss : 0.08579021692276001\n",
            "Epoch : 628 \t Train loss : 0.06763461977243423 \t Test loss : 0.08629914373159409\n",
            "Epoch : 629 \t Train loss : 0.06740759313106537 \t Test loss : 0.08680804818868637\n",
            "Epoch : 630 \t Train loss : 0.0671805664896965 \t Test loss : 0.08731697499752045\n",
            "Epoch : 631 \t Train loss : 0.06696027517318726 \t Test loss : 0.08778758347034454\n",
            "Epoch : 632 \t Train loss : 0.06675725430250168 \t Test loss : 0.08825822174549103\n",
            "Epoch : 633 \t Train loss : 0.0665542334318161 \t Test loss : 0.08872881531715393\n",
            "Epoch : 634 \t Train loss : 0.06635120511054993 \t Test loss : 0.08919946104288101\n",
            "Epoch : 635 \t Train loss : 0.06614818423986435 \t Test loss : 0.0896700769662857\n",
            "Epoch : 636 \t Train loss : 0.06594515591859818 \t Test loss : 0.09014070779085159\n",
            "Epoch : 637 \t Train loss : 0.06574927270412445 \t Test loss : 0.09057312458753586\n",
            "Epoch : 638 \t Train loss : 0.06556858122348785 \t Test loss : 0.09100554883480072\n",
            "Epoch : 639 \t Train loss : 0.06538790464401245 \t Test loss : 0.09143796563148499\n",
            "Epoch : 640 \t Train loss : 0.06520721316337585 \t Test loss : 0.09187039732933044\n",
            "Epoch : 641 \t Train loss : 0.06502653658390045 \t Test loss : 0.09230281412601471\n",
            "Epoch : 642 \t Train loss : 0.06484584510326385 \t Test loss : 0.09273524582386017\n",
            "Epoch : 643 \t Train loss : 0.06466763466596603 \t Test loss : 0.09312976896762848\n",
            "Epoch : 644 \t Train loss : 0.06450760364532471 \t Test loss : 0.09352435916662216\n",
            "Epoch : 645 \t Train loss : 0.06434755772352219 \t Test loss : 0.09391893446445465\n",
            "Epoch : 646 \t Train loss : 0.06418751180171967 \t Test loss : 0.09431348741054535\n",
            "Epoch : 647 \t Train loss : 0.06402748823165894 \t Test loss : 0.09470802545547485\n",
            "Epoch : 648 \t Train loss : 0.06386743485927582 \t Test loss : 0.09510260075330734\n",
            "Epoch : 649 \t Train loss : 0.06370741128921509 \t Test loss : 0.09549716860055923\n",
            "Epoch : 650 \t Train loss : 0.06355012953281403 \t Test loss : 0.09585398435592651\n",
            "Epoch : 651 \t Train loss : 0.06340909749269485 \t Test loss : 0.09621085971593857\n",
            "Epoch : 652 \t Train loss : 0.06326808780431747 \t Test loss : 0.09656772762537003\n",
            "Epoch : 653 \t Train loss : 0.0631270632147789 \t Test loss : 0.0969245582818985\n",
            "Epoch : 654 \t Train loss : 0.06298605352640152 \t Test loss : 0.09728140383958817\n",
            "Epoch : 655 \t Train loss : 0.06284503638744354 \t Test loss : 0.09763824194669724\n",
            "Epoch : 656 \t Train loss : 0.06270402669906616 \t Test loss : 0.0979950949549675\n",
            "Epoch : 657 \t Train loss : 0.06256300956010818 \t Test loss : 0.09835194051265717\n",
            "Epoch : 658 \t Train loss : 0.06242842227220535 \t Test loss : 0.09867141395807266\n",
            "Epoch : 659 \t Train loss : 0.0623047836124897 \t Test loss : 0.09899088740348816\n",
            "Epoch : 660 \t Train loss : 0.06218114495277405 \t Test loss : 0.09931034594774246\n",
            "Epoch : 661 \t Train loss : 0.062057506293058395 \t Test loss : 0.09962981194257736\n",
            "Epoch : 662 \t Train loss : 0.061933863908052444 \t Test loss : 0.09994928538799286\n",
            "Epoch : 663 \t Train loss : 0.06181022524833679 \t Test loss : 0.10026874393224716\n",
            "Epoch : 664 \t Train loss : 0.061686594039201736 \t Test loss : 0.10058820247650146\n",
            "Epoch : 665 \t Train loss : 0.06156294792890549 \t Test loss : 0.10090766847133636\n",
            "Epoch : 666 \t Train loss : 0.061443012207746506 \t Test loss : 0.10118992626667023\n",
            "Epoch : 667 \t Train loss : 0.061335138976573944 \t Test loss : 0.10147217661142349\n",
            "Epoch : 668 \t Train loss : 0.061227280646562576 \t Test loss : 0.10175442695617676\n",
            "Epoch : 669 \t Train loss : 0.06111941486597061 \t Test loss : 0.10203666985034943\n",
            "Epoch : 670 \t Train loss : 0.06101154536008835 \t Test loss : 0.10231892019510269\n",
            "Epoch : 671 \t Train loss : 0.060903675854206085 \t Test loss : 0.10260117053985596\n",
            "Epoch : 672 \t Train loss : 0.06079580634832382 \t Test loss : 0.10288341343402863\n",
            "Epoch : 673 \t Train loss : 0.060687948018312454 \t Test loss : 0.10316567122936249\n",
            "Epoch : 674 \t Train loss : 0.06058008596301079 \t Test loss : 0.10344789922237396\n",
            "Epoch : 675 \t Train loss : 0.06047438830137253 \t Test loss : 0.1036931499838829\n",
            "Epoch : 676 \t Train loss : 0.060380738228559494 \t Test loss : 0.10393838584423065\n",
            "Epoch : 677 \t Train loss : 0.06028708070516586 \t Test loss : 0.10418365150690079\n",
            "Epoch : 678 \t Train loss : 0.060193419456481934 \t Test loss : 0.10442886501550674\n",
            "Epoch : 679 \t Train loss : 0.060099758207798004 \t Test loss : 0.10467410087585449\n",
            "Epoch : 680 \t Train loss : 0.06000610440969467 \t Test loss : 0.10491935163736343\n",
            "Epoch : 681 \t Train loss : 0.05991245061159134 \t Test loss : 0.10516457259654999\n",
            "Epoch : 682 \t Train loss : 0.05981879308819771 \t Test loss : 0.10540981590747833\n",
            "Epoch : 683 \t Train loss : 0.05972512811422348 \t Test loss : 0.10565505176782608\n",
            "Epoch : 684 \t Train loss : 0.059631478041410446 \t Test loss : 0.10590028762817383\n",
            "Epoch : 685 \t Train loss : 0.059538207948207855 \t Test loss : 0.10610880702733994\n",
            "Epoch : 686 \t Train loss : 0.059457190334796906 \t Test loss : 0.10631730407476425\n",
            "Epoch : 687 \t Train loss : 0.05937615782022476 \t Test loss : 0.10652575641870499\n",
            "Epoch : 688 \t Train loss : 0.05929511785507202 \t Test loss : 0.10673432052135468\n",
            "Epoch : 689 \t Train loss : 0.05921409651637077 \t Test loss : 0.10694281756877899\n",
            "Epoch : 690 \t Train loss : 0.05913306400179863 \t Test loss : 0.1071513444185257\n",
            "Epoch : 691 \t Train loss : 0.05905204266309738 \t Test loss : 0.10735981166362762\n",
            "Epoch : 692 \t Train loss : 0.05897100642323494 \t Test loss : 0.10756833851337433\n",
            "Epoch : 693 \t Train loss : 0.058889973908662796 \t Test loss : 0.10777684301137924\n",
            "Epoch : 694 \t Train loss : 0.058808933943510056 \t Test loss : 0.10798535495996475\n",
            "Epoch : 695 \t Train loss : 0.05872790887951851 \t Test loss : 0.10819385200738907\n",
            "Epoch : 696 \t Train loss : 0.058646880090236664 \t Test loss : 0.10840237140655518\n",
            "Epoch : 697 \t Train loss : 0.05856875702738762 \t Test loss : 0.10857430845499039\n",
            "Epoch : 698 \t Train loss : 0.058498818427324295 \t Test loss : 0.1087462455034256\n",
            "Epoch : 699 \t Train loss : 0.05842888355255127 \t Test loss : 0.10891814529895782\n",
            "Epoch : 700 \t Train loss : 0.05835895612835884 \t Test loss : 0.10909006744623184\n",
            "Epoch : 701 \t Train loss : 0.058289021253585815 \t Test loss : 0.10926198959350586\n",
            "Epoch : 702 \t Train loss : 0.05821908265352249 \t Test loss : 0.10943392664194107\n",
            "Epoch : 703 \t Train loss : 0.058149147778749466 \t Test loss : 0.10960586369037628\n",
            "Epoch : 704 \t Train loss : 0.05807922035455704 \t Test loss : 0.1097777858376503\n",
            "Epoch : 705 \t Train loss : 0.058009278029203415 \t Test loss : 0.10994970798492432\n",
            "Epoch : 706 \t Train loss : 0.05793933942914009 \t Test loss : 0.11012163013219833\n",
            "Epoch : 707 \t Train loss : 0.05786941573023796 \t Test loss : 0.11029358208179474\n",
            "Epoch : 708 \t Train loss : 0.057799480855464935 \t Test loss : 0.11046548932790756\n",
            "Epoch : 709 \t Train loss : 0.05772954225540161 \t Test loss : 0.11063742637634277\n",
            "Epoch : 710 \t Train loss : 0.05766121670603752 \t Test loss : 0.11077304184436798\n",
            "Epoch : 711 \t Train loss : 0.05760083347558975 \t Test loss : 0.11090867221355438\n",
            "Epoch : 712 \t Train loss : 0.05754045769572258 \t Test loss : 0.11104430258274078\n",
            "Epoch : 713 \t Train loss : 0.05748007819056511 \t Test loss : 0.1111799031496048\n",
            "Epoch : 714 \t Train loss : 0.05741969496011734 \t Test loss : 0.11131556332111359\n",
            "Epoch : 715 \t Train loss : 0.05735931545495987 \t Test loss : 0.1114511713385582\n",
            "Epoch : 716 \t Train loss : 0.0572989284992218 \t Test loss : 0.111586794257164\n",
            "Epoch : 717 \t Train loss : 0.057238560169935226 \t Test loss : 0.1117224246263504\n",
            "Epoch : 718 \t Train loss : 0.057178180664777756 \t Test loss : 0.11185803264379501\n",
            "Epoch : 719 \t Train loss : 0.05711778253316879 \t Test loss : 0.11199367046356201\n",
            "Epoch : 720 \t Train loss : 0.057057417929172516 \t Test loss : 0.11212930828332901\n",
            "Epoch : 721 \t Train loss : 0.05699703097343445 \t Test loss : 0.11226493120193481\n",
            "Epoch : 722 \t Train loss : 0.056936658918857574 \t Test loss : 0.11240055412054062\n",
            "Epoch : 723 \t Train loss : 0.05687627196311951 \t Test loss : 0.11253616958856583\n",
            "Epoch : 724 \t Train loss : 0.05681589990854263 \t Test loss : 0.11267179250717163\n",
            "Epoch : 725 \t Train loss : 0.05675552040338516 \t Test loss : 0.11280741542577744\n",
            "Epoch : 726 \t Train loss : 0.056699126958847046 \t Test loss : 0.11290695518255234\n",
            "Epoch : 727 \t Train loss : 0.056646816432476044 \t Test loss : 0.11300649493932724\n",
            "Epoch : 728 \t Train loss : 0.056594498455524445 \t Test loss : 0.11310598999261856\n",
            "Epoch : 729 \t Train loss : 0.056542180478572845 \t Test loss : 0.11320555210113525\n",
            "Epoch : 730 \t Train loss : 0.05648987367749214 \t Test loss : 0.11330509185791016\n",
            "Epoch : 731 \t Train loss : 0.05643755942583084 \t Test loss : 0.11340465396642685\n",
            "Epoch : 732 \t Train loss : 0.05638524889945984 \t Test loss : 0.11350415647029877\n",
            "Epoch : 733 \t Train loss : 0.05633293464779854 \t Test loss : 0.11360368877649307\n",
            "Epoch : 734 \t Train loss : 0.05628061294555664 \t Test loss : 0.11370322853326797\n",
            "Epoch : 735 \t Train loss : 0.056228309869766235 \t Test loss : 0.11380274593830109\n",
            "Epoch : 736 \t Train loss : 0.05617598816752434 \t Test loss : 0.11390228569507599\n",
            "Epoch : 737 \t Train loss : 0.05612367391586304 \t Test loss : 0.11400182545185089\n",
            "Epoch : 738 \t Train loss : 0.056071363389492035 \t Test loss : 0.1141013503074646\n",
            "Epoch : 739 \t Train loss : 0.056019049137830734 \t Test loss : 0.1142008900642395\n",
            "Epoch : 740 \t Train loss : 0.055966734886169434 \t Test loss : 0.1143004298210144\n",
            "Epoch : 741 \t Train loss : 0.05591442063450813 \t Test loss : 0.11439995467662811\n",
            "Epoch : 742 \t Train loss : 0.055862098932266235 \t Test loss : 0.11449948698282242\n",
            "Epoch : 743 \t Train loss : 0.05580979585647583 \t Test loss : 0.11459902673959732\n",
            "Epoch : 744 \t Train loss : 0.05575961992144585 \t Test loss : 0.11466272920370102\n",
            "Epoch : 745 \t Train loss : 0.05571388080716133 \t Test loss : 0.11472634226083755\n",
            "Epoch : 746 \t Train loss : 0.05566810816526413 \t Test loss : 0.11479000747203827\n",
            "Epoch : 747 \t Train loss : 0.05562235787510872 \t Test loss : 0.11485366523265839\n",
            "Epoch : 748 \t Train loss : 0.05557660013437271 \t Test loss : 0.1149173229932785\n",
            "Epoch : 749 \t Train loss : 0.055530838668346405 \t Test loss : 0.11498099565505981\n",
            "Epoch : 750 \t Train loss : 0.0554850809276104 \t Test loss : 0.11504466831684113\n",
            "Epoch : 751 \t Train loss : 0.05543933063745499 \t Test loss : 0.11510832607746124\n",
            "Epoch : 752 \t Train loss : 0.05539357662200928 \t Test loss : 0.11517199128866196\n",
            "Epoch : 753 \t Train loss : 0.05534781888127327 \t Test loss : 0.11523566395044327\n",
            "Epoch : 754 \t Train loss : 0.05530206114053726 \t Test loss : 0.1152992993593216\n",
            "Epoch : 755 \t Train loss : 0.055256299674510956 \t Test loss : 0.11536296457052231\n",
            "Epoch : 756 \t Train loss : 0.05521053820848465 \t Test loss : 0.11542663723230362\n",
            "Epoch : 757 \t Train loss : 0.05516479164361954 \t Test loss : 0.11549029499292374\n",
            "Epoch : 758 \t Train loss : 0.05511903017759323 \t Test loss : 0.11555395275354385\n",
            "Epoch : 759 \t Train loss : 0.055073272436857224 \t Test loss : 0.11561763286590576\n",
            "Epoch : 760 \t Train loss : 0.055027514696121216 \t Test loss : 0.11568129062652588\n",
            "Epoch : 761 \t Train loss : 0.05498175695538521 \t Test loss : 0.115744948387146\n",
            "Epoch : 762 \t Train loss : 0.0549360029399395 \t Test loss : 0.11580860614776611\n",
            "Epoch : 763 \t Train loss : 0.05489024519920349 \t Test loss : 0.11587227880954742\n",
            "Epoch : 764 \t Train loss : 0.054844487458467484 \t Test loss : 0.11593594402074814\n",
            "Epoch : 765 \t Train loss : 0.054798729717731476 \t Test loss : 0.11599960178136826\n",
            "Epoch : 766 \t Train loss : 0.05475296825170517 \t Test loss : 0.11606325954198837\n",
            "Epoch : 767 \t Train loss : 0.05470937490463257 \t Test loss : 0.11609132587909698\n",
            "Epoch : 768 \t Train loss : 0.054668717086315155 \t Test loss : 0.116119384765625\n",
            "Epoch : 769 \t Train loss : 0.054628051817417145 \t Test loss : 0.11614744365215302\n",
            "Epoch : 770 \t Train loss : 0.054587386548519135 \t Test loss : 0.11617550998926163\n",
            "Epoch : 771 \t Train loss : 0.05454672500491142 \t Test loss : 0.11620356142520905\n",
            "Epoch : 772 \t Train loss : 0.05450606346130371 \t Test loss : 0.11623163521289825\n",
            "Epoch : 773 \t Train loss : 0.054465401917696 \t Test loss : 0.11625969409942627\n",
            "Epoch : 774 \t Train loss : 0.05442473292350769 \t Test loss : 0.11628775298595428\n",
            "Epoch : 775 \t Train loss : 0.05438407137989998 \t Test loss : 0.1163158193230629\n",
            "Epoch : 776 \t Train loss : 0.05434340983629227 \t Test loss : 0.11634387075901031\n",
            "Epoch : 777 \t Train loss : 0.05430274456739426 \t Test loss : 0.11637194454669952\n",
            "Epoch : 778 \t Train loss : 0.054262083023786545 \t Test loss : 0.11639998108148575\n",
            "Epoch : 779 \t Train loss : 0.054221414029598236 \t Test loss : 0.11642811447381973\n",
            "Epoch : 780 \t Train loss : 0.05418076366186142 \t Test loss : 0.11645610630512238\n",
            "Epoch : 781 \t Train loss : 0.05414009094238281 \t Test loss : 0.11648418754339218\n",
            "Epoch : 782 \t Train loss : 0.0540994293987751 \t Test loss : 0.1165122538805008\n",
            "Epoch : 783 \t Train loss : 0.05405876785516739 \t Test loss : 0.11654032766819\n",
            "Epoch : 784 \t Train loss : 0.05401810258626938 \t Test loss : 0.11656836420297623\n",
            "Epoch : 785 \t Train loss : 0.05397743731737137 \t Test loss : 0.11659642308950424\n",
            "Epoch : 786 \t Train loss : 0.05393676832318306 \t Test loss : 0.11662449687719345\n",
            "Epoch : 787 \t Train loss : 0.053896110504865646 \t Test loss : 0.11665254831314087\n",
            "Epoch : 788 \t Train loss : 0.05385544151067734 \t Test loss : 0.11668062210083008\n",
            "Epoch : 789 \t Train loss : 0.053814779967069626 \t Test loss : 0.1167086735367775\n",
            "Epoch : 790 \t Train loss : 0.05377412587404251 \t Test loss : 0.1167367473244667\n",
            "Epoch : 791 \t Train loss : 0.0537334568798542 \t Test loss : 0.11676480621099472\n",
            "Epoch : 792 \t Train loss : 0.053692787885665894 \t Test loss : 0.11679287254810333\n",
            "Epoch : 793 \t Train loss : 0.05365212634205818 \t Test loss : 0.11682093143463135\n",
            "Epoch : 794 \t Train loss : 0.05361146852374077 \t Test loss : 0.11684898287057877\n",
            "Epoch : 795 \t Train loss : 0.05357079952955246 \t Test loss : 0.11687705665826797\n",
            "Epoch : 796 \t Train loss : 0.05353013798594475 \t Test loss : 0.11690511554479599\n",
            "Epoch : 797 \t Train loss : 0.0534910149872303 \t Test loss : 0.11689772456884384\n",
            "Epoch : 798 \t Train loss : 0.053453993052244186 \t Test loss : 0.11689035594463348\n",
            "Epoch : 799 \t Train loss : 0.05341697856783867 \t Test loss : 0.11688295751810074\n",
            "Epoch : 800 \t Train loss : 0.05337996035814285 \t Test loss : 0.11687557399272919\n",
            "Epoch : 801 \t Train loss : 0.053342945873737335 \t Test loss : 0.11686818301677704\n",
            "Epoch : 802 \t Train loss : 0.05330593138933182 \t Test loss : 0.11686079204082489\n",
            "Epoch : 803 \t Train loss : 0.0532689094543457 \t Test loss : 0.11685340106487274\n",
            "Epoch : 804 \t Train loss : 0.053231894969940186 \t Test loss : 0.11684603989124298\n",
            "Epoch : 805 \t Train loss : 0.053194887936115265 \t Test loss : 0.11683865636587143\n",
            "Epoch : 806 \t Train loss : 0.05315786600112915 \t Test loss : 0.11683128029108047\n",
            "Epoch : 807 \t Train loss : 0.05312085151672363 \t Test loss : 0.11682388931512833\n",
            "Epoch : 808 \t Train loss : 0.053083837032318115 \t Test loss : 0.11681650578975677\n",
            "Epoch : 809 \t Train loss : 0.0530468225479126 \t Test loss : 0.11680911481380463\n",
            "Epoch : 810 \t Train loss : 0.05300980806350708 \t Test loss : 0.11680171638727188\n",
            "Epoch : 811 \t Train loss : 0.05297279357910156 \t Test loss : 0.11679436266422272\n",
            "Epoch : 812 \t Train loss : 0.05293577164411545 \t Test loss : 0.11678697168827057\n",
            "Epoch : 813 \t Train loss : 0.05289875343441963 \t Test loss : 0.11677958816289902\n",
            "Epoch : 814 \t Train loss : 0.052861738950014114 \t Test loss : 0.11677219718694687\n",
            "Epoch : 815 \t Train loss : 0.052824728190898895 \t Test loss : 0.11676480621099472\n",
            "Epoch : 816 \t Train loss : 0.05278771370649338 \t Test loss : 0.11675745248794556\n",
            "Epoch : 817 \t Train loss : 0.05275069549679756 \t Test loss : 0.11675004661083221\n",
            "Epoch : 818 \t Train loss : 0.052713681012392044 \t Test loss : 0.11674268543720245\n",
            "Epoch : 819 \t Train loss : 0.052676666527986526 \t Test loss : 0.1167352944612503\n",
            "Epoch : 820 \t Train loss : 0.05263964459300041 \t Test loss : 0.11672792583703995\n",
            "Epoch : 821 \t Train loss : 0.052602626383304596 \t Test loss : 0.1167205199599266\n",
            "Epoch : 822 \t Train loss : 0.05256561562418938 \t Test loss : 0.11671314388513565\n",
            "Epoch : 823 \t Train loss : 0.052528589963912964 \t Test loss : 0.1167057603597641\n",
            "Epoch : 824 \t Train loss : 0.05249158293008804 \t Test loss : 0.11669838428497314\n",
            "Epoch : 825 \t Train loss : 0.052454568445682526 \t Test loss : 0.116690993309021\n",
            "Epoch : 826 \t Train loss : 0.052417557686567307 \t Test loss : 0.11668361723423004\n",
            "Epoch : 827 \t Train loss : 0.052380532026290894 \t Test loss : 0.11667623370885849\n",
            "Epoch : 828 \t Train loss : 0.05234352499246597 \t Test loss : 0.11666883528232574\n",
            "Epoch : 829 \t Train loss : 0.05230650305747986 \t Test loss : 0.11666146665811539\n",
            "Epoch : 830 \t Train loss : 0.052269481122493744 \t Test loss : 0.11665406078100204\n",
            "Epoch : 831 \t Train loss : 0.052232466638088226 \t Test loss : 0.11664669215679169\n",
            "Epoch : 832 \t Train loss : 0.052195459604263306 \t Test loss : 0.11663933098316193\n",
            "Epoch : 833 \t Train loss : 0.052158452570438385 \t Test loss : 0.11663191020488739\n",
            "Epoch : 834 \t Train loss : 0.052121423184871674 \t Test loss : 0.11662453413009644\n",
            "Epoch : 835 \t Train loss : 0.052084408700466156 \t Test loss : 0.11661715805530548\n",
            "Epoch : 836 \t Train loss : 0.05204739049077034 \t Test loss : 0.11660976707935333\n",
            "Epoch : 837 \t Train loss : 0.052010368555784225 \t Test loss : 0.11660239845514297\n",
            "Epoch : 838 \t Train loss : 0.051973361521959305 \t Test loss : 0.11659500747919083\n",
            "Epoch : 839 \t Train loss : 0.051936350762844086 \t Test loss : 0.11658763885498047\n",
            "Epoch : 840 \t Train loss : 0.05189932510256767 \t Test loss : 0.11658023297786713\n",
            "Epoch : 841 \t Train loss : 0.05186329036951065 \t Test loss : 0.11653773486614227\n",
            "Epoch : 842 \t Train loss : 0.05182849243283272 \t Test loss : 0.11649525165557861\n",
            "Epoch : 843 \t Train loss : 0.05179368332028389 \t Test loss : 0.11645273864269257\n",
            "Epoch : 844 \t Train loss : 0.05175887793302536 \t Test loss : 0.1164102554321289\n",
            "Epoch : 845 \t Train loss : 0.05172407627105713 \t Test loss : 0.11636774241924286\n",
            "Epoch : 846 \t Train loss : 0.0516892746090889 \t Test loss : 0.1163252741098404\n",
            "Epoch : 847 \t Train loss : 0.051654476672410965 \t Test loss : 0.11628274619579315\n",
            "Epoch : 848 \t Train loss : 0.05161966755986214 \t Test loss : 0.11624026298522949\n",
            "Epoch : 849 \t Train loss : 0.05158486217260361 \t Test loss : 0.11619774997234344\n",
            "Epoch : 850 \t Train loss : 0.05155005306005478 \t Test loss : 0.116155244410038\n",
            "Epoch : 851 \t Train loss : 0.05151524394750595 \t Test loss : 0.11611275374889374\n",
            "Epoch : 852 \t Train loss : 0.05148044973611832 \t Test loss : 0.11607025563716888\n",
            "Epoch : 853 \t Train loss : 0.05144564434885979 \t Test loss : 0.11602775752544403\n",
            "Epoch : 854 \t Train loss : 0.051410842686891556 \t Test loss : 0.11598525941371918\n",
            "Epoch : 855 \t Train loss : 0.05137604475021362 \t Test loss : 0.11594277620315552\n",
            "Epoch : 856 \t Train loss : 0.0513412281870842 \t Test loss : 0.11590026319026947\n",
            "Epoch : 857 \t Train loss : 0.05130642652511597 \t Test loss : 0.11585776507854462\n",
            "Epoch : 858 \t Train loss : 0.051271628588438034 \t Test loss : 0.11581528186798096\n",
            "Epoch : 859 \t Train loss : 0.0512368269264698 \t Test loss : 0.11577276140451431\n",
            "Epoch : 860 \t Train loss : 0.051202017813920975 \t Test loss : 0.11573026329278946\n",
            "Epoch : 861 \t Train loss : 0.05116720870137215 \t Test loss : 0.1156877726316452\n",
            "Epoch : 862 \t Train loss : 0.051132410764694214 \t Test loss : 0.11564526706933975\n",
            "Epoch : 863 \t Train loss : 0.05109759420156479 \t Test loss : 0.1156027764081955\n",
            "Epoch : 864 \t Train loss : 0.05106279253959656 \t Test loss : 0.11556027084589005\n",
            "Epoch : 865 \t Train loss : 0.051027994602918625 \t Test loss : 0.115517757833004\n",
            "Epoch : 866 \t Train loss : 0.0509931854903698 \t Test loss : 0.11547528207302094\n",
            "Epoch : 867 \t Train loss : 0.050958383828401566 \t Test loss : 0.11543278396129608\n",
            "Epoch : 868 \t Train loss : 0.05092357471585274 \t Test loss : 0.11539027839899063\n",
            "Epoch : 869 \t Train loss : 0.050888776779174805 \t Test loss : 0.11534778028726578\n",
            "Epoch : 870 \t Train loss : 0.050853975117206573 \t Test loss : 0.11530528217554092\n",
            "Epoch : 871 \t Train loss : 0.050819169729948044 \t Test loss : 0.11526278406381607\n",
            "Epoch : 872 \t Train loss : 0.050784360617399216 \t Test loss : 0.11522028595209122\n",
            "Epoch : 873 \t Train loss : 0.05074955150485039 \t Test loss : 0.11517784744501114\n",
            "Epoch : 874 \t Train loss : 0.050714779645204544 \t Test loss : 0.11513528972864151\n",
            "Epoch : 875 \t Train loss : 0.05067995935678482 \t Test loss : 0.11509279906749725\n",
            "Epoch : 876 \t Train loss : 0.05064515024423599 \t Test loss : 0.1150502935051918\n",
            "Epoch : 877 \t Train loss : 0.05061034485697746 \t Test loss : 0.11500780284404755\n",
            "Epoch : 878 \t Train loss : 0.05057554319500923 \t Test loss : 0.1149652972817421\n",
            "Epoch : 879 \t Train loss : 0.0505407452583313 \t Test loss : 0.11492279917001724\n",
            "Epoch : 880 \t Train loss : 0.05050593614578247 \t Test loss : 0.11488030105829239\n",
            "Epoch : 881 \t Train loss : 0.05047113448381424 \t Test loss : 0.11483780294656754\n",
            "Epoch : 882 \t Train loss : 0.05043632537126541 \t Test loss : 0.11479530483484268\n",
            "Epoch : 883 \t Train loss : 0.05040151998400688 \t Test loss : 0.11475279182195663\n",
            "Epoch : 884 \t Train loss : 0.05036671832203865 \t Test loss : 0.11471028625965118\n",
            "Epoch : 885 \t Train loss : 0.05033191293478012 \t Test loss : 0.11466781049966812\n",
            "Epoch : 886 \t Train loss : 0.05029710382223129 \t Test loss : 0.11462531238794327\n",
            "Epoch : 887 \t Train loss : 0.05026230961084366 \t Test loss : 0.11458279937505722\n",
            "Epoch : 888 \t Train loss : 0.05022750049829483 \t Test loss : 0.11454031616449356\n",
            "Epoch : 889 \t Train loss : 0.0501927025616169 \t Test loss : 0.11449780315160751\n",
            "Epoch : 890 \t Train loss : 0.05015789344906807 \t Test loss : 0.11445530503988266\n",
            "Epoch : 891 \t Train loss : 0.05012308806180954 \t Test loss : 0.1144128069281578\n",
            "Epoch : 892 \t Train loss : 0.05008828639984131 \t Test loss : 0.11437030136585236\n",
            "Epoch : 893 \t Train loss : 0.050053469836711884 \t Test loss : 0.1143278107047081\n",
            "Epoch : 894 \t Train loss : 0.05001867935061455 \t Test loss : 0.11428532749414444\n",
            "Epoch : 895 \t Train loss : 0.04998387396335602 \t Test loss : 0.11424281448125839\n",
            "Epoch : 896 \t Train loss : 0.04994907230138779 \t Test loss : 0.11420030891895294\n",
            "Epoch : 897 \t Train loss : 0.04991426318883896 \t Test loss : 0.11415781825780869\n",
            "Epoch : 898 \t Train loss : 0.04987946152687073 \t Test loss : 0.11411532014608383\n",
            "Epoch : 899 \t Train loss : 0.0498446561396122 \t Test loss : 0.11407282203435898\n",
            "Epoch : 900 \t Train loss : 0.04980985075235367 \t Test loss : 0.11403033882379532\n",
            "Epoch : 901 \t Train loss : 0.04977504163980484 \t Test loss : 0.11398782581090927\n",
            "Epoch : 902 \t Train loss : 0.049740247428417206 \t Test loss : 0.11394532769918442\n",
            "Epoch : 903 \t Train loss : 0.04970543459057808 \t Test loss : 0.11390282958745956\n",
            "Epoch : 904 \t Train loss : 0.04967064410448074 \t Test loss : 0.11386033147573471\n",
            "Epoch : 905 \t Train loss : 0.04963582381606102 \t Test loss : 0.11381782591342926\n",
            "Epoch : 906 \t Train loss : 0.04960102587938309 \t Test loss : 0.1137753278017044\n",
            "Epoch : 907 \t Train loss : 0.04956622049212456 \t Test loss : 0.11373283714056015\n",
            "Epoch : 908 \t Train loss : 0.049531418830156326 \t Test loss : 0.1136903315782547\n",
            "Epoch : 909 \t Train loss : 0.049496617168188095 \t Test loss : 0.11364783346652985\n",
            "Epoch : 910 \t Train loss : 0.049461811780929565 \t Test loss : 0.11360533535480499\n",
            "Epoch : 911 \t Train loss : 0.049427010118961334 \t Test loss : 0.11356284469366074\n",
            "Epoch : 912 \t Train loss : 0.049392204731702805 \t Test loss : 0.11352033913135529\n",
            "Epoch : 913 \t Train loss : 0.049357399344444275 \t Test loss : 0.11347784847021103\n",
            "Epoch : 914 \t Train loss : 0.049322597682476044 \t Test loss : 0.11343534290790558\n",
            "Epoch : 915 \t Train loss : 0.04928779974579811 \t Test loss : 0.11339282989501953\n",
            "Epoch : 916 \t Train loss : 0.04925299063324928 \t Test loss : 0.11335036903619766\n",
            "Epoch : 917 \t Train loss : 0.049218177795410156 \t Test loss : 0.11330785602331161\n",
            "Epoch : 918 \t Train loss : 0.04918338358402252 \t Test loss : 0.11326533555984497\n",
            "Epoch : 919 \t Train loss : 0.0491485670208931 \t Test loss : 0.11322285234928131\n",
            "Epoch : 920 \t Train loss : 0.04911377280950546 \t Test loss : 0.11318035423755646\n",
            "Epoch : 921 \t Train loss : 0.04907896742224693 \t Test loss : 0.1131378561258316\n",
            "Epoch : 922 \t Train loss : 0.049044169485569 \t Test loss : 0.11309535801410675\n",
            "Epoch : 923 \t Train loss : 0.04900936037302017 \t Test loss : 0.1130528599023819\n",
            "Epoch : 924 \t Train loss : 0.04897525906562805 \t Test loss : 0.1129753440618515\n",
            "Epoch : 925 \t Train loss : 0.048941243439912796 \t Test loss : 0.1128978282213211\n",
            "Epoch : 926 \t Train loss : 0.04890722781419754 \t Test loss : 0.11282031238079071\n",
            "Epoch : 927 \t Train loss : 0.048873208463191986 \t Test loss : 0.11274280399084091\n",
            "Epoch : 928 \t Train loss : 0.04883920028805733 \t Test loss : 0.11266529560089111\n",
            "Epoch : 929 \t Train loss : 0.04880518093705177 \t Test loss : 0.11258778721094131\n",
            "Epoch : 930 \t Train loss : 0.04877116158604622 \t Test loss : 0.11251024901866913\n",
            "Epoch : 931 \t Train loss : 0.04873714596033096 \t Test loss : 0.11243275552988052\n",
            "Epoch : 932 \t Train loss : 0.04870312660932541 \t Test loss : 0.11235525459051132\n",
            "Epoch : 933 \t Train loss : 0.04866911098361015 \t Test loss : 0.11227772384881973\n",
            "Epoch : 934 \t Train loss : 0.0486350953578949 \t Test loss : 0.11220023781061172\n",
            "Epoch : 935 \t Train loss : 0.04860108718276024 \t Test loss : 0.11212272942066193\n",
            "Epoch : 936 \t Train loss : 0.048567064106464386 \t Test loss : 0.11204521358013153\n",
            "Epoch : 937 \t Train loss : 0.04853304848074913 \t Test loss : 0.11196768283843994\n",
            "Epoch : 938 \t Train loss : 0.048499032855033875 \t Test loss : 0.11189017444849014\n",
            "Epoch : 939 \t Train loss : 0.04846501350402832 \t Test loss : 0.11181267350912094\n",
            "Epoch : 940 \t Train loss : 0.04843099042773247 \t Test loss : 0.11173514276742935\n",
            "Epoch : 941 \t Train loss : 0.04839698225259781 \t Test loss : 0.11165763437747955\n",
            "Epoch : 942 \t Train loss : 0.04836297407746315 \t Test loss : 0.11158013343811035\n",
            "Epoch : 943 \t Train loss : 0.048328947275877 \t Test loss : 0.11150260269641876\n",
            "Epoch : 944 \t Train loss : 0.04829493910074234 \t Test loss : 0.11142508685588837\n",
            "Epoch : 945 \t Train loss : 0.04826091602444649 \t Test loss : 0.11134759336709976\n",
            "Epoch : 946 \t Train loss : 0.04822690412402153 \t Test loss : 0.11127008497714996\n",
            "Epoch : 947 \t Train loss : 0.048192888498306274 \t Test loss : 0.11119256168603897\n",
            "Epoch : 948 \t Train loss : 0.04815886914730072 \t Test loss : 0.11111506074666977\n",
            "Epoch : 949 \t Train loss : 0.048124849796295166 \t Test loss : 0.11103751510381699\n",
            "Epoch : 950 \t Train loss : 0.04809084162116051 \t Test loss : 0.11096000671386719\n",
            "Epoch : 951 \t Train loss : 0.04805682227015495 \t Test loss : 0.11088252067565918\n",
            "Epoch : 952 \t Train loss : 0.0480227991938591 \t Test loss : 0.11080501228570938\n",
            "Epoch : 953 \t Train loss : 0.04798878729343414 \t Test loss : 0.1107274740934372\n",
            "Epoch : 954 \t Train loss : 0.04795477166771889 \t Test loss : 0.11064998805522919\n",
            "Epoch : 955 \t Train loss : 0.04792075231671333 \t Test loss : 0.1105724573135376\n",
            "Epoch : 956 \t Train loss : 0.04788673669099808 \t Test loss : 0.1104949489235878\n",
            "Epoch : 957 \t Train loss : 0.04785272106528282 \t Test loss : 0.110417440533638\n",
            "Epoch : 958 \t Train loss : 0.04781869798898697 \t Test loss : 0.1103399395942688\n",
            "Epoch : 959 \t Train loss : 0.04778468236327171 \t Test loss : 0.1102624163031578\n",
            "Epoch : 960 \t Train loss : 0.047750670462846756 \t Test loss : 0.11018490791320801\n",
            "Epoch : 961 \t Train loss : 0.0477166585624218 \t Test loss : 0.11010739952325821\n",
            "Epoch : 962 \t Train loss : 0.047682635486125946 \t Test loss : 0.11002989113330841\n",
            "Epoch : 963 \t Train loss : 0.04764862731099129 \t Test loss : 0.10995239019393921\n",
            "Epoch : 964 \t Train loss : 0.04761459678411484 \t Test loss : 0.10987486690282822\n",
            "Epoch : 965 \t Train loss : 0.04758059233427048 \t Test loss : 0.10979734361171722\n",
            "Epoch : 966 \t Train loss : 0.04754657298326492 \t Test loss : 0.10971983522176743\n",
            "Epoch : 967 \t Train loss : 0.04751255363225937 \t Test loss : 0.10964232683181763\n",
            "Epoch : 968 \t Train loss : 0.04747854173183441 \t Test loss : 0.10956480354070663\n",
            "Epoch : 969 \t Train loss : 0.047444529831409454 \t Test loss : 0.10948729515075684\n",
            "Epoch : 970 \t Train loss : 0.0474105104804039 \t Test loss : 0.10940978676080704\n",
            "Epoch : 971 \t Train loss : 0.04737648367881775 \t Test loss : 0.10933227837085724\n",
            "Epoch : 972 \t Train loss : 0.047342486679553986 \t Test loss : 0.10925476253032684\n",
            "Epoch : 973 \t Train loss : 0.047308456152677536 \t Test loss : 0.10917724668979645\n",
            "Epoch : 974 \t Train loss : 0.04727444052696228 \t Test loss : 0.10909974575042725\n",
            "Epoch : 975 \t Train loss : 0.04724042862653732 \t Test loss : 0.10902221500873566\n",
            "Epoch : 976 \t Train loss : 0.04720640927553177 \t Test loss : 0.10894469916820526\n",
            "Epoch : 977 \t Train loss : 0.04717239737510681 \t Test loss : 0.10886721312999725\n",
            "Epoch : 978 \t Train loss : 0.04713837802410126 \t Test loss : 0.10878970474004745\n",
            "Epoch : 979 \t Train loss : 0.0471043661236763 \t Test loss : 0.10871217399835587\n",
            "Epoch : 980 \t Train loss : 0.047070346772670746 \t Test loss : 0.10863467305898666\n",
            "Epoch : 981 \t Train loss : 0.04703633114695549 \t Test loss : 0.10855714976787567\n",
            "Epoch : 982 \t Train loss : 0.04700232297182083 \t Test loss : 0.10847964137792587\n",
            "Epoch : 983 \t Train loss : 0.04696829989552498 \t Test loss : 0.10840213298797607\n",
            "Epoch : 984 \t Train loss : 0.046934276819229126 \t Test loss : 0.10832462459802628\n",
            "Epoch : 985 \t Train loss : 0.04690025746822357 \t Test loss : 0.10824711620807648\n",
            "Epoch : 986 \t Train loss : 0.046866245567798615 \t Test loss : 0.10816959291696548\n",
            "Epoch : 987 \t Train loss : 0.04683221876621246 \t Test loss : 0.10809209197759628\n",
            "Epoch : 988 \t Train loss : 0.046798206865787506 \t Test loss : 0.1080145612359047\n",
            "Epoch : 989 \t Train loss : 0.04676419496536255 \t Test loss : 0.10793706029653549\n",
            "Epoch : 990 \t Train loss : 0.04673018306493759 \t Test loss : 0.1078595370054245\n",
            "Epoch : 991 \t Train loss : 0.04669616371393204 \t Test loss : 0.1077820435166359\n",
            "Epoch : 992 \t Train loss : 0.04666214436292648 \t Test loss : 0.1077045202255249\n",
            "Epoch : 993 \t Train loss : 0.046628136187791824 \t Test loss : 0.1076270118355751\n",
            "Epoch : 994 \t Train loss : 0.04659411311149597 \t Test loss : 0.1075495034456253\n",
            "Epoch : 995 \t Train loss : 0.046560101211071014 \t Test loss : 0.10747198760509491\n",
            "Epoch : 996 \t Train loss : 0.04652608186006546 \t Test loss : 0.10739447176456451\n",
            "Epoch : 997 \t Train loss : 0.046492066234350204 \t Test loss : 0.10731697082519531\n",
            "Epoch : 998 \t Train loss : 0.04645805060863495 \t Test loss : 0.10723946243524551\n",
            "Epoch : 999 \t Train loss : 0.046424031257629395 \t Test loss : 0.10716193914413452\n",
            "Epoch : 1000 \t Train loss : 0.04639001935720444 \t Test loss : 0.10708443075418472\n",
            "Epoch : 1001 \t Train loss : 0.04635600000619888 \t Test loss : 0.10700692981481552\n",
            "Epoch : 1002 \t Train loss : 0.04632198438048363 \t Test loss : 0.10692939907312393\n",
            "Epoch : 1003 \t Train loss : 0.04628797248005867 \t Test loss : 0.10685189068317413\n",
            "Epoch : 1004 \t Train loss : 0.046253956854343414 \t Test loss : 0.10677437484264374\n",
            "Epoch : 1005 \t Train loss : 0.04621993377804756 \t Test loss : 0.10669686645269394\n",
            "Epoch : 1006 \t Train loss : 0.046185918152332306 \t Test loss : 0.10661935806274414\n",
            "Epoch : 1007 \t Train loss : 0.04615189880132675 \t Test loss : 0.10654184967279434\n",
            "Epoch : 1008 \t Train loss : 0.046117886900901794 \t Test loss : 0.10646434128284454\n",
            "Epoch : 1009 \t Train loss : 0.04608387127518654 \t Test loss : 0.10638681799173355\n",
            "Epoch : 1010 \t Train loss : 0.04604985564947128 \t Test loss : 0.10630931705236435\n",
            "Epoch : 1011 \t Train loss : 0.04601583629846573 \t Test loss : 0.10623180866241455\n",
            "Epoch : 1012 \t Train loss : 0.04598182439804077 \t Test loss : 0.10615428537130356\n",
            "Epoch : 1013 \t Train loss : 0.04594780504703522 \t Test loss : 0.10607676208019257\n",
            "Epoch : 1014 \t Train loss : 0.04591378569602966 \t Test loss : 0.10599925369024277\n",
            "Epoch : 1015 \t Train loss : 0.045879773795604706 \t Test loss : 0.10592174530029297\n",
            "Epoch : 1016 \t Train loss : 0.04584575444459915 \t Test loss : 0.10584423691034317\n",
            "Epoch : 1017 \t Train loss : 0.045811742544174194 \t Test loss : 0.10576672852039337\n",
            "Epoch : 1018 \t Train loss : 0.04577772691845894 \t Test loss : 0.10568919032812119\n",
            "Epoch : 1019 \t Train loss : 0.045743703842163086 \t Test loss : 0.10561170428991318\n",
            "Epoch : 1020 \t Train loss : 0.04570969194173813 \t Test loss : 0.10553419589996338\n",
            "Epoch : 1021 \t Train loss : 0.045675672590732574 \t Test loss : 0.10545668751001358\n",
            "Epoch : 1022 \t Train loss : 0.04564165696501732 \t Test loss : 0.10537917912006378\n",
            "Epoch : 1023 \t Train loss : 0.045607637614011765 \t Test loss : 0.10530171543359756\n",
            "Epoch : 1024 \t Train loss : 0.0455736443400383 \t Test loss : 0.10522415488958359\n",
            "Epoch : 1025 \t Train loss : 0.04553960636258125 \t Test loss : 0.10514664649963379\n",
            "Epoch : 1026 \t Train loss : 0.045505598187446594 \t Test loss : 0.1050691157579422\n",
            "Epoch : 1027 \t Train loss : 0.04547157511115074 \t Test loss : 0.1049915999174118\n",
            "Epoch : 1028 \t Train loss : 0.045437563210725784 \t Test loss : 0.104914091527462\n",
            "Epoch : 1029 \t Train loss : 0.04540354013442993 \t Test loss : 0.10483656823635101\n",
            "Epoch : 1030 \t Train loss : 0.045369524508714676 \t Test loss : 0.10475907474756241\n",
            "Epoch : 1031 \t Train loss : 0.04533550888299942 \t Test loss : 0.10468156635761261\n",
            "Epoch : 1032 \t Train loss : 0.04530149698257446 \t Test loss : 0.10460405051708221\n",
            "Epoch : 1033 \t Train loss : 0.04526748135685921 \t Test loss : 0.10452654212713242\n",
            "Epoch : 1034 \t Train loss : 0.04523346200585365 \t Test loss : 0.10444901883602142\n",
            "Epoch : 1035 \t Train loss : 0.0451994463801384 \t Test loss : 0.10437150299549103\n",
            "Epoch : 1036 \t Train loss : 0.04516543075442314 \t Test loss : 0.10429398715496063\n",
            "Epoch : 1037 \t Train loss : 0.045131418853998184 \t Test loss : 0.10421647131443024\n",
            "Epoch : 1038 \t Train loss : 0.04509739950299263 \t Test loss : 0.10413894802331924\n",
            "Epoch : 1039 \t Train loss : 0.04506336897611618 \t Test loss : 0.10406146198511124\n",
            "Epoch : 1040 \t Train loss : 0.04502936452627182 \t Test loss : 0.10398395359516144\n",
            "Epoch : 1041 \t Train loss : 0.044995348900556564 \t Test loss : 0.10390643775463104\n",
            "Epoch : 1042 \t Train loss : 0.04496132954955101 \t Test loss : 0.10382892936468124\n",
            "Epoch : 1043 \t Train loss : 0.04492731764912605 \t Test loss : 0.10375139862298965\n",
            "Epoch : 1044 \t Train loss : 0.044893305748701096 \t Test loss : 0.10367391258478165\n",
            "Epoch : 1045 \t Train loss : 0.04485929012298584 \t Test loss : 0.10359637439250946\n",
            "Epoch : 1046 \t Train loss : 0.04482526704668999 \t Test loss : 0.10351888835430145\n",
            "Epoch : 1047 \t Train loss : 0.04479125142097473 \t Test loss : 0.10344137996435165\n",
            "Epoch : 1048 \t Train loss : 0.044757239520549774 \t Test loss : 0.10336387157440186\n",
            "Epoch : 1049 \t Train loss : 0.04472322016954422 \t Test loss : 0.10328634083271027\n",
            "Epoch : 1050 \t Train loss : 0.04468919709324837 \t Test loss : 0.10320883989334106\n",
            "Epoch : 1051 \t Train loss : 0.04465518146753311 \t Test loss : 0.10313131660223007\n",
            "Epoch : 1052 \t Train loss : 0.044621169567108154 \t Test loss : 0.10305380821228027\n",
            "Epoch : 1053 \t Train loss : 0.0445871502161026 \t Test loss : 0.10297627747058868\n",
            "Epoch : 1054 \t Train loss : 0.044553134590387344 \t Test loss : 0.10289877653121948\n",
            "Epoch : 1055 \t Train loss : 0.04451911896467209 \t Test loss : 0.10282126814126968\n",
            "Epoch : 1056 \t Train loss : 0.044485099613666534 \t Test loss : 0.10274376720190048\n",
            "Epoch : 1057 \t Train loss : 0.044451091438531876 \t Test loss : 0.10266625881195068\n",
            "Epoch : 1058 \t Train loss : 0.04441707208752632 \t Test loss : 0.10258875042200089\n",
            "Epoch : 1059 \t Train loss : 0.04438305273652077 \t Test loss : 0.10251124203205109\n",
            "Epoch : 1060 \t Train loss : 0.04434903711080551 \t Test loss : 0.1024337187409401\n",
            "Epoch : 1061 \t Train loss : 0.04431503266096115 \t Test loss : 0.1023561954498291\n",
            "Epoch : 1062 \t Train loss : 0.0442810133099556 \t Test loss : 0.1022786870598793\n",
            "Epoch : 1063 \t Train loss : 0.04424699395895004 \t Test loss : 0.1022011786699295\n",
            "Epoch : 1064 \t Train loss : 0.04421297460794449 \t Test loss : 0.10212365537881851\n",
            "Epoch : 1065 \t Train loss : 0.044178951531648636 \t Test loss : 0.10204614698886871\n",
            "Epoch : 1066 \t Train loss : 0.04414493590593338 \t Test loss : 0.10196864604949951\n",
            "Epoch : 1067 \t Train loss : 0.04411093145608902 \t Test loss : 0.10189113765954971\n",
            "Epoch : 1068 \t Train loss : 0.04407690465450287 \t Test loss : 0.10181362926959991\n",
            "Epoch : 1069 \t Train loss : 0.04404289275407791 \t Test loss : 0.10173610597848892\n",
            "Epoch : 1070 \t Train loss : 0.04400887340307236 \t Test loss : 0.10165858268737793\n",
            "Epoch : 1071 \t Train loss : 0.0439748615026474 \t Test loss : 0.10158108174800873\n",
            "Epoch : 1072 \t Train loss : 0.04394084960222244 \t Test loss : 0.10150357335805893\n",
            "Epoch : 1073 \t Train loss : 0.04390683025121689 \t Test loss : 0.10142605006694794\n",
            "Epoch : 1074 \t Train loss : 0.043872810900211334 \t Test loss : 0.10134854167699814\n",
            "Epoch : 1075 \t Train loss : 0.04383879154920578 \t Test loss : 0.10127105563879013\n",
            "Epoch : 1076 \t Train loss : 0.04380476847290993 \t Test loss : 0.10119352489709854\n",
            "Epoch : 1077 \t Train loss : 0.04377075657248497 \t Test loss : 0.10111600160598755\n",
            "Epoch : 1078 \t Train loss : 0.04373674467206001 \t Test loss : 0.10103850066661835\n",
            "Epoch : 1079 \t Train loss : 0.04370272532105446 \t Test loss : 0.10096098482608795\n",
            "Epoch : 1080 \t Train loss : 0.0436687096953392 \t Test loss : 0.10088346898555756\n",
            "Epoch : 1081 \t Train loss : 0.043634701520204544 \t Test loss : 0.10080595314502716\n",
            "Epoch : 1082 \t Train loss : 0.04360067471861839 \t Test loss : 0.10072846710681915\n",
            "Epoch : 1083 \t Train loss : 0.04356665536761284 \t Test loss : 0.10065092891454697\n",
            "Epoch : 1084 \t Train loss : 0.04353264719247818 \t Test loss : 0.10057344287633896\n",
            "Epoch : 1085 \t Train loss : 0.043498627841472626 \t Test loss : 0.10049591213464737\n",
            "Epoch : 1086 \t Train loss : 0.04346460849046707 \t Test loss : 0.10041840374469757\n",
            "Epoch : 1087 \t Train loss : 0.043430592864751816 \t Test loss : 0.10034088790416718\n",
            "Epoch : 1088 \t Train loss : 0.04339658468961716 \t Test loss : 0.10026337951421738\n",
            "Epoch : 1089 \t Train loss : 0.0433625653386116 \t Test loss : 0.10018587112426758\n",
            "Epoch : 1090 \t Train loss : 0.04332854598760605 \t Test loss : 0.10010834783315659\n",
            "Epoch : 1091 \t Train loss : 0.043294526636600494 \t Test loss : 0.1000308245420456\n",
            "Epoch : 1092 \t Train loss : 0.043260518461465836 \t Test loss : 0.09995333850383759\n",
            "Epoch : 1093 \t Train loss : 0.04322649911046028 \t Test loss : 0.09987581521272659\n",
            "Epoch : 1094 \t Train loss : 0.04319247603416443 \t Test loss : 0.0997983068227768\n",
            "Epoch : 1095 \t Train loss : 0.04315846413373947 \t Test loss : 0.099720798432827\n",
            "Epoch : 1096 \t Train loss : 0.043124448508024216 \t Test loss : 0.099643275141716\n",
            "Epoch : 1097 \t Train loss : 0.04309043288230896 \t Test loss : 0.09956572204828262\n",
            "Epoch : 1098 \t Train loss : 0.04305640980601311 \t Test loss : 0.09948824346065521\n",
            "Epoch : 1099 \t Train loss : 0.04302240163087845 \t Test loss : 0.09941074252128601\n",
            "Epoch : 1100 \t Train loss : 0.04298838600516319 \t Test loss : 0.09933322668075562\n",
            "Epoch : 1101 \t Train loss : 0.04295436665415764 \t Test loss : 0.09925572574138641\n",
            "Epoch : 1102 \t Train loss : 0.04292035102844238 \t Test loss : 0.09917820245027542\n",
            "Epoch : 1103 \t Train loss : 0.04288633167743683 \t Test loss : 0.09910070151090622\n",
            "Epoch : 1104 \t Train loss : 0.04285232350230217 \t Test loss : 0.09902318567037582\n",
            "Epoch : 1105 \t Train loss : 0.04281830042600632 \t Test loss : 0.09894566982984543\n",
            "Epoch : 1106 \t Train loss : 0.04278428107500076 \t Test loss : 0.09886815398931503\n",
            "Epoch : 1107 \t Train loss : 0.04275026172399521 \t Test loss : 0.09879066050052643\n",
            "Epoch : 1108 \t Train loss : 0.04271624609827995 \t Test loss : 0.09871313720941544\n",
            "Epoch : 1109 \t Train loss : 0.042682237923145294 \t Test loss : 0.09863562881946564\n",
            "Epoch : 1110 \t Train loss : 0.04264821857213974 \t Test loss : 0.09855811297893524\n",
            "Epoch : 1111 \t Train loss : 0.042614202946424484 \t Test loss : 0.09848061203956604\n",
            "Epoch : 1112 \t Train loss : 0.04258019104599953 \t Test loss : 0.09840307384729385\n",
            "Epoch : 1113 \t Train loss : 0.042546190321445465 \t Test loss : 0.09832557290792465\n",
            "Epoch : 1114 \t Train loss : 0.042512159794569016 \t Test loss : 0.09824806451797485\n",
            "Epoch : 1115 \t Train loss : 0.04247814044356346 \t Test loss : 0.09817056357860565\n",
            "Epoch : 1116 \t Train loss : 0.04244411736726761 \t Test loss : 0.09809304773807526\n",
            "Epoch : 1117 \t Train loss : 0.04241011291742325 \t Test loss : 0.09801553189754486\n",
            "Epoch : 1118 \t Train loss : 0.042376089841127396 \t Test loss : 0.09793802350759506\n",
            "Epoch : 1119 \t Train loss : 0.04234207421541214 \t Test loss : 0.09786050021648407\n",
            "Epoch : 1120 \t Train loss : 0.042308054864406586 \t Test loss : 0.09778299182653427\n",
            "Epoch : 1121 \t Train loss : 0.04227403923869133 \t Test loss : 0.09770546853542328\n",
            "Epoch : 1122 \t Train loss : 0.042240019887685776 \t Test loss : 0.09762796759605408\n",
            "Epoch : 1123 \t Train loss : 0.04220601171255112 \t Test loss : 0.09755045920610428\n",
            "Epoch : 1124 \t Train loss : 0.04217199236154556 \t Test loss : 0.09747293591499329\n",
            "Epoch : 1125 \t Train loss : 0.042137980461120605 \t Test loss : 0.09739542752504349\n",
            "Epoch : 1126 \t Train loss : 0.04210396111011505 \t Test loss : 0.09731792658567429\n",
            "Epoch : 1127 \t Train loss : 0.042069945484399796 \t Test loss : 0.09724042564630508\n",
            "Epoch : 1128 \t Train loss : 0.04203592985868454 \t Test loss : 0.09716290980577469\n",
            "Epoch : 1129 \t Train loss : 0.042001910507678986 \t Test loss : 0.0970853865146637\n",
            "Epoch : 1130 \t Train loss : 0.04196789488196373 \t Test loss : 0.0970078781247139\n",
            "Epoch : 1131 \t Train loss : 0.041933879256248474 \t Test loss : 0.0969303622841835\n",
            "Epoch : 1132 \t Train loss : 0.04189985990524292 \t Test loss : 0.0968528464436531\n",
            "Epoch : 1133 \t Train loss : 0.041865844279527664 \t Test loss : 0.09677533805370331\n",
            "Epoch : 1134 \t Train loss : 0.04183182865381241 \t Test loss : 0.0966978445649147\n",
            "Epoch : 1135 \t Train loss : 0.041797809302806854 \t Test loss : 0.09662030637264252\n",
            "Epoch : 1136 \t Train loss : 0.0417637974023819 \t Test loss : 0.09654280543327332\n",
            "Epoch : 1137 \t Train loss : 0.04172977805137634 \t Test loss : 0.09646529704332352\n",
            "Epoch : 1138 \t Train loss : 0.041695766150951385 \t Test loss : 0.09638778865337372\n",
            "Epoch : 1139 \t Train loss : 0.04166174679994583 \t Test loss : 0.09631027281284332\n",
            "Epoch : 1140 \t Train loss : 0.04162772744894028 \t Test loss : 0.09623275697231293\n",
            "Epoch : 1141 \t Train loss : 0.04159371554851532 \t Test loss : 0.09615524113178253\n",
            "Epoch : 1142 \t Train loss : 0.04155970364809036 \t Test loss : 0.09607771784067154\n",
            "Epoch : 1143 \t Train loss : 0.04152568429708481 \t Test loss : 0.09600021690130234\n",
            "Epoch : 1144 \t Train loss : 0.04149166867136955 \t Test loss : 0.09592270851135254\n",
            "Epoch : 1145 \t Train loss : 0.0414576381444931 \t Test loss : 0.09584520012140274\n",
            "Epoch : 1146 \t Train loss : 0.041423629969358444 \t Test loss : 0.09576767683029175\n",
            "Epoch : 1147 \t Train loss : 0.04138961434364319 \t Test loss : 0.09569017589092255\n",
            "Epoch : 1148 \t Train loss : 0.041355594992637634 \t Test loss : 0.09561266005039215\n",
            "Epoch : 1149 \t Train loss : 0.04132158309221268 \t Test loss : 0.09553515166044235\n",
            "Epoch : 1150 \t Train loss : 0.04128757491707802 \t Test loss : 0.09545762836933136\n",
            "Epoch : 1151 \t Train loss : 0.041253551840782166 \t Test loss : 0.09538012742996216\n",
            "Epoch : 1152 \t Train loss : 0.04121953621506691 \t Test loss : 0.09530260413885117\n",
            "Epoch : 1153 \t Train loss : 0.041185520589351654 \t Test loss : 0.09522509574890137\n",
            "Epoch : 1154 \t Train loss : 0.0411515012383461 \t Test loss : 0.09514759480953217\n",
            "Epoch : 1155 \t Train loss : 0.04111748933792114 \t Test loss : 0.09507006406784058\n",
            "Epoch : 1156 \t Train loss : 0.04108347371220589 \t Test loss : 0.09499255567789078\n",
            "Epoch : 1157 \t Train loss : 0.04104945808649063 \t Test loss : 0.09491506218910217\n",
            "Epoch : 1158 \t Train loss : 0.04101542755961418 \t Test loss : 0.09483753889799118\n",
            "Epoch : 1159 \t Train loss : 0.04098141938447952 \t Test loss : 0.09476003050804138\n",
            "Epoch : 1160 \t Train loss : 0.040947407484054565 \t Test loss : 0.09468251466751099\n",
            "Epoch : 1161 \t Train loss : 0.04091339185833931 \t Test loss : 0.09460499882698059\n",
            "Epoch : 1162 \t Train loss : 0.040879376232624054 \t Test loss : 0.09452749788761139\n",
            "Epoch : 1163 \t Train loss : 0.0408453568816185 \t Test loss : 0.094449982047081\n",
            "Epoch : 1164 \t Train loss : 0.040811337530612946 \t Test loss : 0.0943724662065506\n",
            "Epoch : 1165 \t Train loss : 0.04077732190489769 \t Test loss : 0.0942949503660202\n",
            "Epoch : 1166 \t Train loss : 0.04074329882860184 \t Test loss : 0.0942174419760704\n",
            "Epoch : 1167 \t Train loss : 0.04070928692817688 \t Test loss : 0.0941399335861206\n",
            "Epoch : 1168 \t Train loss : 0.04067527502775192 \t Test loss : 0.09406241774559021\n",
            "Epoch : 1169 \t Train loss : 0.04064125567674637 \t Test loss : 0.09398490190505981\n",
            "Epoch : 1170 \t Train loss : 0.04060724005103111 \t Test loss : 0.09390740096569061\n",
            "Epoch : 1171 \t Train loss : 0.04057322070002556 \t Test loss : 0.09382988512516022\n",
            "Epoch : 1172 \t Train loss : 0.0405392050743103 \t Test loss : 0.09375236928462982\n",
            "Epoch : 1173 \t Train loss : 0.040505193173885345 \t Test loss : 0.09367485344409943\n",
            "Epoch : 1174 \t Train loss : 0.04047117382287979 \t Test loss : 0.09359734505414963\n",
            "Epoch : 1175 \t Train loss : 0.040437161922454834 \t Test loss : 0.09351983666419983\n",
            "Epoch : 1176 \t Train loss : 0.04040314257144928 \t Test loss : 0.09344233572483063\n",
            "Epoch : 1177 \t Train loss : 0.04036913067102432 \t Test loss : 0.09336481243371964\n",
            "Epoch : 1178 \t Train loss : 0.04033511132001877 \t Test loss : 0.09328730404376984\n",
            "Epoch : 1179 \t Train loss : 0.04030109569430351 \t Test loss : 0.09320978820323944\n",
            "Epoch : 1180 \t Train loss : 0.04026708006858826 \t Test loss : 0.09313227981328964\n",
            "Epoch : 1181 \t Train loss : 0.040233056992292404 \t Test loss : 0.09305474907159805\n",
            "Epoch : 1182 \t Train loss : 0.040199048817157745 \t Test loss : 0.09297724068164825\n",
            "Epoch : 1183 \t Train loss : 0.04016502946615219 \t Test loss : 0.09289975464344025\n",
            "Epoch : 1184 \t Train loss : 0.04013101011514664 \t Test loss : 0.09282223135232925\n",
            "Epoch : 1185 \t Train loss : 0.04009699076414108 \t Test loss : 0.09274472296237946\n",
            "Epoch : 1186 \t Train loss : 0.040062982589006424 \t Test loss : 0.09266719967126846\n",
            "Epoch : 1187 \t Train loss : 0.040028952062129974 \t Test loss : 0.09258969873189926\n",
            "Epoch : 1188 \t Train loss : 0.03999494016170502 \t Test loss : 0.09251217544078827\n",
            "Epoch : 1189 \t Train loss : 0.03996092826128006 \t Test loss : 0.09243467450141907\n",
            "Epoch : 1190 \t Train loss : 0.039926908910274506 \t Test loss : 0.09235714375972748\n",
            "Epoch : 1191 \t Train loss : 0.03989290073513985 \t Test loss : 0.09227965027093887\n",
            "Epoch : 1192 \t Train loss : 0.03985888138413429 \t Test loss : 0.09220214188098907\n",
            "Epoch : 1193 \t Train loss : 0.03982486575841904 \t Test loss : 0.09212461858987808\n",
            "Epoch : 1194 \t Train loss : 0.03979084640741348 \t Test loss : 0.09204710274934769\n",
            "Epoch : 1195 \t Train loss : 0.039756834506988525 \t Test loss : 0.09196960926055908\n",
            "Epoch : 1196 \t Train loss : 0.039722807705402374 \t Test loss : 0.09189207851886749\n",
            "Epoch : 1197 \t Train loss : 0.03968879580497742 \t Test loss : 0.0918145701289177\n",
            "Epoch : 1198 \t Train loss : 0.03965478390455246 \t Test loss : 0.09173706918954849\n",
            "Epoch : 1199 \t Train loss : 0.039620764553546906 \t Test loss : 0.0916595458984375\n",
            "Epoch : 1200 \t Train loss : 0.03958674520254135 \t Test loss : 0.0915820375084877\n",
            "Epoch : 1201 \t Train loss : 0.039552729576826096 \t Test loss : 0.0915045291185379\n",
            "Epoch : 1202 \t Train loss : 0.039518725126981735 \t Test loss : 0.09142700582742691\n",
            "Epoch : 1203 \t Train loss : 0.03948470205068588 \t Test loss : 0.09134949743747711\n",
            "Epoch : 1204 \t Train loss : 0.03945068642497063 \t Test loss : 0.09127198904752731\n",
            "Epoch : 1205 \t Train loss : 0.039416663348674774 \t Test loss : 0.09119446575641632\n",
            "Epoch : 1206 \t Train loss : 0.03938265144824982 \t Test loss : 0.09111697971820831\n",
            "Epoch : 1207 \t Train loss : 0.03934863209724426 \t Test loss : 0.09103945642709732\n",
            "Epoch : 1208 \t Train loss : 0.039314620196819305 \t Test loss : 0.09096195548772812\n",
            "Epoch : 1209 \t Train loss : 0.03928060829639435 \t Test loss : 0.09088443219661713\n",
            "Epoch : 1210 \t Train loss : 0.039246585220098495 \t Test loss : 0.09080691635608673\n",
            "Epoch : 1211 \t Train loss : 0.03921256214380264 \t Test loss : 0.09072940051555634\n",
            "Epoch : 1212 \t Train loss : 0.039178553968667984 \t Test loss : 0.09065189212560654\n",
            "Epoch : 1213 \t Train loss : 0.03914453834295273 \t Test loss : 0.09057437628507614\n",
            "Epoch : 1214 \t Train loss : 0.03911052271723747 \t Test loss : 0.09049687534570694\n",
            "Epoch : 1215 \t Train loss : 0.03907650336623192 \t Test loss : 0.09041935205459595\n",
            "Epoch : 1216 \t Train loss : 0.03904249519109726 \t Test loss : 0.09034185856580734\n",
            "Epoch : 1217 \t Train loss : 0.039008475840091705 \t Test loss : 0.09026433527469635\n",
            "Epoch : 1218 \t Train loss : 0.03897445276379585 \t Test loss : 0.09018681943416595\n",
            "Epoch : 1219 \t Train loss : 0.038940440863370895 \t Test loss : 0.09010930359363556\n",
            "Epoch : 1220 \t Train loss : 0.03890642151236534 \t Test loss : 0.09003179520368576\n",
            "Epoch : 1221 \t Train loss : 0.03887240216135979 \t Test loss : 0.08995427936315536\n",
            "Epoch : 1222 \t Train loss : 0.03883839398622513 \t Test loss : 0.08987677097320557\n",
            "Epoch : 1223 \t Train loss : 0.038804374635219574 \t Test loss : 0.08979926258325577\n",
            "Epoch : 1224 \t Train loss : 0.03877035900950432 \t Test loss : 0.08972174674272537\n",
            "Epoch : 1225 \t Train loss : 0.038736335933208466 \t Test loss : 0.08964423835277557\n",
            "Epoch : 1226 \t Train loss : 0.03870232403278351 \t Test loss : 0.08956672996282578\n",
            "Epoch : 1227 \t Train loss : 0.03866831213235855 \t Test loss : 0.08948922902345657\n",
            "Epoch : 1228 \t Train loss : 0.038634292781353 \t Test loss : 0.08941169828176498\n",
            "Epoch : 1229 \t Train loss : 0.03860027715563774 \t Test loss : 0.08933417499065399\n",
            "Epoch : 1230 \t Train loss : 0.03856625780463219 \t Test loss : 0.08925668150186539\n",
            "Epoch : 1231 \t Train loss : 0.03853223845362663 \t Test loss : 0.08917917311191559\n",
            "Epoch : 1232 \t Train loss : 0.03849823400378227 \t Test loss : 0.0891016498208046\n",
            "Epoch : 1233 \t Train loss : 0.03846421092748642 \t Test loss : 0.0890241414308548\n",
            "Epoch : 1234 \t Train loss : 0.038430195301771164 \t Test loss : 0.0889466255903244\n",
            "Epoch : 1235 \t Train loss : 0.03839617595076561 \t Test loss : 0.0888691172003746\n",
            "Epoch : 1236 \t Train loss : 0.03836216405034065 \t Test loss : 0.08879160135984421\n",
            "Epoch : 1237 \t Train loss : 0.0383281484246254 \t Test loss : 0.08871409296989441\n",
            "Epoch : 1238 \t Train loss : 0.03829412907361984 \t Test loss : 0.08863656222820282\n",
            "Epoch : 1239 \t Train loss : 0.03826011344790459 \t Test loss : 0.08855907618999481\n",
            "Epoch : 1240 \t Train loss : 0.03822609782218933 \t Test loss : 0.08848156034946442\n",
            "Epoch : 1241 \t Train loss : 0.038192082196474075 \t Test loss : 0.08840404450893402\n",
            "Epoch : 1242 \t Train loss : 0.03815806284546852 \t Test loss : 0.08832653611898422\n",
            "Epoch : 1243 \t Train loss : 0.038124047219753265 \t Test loss : 0.08824902027845383\n",
            "Epoch : 1244 \t Train loss : 0.03809003159403801 \t Test loss : 0.08817150443792343\n",
            "Epoch : 1245 \t Train loss : 0.03805601969361305 \t Test loss : 0.08809397369623184\n",
            "Epoch : 1246 \t Train loss : 0.0380220040678978 \t Test loss : 0.08801648765802383\n",
            "Epoch : 1247 \t Train loss : 0.037987980991601944 \t Test loss : 0.08793897926807404\n",
            "Epoch : 1248 \t Train loss : 0.03795396909117699 \t Test loss : 0.08786146342754364\n",
            "Epoch : 1249 \t Train loss : 0.03791994974017143 \t Test loss : 0.08778394758701324\n",
            "Epoch : 1250 \t Train loss : 0.03788592666387558 \t Test loss : 0.08770643174648285\n",
            "Epoch : 1251 \t Train loss : 0.03785191476345062 \t Test loss : 0.08762892335653305\n",
            "Epoch : 1252 \t Train loss : 0.03781789913773537 \t Test loss : 0.08755141496658325\n",
            "Epoch : 1253 \t Train loss : 0.03778388351202011 \t Test loss : 0.08747389167547226\n",
            "Epoch : 1254 \t Train loss : 0.037749867886304855 \t Test loss : 0.08739639818668365\n",
            "Epoch : 1255 \t Train loss : 0.0377158485352993 \t Test loss : 0.08731888234615326\n",
            "Epoch : 1256 \t Train loss : 0.037681836634874344 \t Test loss : 0.08724137395620346\n",
            "Epoch : 1257 \t Train loss : 0.03764781728386879 \t Test loss : 0.08716385811567307\n",
            "Epoch : 1258 \t Train loss : 0.037613801658153534 \t Test loss : 0.08708634227514267\n",
            "Epoch : 1259 \t Train loss : 0.03757978230714798 \t Test loss : 0.08700884133577347\n",
            "Epoch : 1260 \t Train loss : 0.037545766681432724 \t Test loss : 0.08693132549524307\n",
            "Epoch : 1261 \t Train loss : 0.03751176595687866 \t Test loss : 0.08685380965471268\n",
            "Epoch : 1262 \t Train loss : 0.03747773915529251 \t Test loss : 0.08677628636360168\n",
            "Epoch : 1263 \t Train loss : 0.03744371980428696 \t Test loss : 0.08669879287481308\n",
            "Epoch : 1264 \t Train loss : 0.0374097041785717 \t Test loss : 0.08662126958370209\n",
            "Epoch : 1265 \t Train loss : 0.03737568482756615 \t Test loss : 0.08654375374317169\n",
            "Epoch : 1266 \t Train loss : 0.03734166547656059 \t Test loss : 0.0864662379026413\n",
            "Epoch : 1267 \t Train loss : 0.037307657301425934 \t Test loss : 0.0863887369632721\n",
            "Epoch : 1268 \t Train loss : 0.03727363795042038 \t Test loss : 0.0863112211227417\n",
            "Epoch : 1269 \t Train loss : 0.037239622324705124 \t Test loss : 0.0862337127327919\n",
            "Epoch : 1270 \t Train loss : 0.03720559924840927 \t Test loss : 0.0861562117934227\n",
            "Epoch : 1271 \t Train loss : 0.037171587347984314 \t Test loss : 0.0860786885023117\n",
            "Epoch : 1272 \t Train loss : 0.037137579172849655 \t Test loss : 0.08600117266178131\n",
            "Epoch : 1273 \t Train loss : 0.0371035598218441 \t Test loss : 0.08592365682125092\n",
            "Epoch : 1274 \t Train loss : 0.03706954047083855 \t Test loss : 0.08584614843130112\n",
            "Epoch : 1275 \t Train loss : 0.03703552484512329 \t Test loss : 0.0857686772942543\n",
            "Epoch : 1276 \t Train loss : 0.03700149431824684 \t Test loss : 0.08569113910198212\n",
            "Epoch : 1277 \t Train loss : 0.03696749359369278 \t Test loss : 0.08561360836029053\n",
            "Epoch : 1278 \t Train loss : 0.036933477967977524 \t Test loss : 0.08553610742092133\n",
            "Epoch : 1279 \t Train loss : 0.03689945861697197 \t Test loss : 0.08545860648155212\n",
            "Epoch : 1280 \t Train loss : 0.03686545044183731 \t Test loss : 0.08538107573986053\n",
            "Epoch : 1281 \t Train loss : 0.03683143109083176 \t Test loss : 0.08530355989933014\n",
            "Epoch : 1282 \t Train loss : 0.036797408014535904 \t Test loss : 0.08522605150938034\n",
            "Epoch : 1283 \t Train loss : 0.03676338866353035 \t Test loss : 0.08514855057001114\n",
            "Epoch : 1284 \t Train loss : 0.03672937676310539 \t Test loss : 0.08507102727890015\n",
            "Epoch : 1285 \t Train loss : 0.03669535368680954 \t Test loss : 0.08499351888895035\n",
            "Epoch : 1286 \t Train loss : 0.03666134923696518 \t Test loss : 0.08491601049900055\n",
            "Epoch : 1287 \t Train loss : 0.03662732616066933 \t Test loss : 0.08483849465847015\n",
            "Epoch : 1288 \t Train loss : 0.03659331053495407 \t Test loss : 0.08476098626852036\n",
            "Epoch : 1289 \t Train loss : 0.03655930235981941 \t Test loss : 0.08468347042798996\n",
            "Epoch : 1290 \t Train loss : 0.03652527928352356 \t Test loss : 0.08460593968629837\n",
            "Epoch : 1291 \t Train loss : 0.036491259932518005 \t Test loss : 0.08452844619750977\n",
            "Epoch : 1292 \t Train loss : 0.03645724803209305 \t Test loss : 0.08445093780755997\n",
            "Epoch : 1293 \t Train loss : 0.03642323613166809 \t Test loss : 0.08437342941761017\n",
            "Epoch : 1294 \t Train loss : 0.03638921305537224 \t Test loss : 0.08429590612649918\n",
            "Epoch : 1295 \t Train loss : 0.03635520115494728 \t Test loss : 0.08421839028596878\n",
            "Epoch : 1296 \t Train loss : 0.03632118180394173 \t Test loss : 0.08414088189601898\n",
            "Epoch : 1297 \t Train loss : 0.03628716617822647 \t Test loss : 0.08406336605548859\n",
            "Epoch : 1298 \t Train loss : 0.03625314310193062 \t Test loss : 0.08398587256669998\n",
            "Epoch : 1299 \t Train loss : 0.03621913120150566 \t Test loss : 0.08390835672616959\n",
            "Epoch : 1300 \t Train loss : 0.036185119301080704 \t Test loss : 0.0838308334350586\n",
            "Epoch : 1301 \t Train loss : 0.03615109622478485 \t Test loss : 0.0837533250451088\n",
            "Epoch : 1302 \t Train loss : 0.036117084324359894 \t Test loss : 0.083675816655159\n",
            "Epoch : 1303 \t Train loss : 0.03608306869864464 \t Test loss : 0.0835983008146286\n",
            "Epoch : 1304 \t Train loss : 0.036049049347639084 \t Test loss : 0.0835207849740982\n",
            "Epoch : 1305 \t Train loss : 0.03601503372192383 \t Test loss : 0.0834432914853096\n",
            "Epoch : 1306 \t Train loss : 0.03598101809620857 \t Test loss : 0.0833657756447792\n",
            "Epoch : 1307 \t Train loss : 0.03594700247049332 \t Test loss : 0.0832882672548294\n",
            "Epoch : 1308 \t Train loss : 0.03591298311948776 \t Test loss : 0.08321075141429901\n",
            "Epoch : 1309 \t Train loss : 0.03587896749377251 \t Test loss : 0.08313323557376862\n",
            "Epoch : 1310 \t Train loss : 0.03584495559334755 \t Test loss : 0.08305571973323822\n",
            "Epoch : 1311 \t Train loss : 0.035810939967632294 \t Test loss : 0.08297820389270782\n",
            "Epoch : 1312 \t Train loss : 0.03577692061662674 \t Test loss : 0.08290068060159683\n",
            "Epoch : 1313 \t Train loss : 0.035742904990911484 \t Test loss : 0.08282318711280823\n",
            "Epoch : 1314 \t Train loss : 0.035708893090486526 \t Test loss : 0.08274567127227783\n",
            "Epoch : 1315 \t Train loss : 0.03567487373948097 \t Test loss : 0.08266816288232803\n",
            "Epoch : 1316 \t Train loss : 0.03564085811376572 \t Test loss : 0.08259063959121704\n",
            "Epoch : 1317 \t Train loss : 0.03560683876276016 \t Test loss : 0.08251313120126724\n",
            "Epoch : 1318 \t Train loss : 0.03557281941175461 \t Test loss : 0.08243563771247864\n",
            "Epoch : 1319 \t Train loss : 0.03553880378603935 \t Test loss : 0.08235812187194824\n",
            "Epoch : 1320 \t Train loss : 0.0355047881603241 \t Test loss : 0.08228059858083725\n",
            "Epoch : 1321 \t Train loss : 0.03547077625989914 \t Test loss : 0.08220308274030685\n",
            "Epoch : 1322 \t Train loss : 0.035436756908893585 \t Test loss : 0.08212557435035706\n",
            "Epoch : 1323 \t Train loss : 0.03540273755788803 \t Test loss : 0.08204806596040726\n",
            "Epoch : 1324 \t Train loss : 0.035368721932172775 \t Test loss : 0.08197055011987686\n",
            "Epoch : 1325 \t Train loss : 0.03533470630645752 \t Test loss : 0.08189304172992706\n",
            "Epoch : 1326 \t Train loss : 0.035300690680742264 \t Test loss : 0.08181552588939667\n",
            "Epoch : 1327 \t Train loss : 0.03526667505502701 \t Test loss : 0.08173802495002747\n",
            "Epoch : 1328 \t Train loss : 0.03523266315460205 \t Test loss : 0.08166051656007767\n",
            "Epoch : 1329 \t Train loss : 0.0351986438035965 \t Test loss : 0.08158299326896667\n",
            "Epoch : 1330 \t Train loss : 0.03516462445259094 \t Test loss : 0.08150548487901688\n",
            "Epoch : 1331 \t Train loss : 0.035130612552165985 \t Test loss : 0.08142796903848648\n",
            "Epoch : 1332 \t Train loss : 0.03509659320116043 \t Test loss : 0.08135046064853668\n",
            "Epoch : 1333 \t Train loss : 0.03506257385015488 \t Test loss : 0.08127294480800629\n",
            "Epoch : 1334 \t Train loss : 0.03502855822443962 \t Test loss : 0.08119545131921768\n",
            "Epoch : 1335 \t Train loss : 0.03499455004930496 \t Test loss : 0.08111792802810669\n",
            "Epoch : 1336 \t Train loss : 0.03496052697300911 \t Test loss : 0.08104041963815689\n",
            "Epoch : 1337 \t Train loss : 0.034926511347293854 \t Test loss : 0.0809628963470459\n",
            "Epoch : 1338 \t Train loss : 0.0348924957215786 \t Test loss : 0.0808853879570961\n",
            "Epoch : 1339 \t Train loss : 0.034858476370573044 \t Test loss : 0.0808078944683075\n",
            "Epoch : 1340 \t Train loss : 0.03482446074485779 \t Test loss : 0.0807303637266159\n",
            "Epoch : 1341 \t Train loss : 0.03479044511914253 \t Test loss : 0.08065284788608551\n",
            "Epoch : 1342 \t Train loss : 0.03475642949342728 \t Test loss : 0.08057533204555511\n",
            "Epoch : 1343 \t Train loss : 0.03472241014242172 \t Test loss : 0.08049783110618591\n",
            "Epoch : 1344 \t Train loss : 0.034688401967287064 \t Test loss : 0.08042030781507492\n",
            "Epoch : 1345 \t Train loss : 0.03465437889099121 \t Test loss : 0.08034279942512512\n",
            "Epoch : 1346 \t Train loss : 0.034620363265275955 \t Test loss : 0.08026528358459473\n",
            "Epoch : 1347 \t Train loss : 0.0345863476395607 \t Test loss : 0.08018777519464493\n",
            "Epoch : 1348 \t Train loss : 0.034552332013845444 \t Test loss : 0.08011027425527573\n",
            "Epoch : 1349 \t Train loss : 0.03451831266283989 \t Test loss : 0.08003272861242294\n",
            "Epoch : 1350 \t Train loss : 0.03448430448770523 \t Test loss : 0.07995523512363434\n",
            "Epoch : 1351 \t Train loss : 0.034450285136699677 \t Test loss : 0.07987772673368454\n",
            "Epoch : 1352 \t Train loss : 0.03441626951098442 \t Test loss : 0.07980021834373474\n",
            "Epoch : 1353 \t Train loss : 0.03438225015997887 \t Test loss : 0.07972269505262375\n",
            "Epoch : 1354 \t Train loss : 0.03434823453426361 \t Test loss : 0.07964520901441574\n",
            "Epoch : 1355 \t Train loss : 0.034314218908548355 \t Test loss : 0.07956767082214355\n",
            "Epoch : 1356 \t Train loss : 0.0342802032828331 \t Test loss : 0.07949017733335495\n",
            "Epoch : 1357 \t Train loss : 0.034246183931827545 \t Test loss : 0.07941265404224396\n",
            "Epoch : 1358 \t Train loss : 0.03421216458082199 \t Test loss : 0.07933513820171356\n",
            "Epoch : 1359 \t Train loss : 0.034178152680397034 \t Test loss : 0.07925762236118317\n",
            "Epoch : 1360 \t Train loss : 0.03414413332939148 \t Test loss : 0.07918012887239456\n",
            "Epoch : 1361 \t Train loss : 0.03411012142896652 \t Test loss : 0.07910261303186417\n",
            "Epoch : 1362 \t Train loss : 0.03407610207796097 \t Test loss : 0.07902510464191437\n",
            "Epoch : 1363 \t Train loss : 0.03404209017753601 \t Test loss : 0.07894759625196457\n",
            "Epoch : 1364 \t Train loss : 0.03400806710124016 \t Test loss : 0.07887008786201477\n",
            "Epoch : 1365 \t Train loss : 0.0339740514755249 \t Test loss : 0.07879255712032318\n",
            "Epoch : 1366 \t Train loss : 0.03394003584980965 \t Test loss : 0.07871505618095398\n",
            "Epoch : 1367 \t Train loss : 0.03390602394938469 \t Test loss : 0.07863754034042358\n",
            "Epoch : 1368 \t Train loss : 0.033872004598379135 \t Test loss : 0.07856002449989319\n",
            "Epoch : 1369 \t Train loss : 0.03383799269795418 \t Test loss : 0.07848250865936279\n",
            "Epoch : 1370 \t Train loss : 0.033803973346948624 \t Test loss : 0.07840500771999359\n",
            "Epoch : 1371 \t Train loss : 0.03376995027065277 \t Test loss : 0.0783274695277214\n",
            "Epoch : 1372 \t Train loss : 0.03373594954609871 \t Test loss : 0.0782499760389328\n",
            "Epoch : 1373 \t Train loss : 0.03370191901922226 \t Test loss : 0.0781724601984024\n",
            "Epoch : 1374 \t Train loss : 0.0336679108440876 \t Test loss : 0.0780949592590332\n",
            "Epoch : 1375 \t Train loss : 0.033633895218372345 \t Test loss : 0.07801742851734161\n",
            "Epoch : 1376 \t Train loss : 0.033599868416786194 \t Test loss : 0.07793992757797241\n",
            "Epoch : 1377 \t Train loss : 0.03356585651636124 \t Test loss : 0.07786242663860321\n",
            "Epoch : 1378 \t Train loss : 0.03353184089064598 \t Test loss : 0.07778491824865341\n",
            "Epoch : 1379 \t Train loss : 0.033497825264930725 \t Test loss : 0.07770739495754242\n",
            "Epoch : 1380 \t Train loss : 0.03346381336450577 \t Test loss : 0.07762988656759262\n",
            "Epoch : 1381 \t Train loss : 0.033429794013500214 \t Test loss : 0.07755237072706223\n",
            "Epoch : 1382 \t Train loss : 0.033395785838365555 \t Test loss : 0.07747484743595123\n",
            "Epoch : 1383 \t Train loss : 0.0333617627620697 \t Test loss : 0.07739735394716263\n",
            "Epoch : 1384 \t Train loss : 0.03332773968577385 \t Test loss : 0.07731983810663223\n",
            "Epoch : 1385 \t Train loss : 0.033293720334768295 \t Test loss : 0.07724232226610184\n",
            "Epoch : 1386 \t Train loss : 0.033259712159633636 \t Test loss : 0.07716480642557144\n",
            "Epoch : 1387 \t Train loss : 0.03322568163275719 \t Test loss : 0.07708731293678284\n",
            "Epoch : 1388 \t Train loss : 0.03319167345762253 \t Test loss : 0.07700978219509125\n",
            "Epoch : 1389 \t Train loss : 0.03315766528248787 \t Test loss : 0.07693227380514145\n",
            "Epoch : 1390 \t Train loss : 0.033123649656772614 \t Test loss : 0.07685475051403046\n",
            "Epoch : 1391 \t Train loss : 0.03308963030576706 \t Test loss : 0.07677726447582245\n",
            "Epoch : 1392 \t Train loss : 0.033055610954761505 \t Test loss : 0.07669974863529205\n",
            "Epoch : 1393 \t Train loss : 0.03302159905433655 \t Test loss : 0.07662222534418106\n",
            "Epoch : 1394 \t Train loss : 0.032987575978040695 \t Test loss : 0.07654470950365067\n",
            "Epoch : 1395 \t Train loss : 0.032953567802906036 \t Test loss : 0.07646720856428146\n",
            "Epoch : 1396 \t Train loss : 0.032919544726610184 \t Test loss : 0.07638968527317047\n",
            "Epoch : 1397 \t Train loss : 0.03288552910089493 \t Test loss : 0.07631217688322067\n",
            "Epoch : 1398 \t Train loss : 0.03285151720046997 \t Test loss : 0.07623466849327087\n",
            "Epoch : 1399 \t Train loss : 0.03281749412417412 \t Test loss : 0.07615715265274048\n",
            "Epoch : 1400 \t Train loss : 0.03278348594903946 \t Test loss : 0.07607964426279068\n",
            "Epoch : 1401 \t Train loss : 0.032749466598033905 \t Test loss : 0.07600214332342148\n",
            "Epoch : 1402 \t Train loss : 0.03271544724702835 \t Test loss : 0.07592462003231049\n",
            "Epoch : 1403 \t Train loss : 0.032681435346603394 \t Test loss : 0.07584710419178009\n",
            "Epoch : 1404 \t Train loss : 0.03264741599559784 \t Test loss : 0.0757695883512497\n",
            "Epoch : 1405 \t Train loss : 0.032613396644592285 \t Test loss : 0.0756920799612999\n",
            "Epoch : 1406 \t Train loss : 0.03257938474416733 \t Test loss : 0.0756145790219307\n",
            "Epoch : 1407 \t Train loss : 0.032545365393161774 \t Test loss : 0.0755370631814003\n",
            "Epoch : 1408 \t Train loss : 0.03251134976744652 \t Test loss : 0.0754595547914505\n",
            "Epoch : 1409 \t Train loss : 0.03247733414173126 \t Test loss : 0.0753820389509201\n",
            "Epoch : 1410 \t Train loss : 0.032443318516016006 \t Test loss : 0.07530452311038971\n",
            "Epoch : 1411 \t Train loss : 0.03240929916501045 \t Test loss : 0.07522700726985931\n",
            "Epoch : 1412 \t Train loss : 0.0323752835392952 \t Test loss : 0.07514950633049011\n",
            "Epoch : 1413 \t Train loss : 0.03234126791357994 \t Test loss : 0.07507198303937912\n",
            "Epoch : 1414 \t Train loss : 0.03230724856257439 \t Test loss : 0.07499447464942932\n",
            "Epoch : 1415 \t Train loss : 0.03227323666214943 \t Test loss : 0.07491696625947952\n",
            "Epoch : 1416 \t Train loss : 0.032239217311143875 \t Test loss : 0.07483945041894913\n",
            "Epoch : 1417 \t Train loss : 0.03220520541071892 \t Test loss : 0.07476194202899933\n",
            "Epoch : 1418 \t Train loss : 0.032171182334423065 \t Test loss : 0.07468441128730774\n",
            "Epoch : 1419 \t Train loss : 0.03213716670870781 \t Test loss : 0.07460691779851913\n",
            "Epoch : 1420 \t Train loss : 0.032103147357702255 \t Test loss : 0.07452940195798874\n",
            "Epoch : 1421 \t Train loss : 0.032069139182567596 \t Test loss : 0.07445189356803894\n",
            "Epoch : 1422 \t Train loss : 0.03203511983156204 \t Test loss : 0.07437438517808914\n",
            "Epoch : 1423 \t Train loss : 0.03200110048055649 \t Test loss : 0.07429692894220352\n",
            "Epoch : 1424 \t Train loss : 0.03196711093187332 \t Test loss : 0.07421936094760895\n",
            "Epoch : 1425 \t Train loss : 0.03193306922912598 \t Test loss : 0.07414185255765915\n",
            "Epoch : 1426 \t Train loss : 0.03189905732870102 \t Test loss : 0.07406432926654816\n",
            "Epoch : 1427 \t Train loss : 0.031865037977695465 \t Test loss : 0.07398682087659836\n",
            "Epoch : 1428 \t Train loss : 0.03183102607727051 \t Test loss : 0.07390929758548737\n",
            "Epoch : 1429 \t Train loss : 0.031797003000974655 \t Test loss : 0.07383178174495697\n",
            "Epoch : 1430 \t Train loss : 0.0317629911005497 \t Test loss : 0.07375428080558777\n",
            "Epoch : 1431 \t Train loss : 0.03172897547483444 \t Test loss : 0.07367677986621857\n",
            "Epoch : 1432 \t Train loss : 0.031694959849119186 \t Test loss : 0.07359926402568817\n",
            "Epoch : 1433 \t Train loss : 0.03166094422340393 \t Test loss : 0.07352174818515778\n",
            "Epoch : 1434 \t Train loss : 0.03162692114710808 \t Test loss : 0.07344423979520798\n",
            "Epoch : 1435 \t Train loss : 0.03159291297197342 \t Test loss : 0.07336671650409698\n",
            "Epoch : 1436 \t Train loss : 0.031558893620967865 \t Test loss : 0.07328920811414719\n",
            "Epoch : 1437 \t Train loss : 0.03152487799525261 \t Test loss : 0.0732116848230362\n",
            "Epoch : 1438 \t Train loss : 0.031490862369537354 \t Test loss : 0.0731341615319252\n",
            "Epoch : 1439 \t Train loss : 0.0314568355679512 \t Test loss : 0.07305668294429779\n",
            "Epoch : 1440 \t Train loss : 0.03142283111810684 \t Test loss : 0.0729791671037674\n",
            "Epoch : 1441 \t Train loss : 0.03138881176710129 \t Test loss : 0.072901651263237\n",
            "Epoch : 1442 \t Train loss : 0.031354792416095734 \t Test loss : 0.0728241354227066\n",
            "Epoch : 1443 \t Train loss : 0.031320780515670776 \t Test loss : 0.07274661958217621\n",
            "Epoch : 1444 \t Train loss : 0.03128676488995552 \t Test loss : 0.072669118642807\n",
            "Epoch : 1445 \t Train loss : 0.031252749264240265 \t Test loss : 0.07259158790111542\n",
            "Epoch : 1446 \t Train loss : 0.03121873177587986 \t Test loss : 0.07251410186290741\n",
            "Epoch : 1447 \t Train loss : 0.031184712424874306 \t Test loss : 0.07243658602237701\n",
            "Epoch : 1448 \t Train loss : 0.0311506986618042 \t Test loss : 0.07235907018184662\n",
            "Epoch : 1449 \t Train loss : 0.031116683036088943 \t Test loss : 0.07228155434131622\n",
            "Epoch : 1450 \t Train loss : 0.03108266368508339 \t Test loss : 0.07220404595136642\n",
            "Epoch : 1451 \t Train loss : 0.031048646196722984 \t Test loss : 0.07212653011083603\n",
            "Epoch : 1452 \t Train loss : 0.031014632433652878 \t Test loss : 0.07204902172088623\n",
            "Epoch : 1453 \t Train loss : 0.030980613082647324 \t Test loss : 0.07197149842977524\n",
            "Epoch : 1454 \t Train loss : 0.030946597456932068 \t Test loss : 0.07189400494098663\n",
            "Epoch : 1455 \t Train loss : 0.030912581831216812 \t Test loss : 0.07181648164987564\n",
            "Epoch : 1456 \t Train loss : 0.030878562480211258 \t Test loss : 0.07173898071050644\n",
            "Epoch : 1457 \t Train loss : 0.03084454871714115 \t Test loss : 0.07166146486997604\n",
            "Epoch : 1458 \t Train loss : 0.030810534954071045 \t Test loss : 0.07158395648002625\n",
            "Epoch : 1459 \t Train loss : 0.03077651560306549 \t Test loss : 0.07150644063949585\n",
            "Epoch : 1460 \t Train loss : 0.030742499977350235 \t Test loss : 0.07142893224954605\n",
            "Epoch : 1461 \t Train loss : 0.030708491802215576 \t Test loss : 0.07135141640901566\n",
            "Epoch : 1462 \t Train loss : 0.030674468725919724 \t Test loss : 0.07127390056848526\n",
            "Epoch : 1463 \t Train loss : 0.030640456825494766 \t Test loss : 0.07119639217853546\n",
            "Epoch : 1464 \t Train loss : 0.030606437474489212 \t Test loss : 0.07111887633800507\n",
            "Epoch : 1465 \t Train loss : 0.030572418123483658 \t Test loss : 0.07104136049747467\n",
            "Epoch : 1466 \t Train loss : 0.030538400635123253 \t Test loss : 0.07096385210752487\n",
            "Epoch : 1467 \t Train loss : 0.030504394322633743 \t Test loss : 0.07088635116815567\n",
            "Epoch : 1468 \t Train loss : 0.030470367521047592 \t Test loss : 0.07080882787704468\n",
            "Epoch : 1469 \t Train loss : 0.030436355620622635 \t Test loss : 0.07073131203651428\n",
            "Epoch : 1470 \t Train loss : 0.03040233626961708 \t Test loss : 0.07065379619598389\n",
            "Epoch : 1471 \t Train loss : 0.030368324369192123 \t Test loss : 0.07057629525661469\n",
            "Epoch : 1472 \t Train loss : 0.030334308743476868 \t Test loss : 0.07049879431724548\n",
            "Epoch : 1473 \t Train loss : 0.030300289392471313 \t Test loss : 0.0704212635755539\n",
            "Epoch : 1474 \t Train loss : 0.030266273766756058 \t Test loss : 0.0703437551856041\n",
            "Epoch : 1475 \t Train loss : 0.030232256278395653 \t Test loss : 0.07026626914739609\n",
            "Epoch : 1476 \t Train loss : 0.03019823133945465 \t Test loss : 0.0701887384057045\n",
            "Epoch : 1477 \t Train loss : 0.030164221301674843 \t Test loss : 0.0701112151145935\n",
            "Epoch : 1478 \t Train loss : 0.030130211263895035 \t Test loss : 0.0700337141752243\n",
            "Epoch : 1479 \t Train loss : 0.03009619191288948 \t Test loss : 0.06995619833469391\n",
            "Epoch : 1480 \t Train loss : 0.030062174424529076 \t Test loss : 0.06987868249416351\n",
            "Epoch : 1481 \t Train loss : 0.03002815879881382 \t Test loss : 0.06980116665363312\n",
            "Epoch : 1482 \t Train loss : 0.029994139447808266 \t Test loss : 0.06972368061542511\n",
            "Epoch : 1483 \t Train loss : 0.02996012009680271 \t Test loss : 0.06964614987373352\n",
            "Epoch : 1484 \t Train loss : 0.029926110059022903 \t Test loss : 0.06956864148378372\n",
            "Epoch : 1485 \t Train loss : 0.0298920925706625 \t Test loss : 0.06949112564325333\n",
            "Epoch : 1486 \t Train loss : 0.029858073219656944 \t Test loss : 0.06941361725330353\n",
            "Epoch : 1487 \t Train loss : 0.02982405386865139 \t Test loss : 0.06933610141277313\n",
            "Epoch : 1488 \t Train loss : 0.02979004755616188 \t Test loss : 0.06925859302282333\n",
            "Epoch : 1489 \t Train loss : 0.029756028205156326 \t Test loss : 0.06918107718229294\n",
            "Epoch : 1490 \t Train loss : 0.02972201071679592 \t Test loss : 0.06910356134176254\n",
            "Epoch : 1491 \t Train loss : 0.029687989503145218 \t Test loss : 0.06902605295181274\n",
            "Epoch : 1492 \t Train loss : 0.02965397760272026 \t Test loss : 0.06894855201244354\n",
            "Epoch : 1493 \t Train loss : 0.029619961977005005 \t Test loss : 0.06887102872133255\n",
            "Epoch : 1494 \t Train loss : 0.02958594635128975 \t Test loss : 0.06879352033138275\n",
            "Epoch : 1495 \t Train loss : 0.029551932588219643 \t Test loss : 0.06871600449085236\n",
            "Epoch : 1496 \t Train loss : 0.02951791323721409 \t Test loss : 0.06863848865032196\n",
            "Epoch : 1497 \t Train loss : 0.029483899474143982 \t Test loss : 0.06856093555688858\n",
            "Epoch : 1498 \t Train loss : 0.02944987453520298 \t Test loss : 0.06848345696926117\n",
            "Epoch : 1499 \t Train loss : 0.029415864497423172 \t Test loss : 0.06840595602989197\n",
            "Epoch : 1500 \t Train loss : 0.029381852596998215 \t Test loss : 0.06832844018936157\n",
            "Epoch : 1501 \t Train loss : 0.02934783138334751 \t Test loss : 0.06825093179941177\n",
            "Epoch : 1502 \t Train loss : 0.029313813894987106 \t Test loss : 0.06817342340946198\n",
            "Epoch : 1503 \t Train loss : 0.029279794543981552 \t Test loss : 0.06809591501951218\n",
            "Epoch : 1504 \t Train loss : 0.029245788231492043 \t Test loss : 0.06801839917898178\n",
            "Epoch : 1505 \t Train loss : 0.02921176515519619 \t Test loss : 0.06794089823961258\n",
            "Epoch : 1506 \t Train loss : 0.029177749529480934 \t Test loss : 0.06786336749792099\n",
            "Epoch : 1507 \t Train loss : 0.02914372645318508 \t Test loss : 0.06778587400913239\n",
            "Epoch : 1508 \t Train loss : 0.029109710827469826 \t Test loss : 0.06770835071802139\n",
            "Epoch : 1509 \t Train loss : 0.029075700789690018 \t Test loss : 0.0676308423280716\n",
            "Epoch : 1510 \t Train loss : 0.029041683301329613 \t Test loss : 0.0675533264875412\n",
            "Epoch : 1511 \t Train loss : 0.029007667675614357 \t Test loss : 0.0674758180975914\n",
            "Epoch : 1512 \t Train loss : 0.02897365391254425 \t Test loss : 0.06739828735589981\n",
            "Epoch : 1513 \t Train loss : 0.02893965318799019 \t Test loss : 0.0673207938671112\n",
            "Epoch : 1514 \t Train loss : 0.02890561893582344 \t Test loss : 0.067243292927742\n",
            "Epoch : 1515 \t Train loss : 0.028871605172753334 \t Test loss : 0.06716577708721161\n",
            "Epoch : 1516 \t Train loss : 0.02883758209645748 \t Test loss : 0.06708826124668121\n",
            "Epoch : 1517 \t Train loss : 0.028803572058677673 \t Test loss : 0.06701074540615082\n",
            "Epoch : 1518 \t Train loss : 0.02876955270767212 \t Test loss : 0.06693323701620102\n",
            "Epoch : 1519 \t Train loss : 0.028735537081956863 \t Test loss : 0.06685571372509003\n",
            "Epoch : 1520 \t Train loss : 0.02870151959359646 \t Test loss : 0.06677820533514023\n",
            "Epoch : 1521 \t Train loss : 0.028667503967881203 \t Test loss : 0.06670068204402924\n",
            "Epoch : 1522 \t Train loss : 0.02863348461687565 \t Test loss : 0.06662318855524063\n",
            "Epoch : 1523 \t Train loss : 0.028599470853805542 \t Test loss : 0.06654567271471024\n",
            "Epoch : 1524 \t Train loss : 0.028565455228090286 \t Test loss : 0.06646815687417984\n",
            "Epoch : 1525 \t Train loss : 0.02853143773972988 \t Test loss : 0.06639064103364944\n",
            "Epoch : 1526 \t Train loss : 0.028497423976659775 \t Test loss : 0.06631314009428024\n",
            "Epoch : 1527 \t Train loss : 0.02846340835094452 \t Test loss : 0.06623563915491104\n",
            "Epoch : 1528 \t Train loss : 0.028429388999938965 \t Test loss : 0.06615812331438065\n",
            "Epoch : 1529 \t Train loss : 0.02839537523686886 \t Test loss : 0.06608060002326965\n",
            "Epoch : 1530 \t Train loss : 0.028361359611153603 \t Test loss : 0.06600309163331985\n",
            "Epoch : 1531 \t Train loss : 0.028327342122793198 \t Test loss : 0.06592557579278946\n",
            "Epoch : 1532 \t Train loss : 0.028293322771787643 \t Test loss : 0.06584805995225906\n",
            "Epoch : 1533 \t Train loss : 0.028259310871362686 \t Test loss : 0.06577055156230927\n",
            "Epoch : 1534 \t Train loss : 0.02822529338300228 \t Test loss : 0.06569305807352066\n",
            "Epoch : 1535 \t Train loss : 0.028191272169351578 \t Test loss : 0.06561551988124847\n",
            "Epoch : 1536 \t Train loss : 0.02815725840628147 \t Test loss : 0.06553801149129868\n",
            "Epoch : 1537 \t Train loss : 0.028123244643211365 \t Test loss : 0.06546051055192947\n",
            "Epoch : 1538 \t Train loss : 0.02808922901749611 \t Test loss : 0.06538300216197968\n",
            "Epoch : 1539 \t Train loss : 0.028055209666490555 \t Test loss : 0.06530548632144928\n",
            "Epoch : 1540 \t Train loss : 0.02802119217813015 \t Test loss : 0.06522797048091888\n",
            "Epoch : 1541 \t Train loss : 0.027987182140350342 \t Test loss : 0.06515045464038849\n",
            "Epoch : 1542 \t Train loss : 0.027953168377280235 \t Test loss : 0.0650729313492775\n",
            "Epoch : 1543 \t Train loss : 0.027919143438339233 \t Test loss : 0.0649954304099083\n",
            "Epoch : 1544 \t Train loss : 0.027885133400559425 \t Test loss : 0.0649179145693779\n",
            "Epoch : 1545 \t Train loss : 0.02785111404955387 \t Test loss : 0.0648404136300087\n",
            "Epoch : 1546 \t Train loss : 0.027817094698548317 \t Test loss : 0.0647628977894783\n",
            "Epoch : 1547 \t Train loss : 0.02778308093547821 \t Test loss : 0.0646853893995285\n",
            "Epoch : 1548 \t Train loss : 0.027749061584472656 \t Test loss : 0.06460787355899811\n",
            "Epoch : 1549 \t Train loss : 0.0277150459587574 \t Test loss : 0.06453036516904831\n",
            "Epoch : 1550 \t Train loss : 0.02768103778362274 \t Test loss : 0.06445284187793732\n",
            "Epoch : 1551 \t Train loss : 0.02764701470732689 \t Test loss : 0.06437534093856812\n",
            "Epoch : 1552 \t Train loss : 0.027612999081611633 \t Test loss : 0.06429781764745712\n",
            "Epoch : 1553 \t Train loss : 0.027578983455896378 \t Test loss : 0.06422030925750732\n",
            "Epoch : 1554 \t Train loss : 0.027544964104890823 \t Test loss : 0.06414280086755753\n",
            "Epoch : 1555 \t Train loss : 0.027510952204465866 \t Test loss : 0.06406527757644653\n",
            "Epoch : 1556 \t Train loss : 0.02747693657875061 \t Test loss : 0.06398776918649673\n",
            "Epoch : 1557 \t Train loss : 0.027442922815680504 \t Test loss : 0.06391026824712753\n",
            "Epoch : 1558 \t Train loss : 0.0274089016020298 \t Test loss : 0.06383275240659714\n",
            "Epoch : 1559 \t Train loss : 0.027374884113669395 \t Test loss : 0.06375524401664734\n",
            "Epoch : 1560 \t Train loss : 0.02734086848795414 \t Test loss : 0.06367772817611694\n",
            "Epoch : 1561 \t Train loss : 0.027306854724884033 \t Test loss : 0.06360021233558655\n",
            "Epoch : 1562 \t Train loss : 0.02727283164858818 \t Test loss : 0.06352271139621735\n",
            "Epoch : 1563 \t Train loss : 0.027238821610808372 \t Test loss : 0.06344518810510635\n",
            "Epoch : 1564 \t Train loss : 0.02720480039715767 \t Test loss : 0.06336767226457596\n",
            "Epoch : 1565 \t Train loss : 0.027170788496732712 \t Test loss : 0.06329016387462616\n",
            "Epoch : 1566 \t Train loss : 0.02713676355779171 \t Test loss : 0.06321265548467636\n",
            "Epoch : 1567 \t Train loss : 0.027102749794721603 \t Test loss : 0.06313514709472656\n",
            "Epoch : 1568 \t Train loss : 0.027068734169006348 \t Test loss : 0.06305763125419617\n",
            "Epoch : 1569 \t Train loss : 0.027034718543291092 \t Test loss : 0.06298011541366577\n",
            "Epoch : 1570 \t Train loss : 0.027000704780220985 \t Test loss : 0.06290261447429657\n",
            "Epoch : 1571 \t Train loss : 0.02696668542921543 \t Test loss : 0.06282509863376617\n",
            "Epoch : 1572 \t Train loss : 0.026932675391435623 \t Test loss : 0.06274759024381638\n",
            "Epoch : 1573 \t Train loss : 0.02689865231513977 \t Test loss : 0.06267006695270538\n",
            "Epoch : 1574 \t Train loss : 0.026864636689424515 \t Test loss : 0.06259255856275558\n",
            "Epoch : 1575 \t Train loss : 0.026830624788999557 \t Test loss : 0.06251505017280579\n",
            "Epoch : 1576 \t Train loss : 0.026796605437994003 \t Test loss : 0.06243755295872688\n",
            "Epoch : 1577 \t Train loss : 0.026762589812278748 \t Test loss : 0.06236002594232559\n",
            "Epoch : 1578 \t Train loss : 0.026728570461273193 \t Test loss : 0.062282513827085495\n",
            "Epoch : 1579 \t Train loss : 0.026694560423493385 \t Test loss : 0.062205005437135696\n",
            "Epoch : 1580 \t Train loss : 0.02666054107248783 \t Test loss : 0.0621274895966053\n",
            "Epoch : 1581 \t Train loss : 0.026626523584127426 \t Test loss : 0.06204996258020401\n",
            "Epoch : 1582 \t Train loss : 0.02659251168370247 \t Test loss : 0.06197245791554451\n",
            "Epoch : 1583 \t Train loss : 0.026558494195342064 \t Test loss : 0.061894964426755905\n",
            "Epoch : 1584 \t Train loss : 0.026524478569626808 \t Test loss : 0.06181744486093521\n",
            "Epoch : 1585 \t Train loss : 0.026490455493330956 \t Test loss : 0.061739932745695114\n",
            "Epoch : 1586 \t Train loss : 0.026456449180841446 \t Test loss : 0.06166241317987442\n",
            "Epoch : 1587 \t Train loss : 0.026422416791319847 \t Test loss : 0.06158491224050522\n",
            "Epoch : 1588 \t Train loss : 0.02638840675354004 \t Test loss : 0.061507392674684525\n",
            "Epoch : 1589 \t Train loss : 0.026354392990469933 \t Test loss : 0.061429888010025024\n",
            "Epoch : 1590 \t Train loss : 0.026320379227399826 \t Test loss : 0.061352360993623734\n",
            "Epoch : 1591 \t Train loss : 0.02628636360168457 \t Test loss : 0.06127486377954483\n",
            "Epoch : 1592 \t Train loss : 0.026252344250679016 \t Test loss : 0.06119735166430473\n",
            "Epoch : 1593 \t Train loss : 0.02621833048760891 \t Test loss : 0.06111983209848404\n",
            "Epoch : 1594 \t Train loss : 0.026184309273958206 \t Test loss : 0.061042316257953644\n",
            "Epoch : 1595 \t Train loss : 0.02615029737353325 \t Test loss : 0.06096482276916504\n",
            "Epoch : 1596 \t Train loss : 0.026116276159882545 \t Test loss : 0.06088728830218315\n",
            "Epoch : 1597 \t Train loss : 0.02608226239681244 \t Test loss : 0.06080978363752365\n",
            "Epoch : 1598 \t Train loss : 0.026048248633742332 \t Test loss : 0.06073228269815445\n",
            "Epoch : 1599 \t Train loss : 0.02601422742009163 \t Test loss : 0.06065475940704346\n",
            "Epoch : 1600 \t Train loss : 0.025980215519666672 \t Test loss : 0.06057725101709366\n",
            "Epoch : 1601 \t Train loss : 0.02594619058072567 \t Test loss : 0.06049973890185356\n",
            "Epoch : 1602 \t Train loss : 0.025912189856171608 \t Test loss : 0.06042221933603287\n",
            "Epoch : 1603 \t Train loss : 0.025878166779875755 \t Test loss : 0.06034470722079277\n",
            "Epoch : 1604 \t Train loss : 0.0258441474288702 \t Test loss : 0.06026720255613327\n",
            "Epoch : 1605 \t Train loss : 0.025810128077864647 \t Test loss : 0.060189682990312576\n",
            "Epoch : 1606 \t Train loss : 0.02577611431479454 \t Test loss : 0.06011218950152397\n",
            "Epoch : 1607 \t Train loss : 0.025742094963788986 \t Test loss : 0.06003466993570328\n",
            "Epoch : 1608 \t Train loss : 0.025708084926009178 \t Test loss : 0.059957168996334076\n",
            "Epoch : 1609 \t Train loss : 0.025674069300293922 \t Test loss : 0.059879641979932785\n",
            "Epoch : 1610 \t Train loss : 0.025640049949288368 \t Test loss : 0.059802133589982986\n",
            "Epoch : 1611 \t Train loss : 0.025606030598282814 \t Test loss : 0.05972462147474289\n",
            "Epoch : 1612 \t Train loss : 0.02557201310992241 \t Test loss : 0.059647105634212494\n",
            "Epoch : 1613 \t Train loss : 0.0255380030721426 \t Test loss : 0.0595695897936821\n",
            "Epoch : 1614 \t Train loss : 0.025503987446427345 \t Test loss : 0.0594920888543129\n",
            "Epoch : 1615 \t Train loss : 0.02546996809542179 \t Test loss : 0.05941455811262131\n",
            "Epoch : 1616 \t Train loss : 0.025435958057641983 \t Test loss : 0.0593370720744133\n",
            "Epoch : 1617 \t Train loss : 0.02540193870663643 \t Test loss : 0.059259552508592606\n",
            "Epoch : 1618 \t Train loss : 0.025367915630340576 \t Test loss : 0.05918203666806221\n",
            "Epoch : 1619 \t Train loss : 0.025333905592560768 \t Test loss : 0.059104520827531815\n",
            "Epoch : 1620 \t Train loss : 0.025299886241555214 \t Test loss : 0.05902700871229172\n",
            "Epoch : 1621 \t Train loss : 0.02526586875319481 \t Test loss : 0.05894949287176132\n",
            "Epoch : 1622 \t Train loss : 0.02523185685276985 \t Test loss : 0.05887199193239212\n",
            "Epoch : 1623 \t Train loss : 0.025197833776474 \t Test loss : 0.058794476091861725\n",
            "Epoch : 1624 \t Train loss : 0.02516382373869419 \t Test loss : 0.05871696025133133\n",
            "Epoch : 1625 \t Train loss : 0.025129806250333786 \t Test loss : 0.05863945558667183\n",
            "Epoch : 1626 \t Train loss : 0.02509579062461853 \t Test loss : 0.05856194347143173\n",
            "Epoch : 1627 \t Train loss : 0.025061774998903275 \t Test loss : 0.05848444253206253\n",
            "Epoch : 1628 \t Train loss : 0.02502775564789772 \t Test loss : 0.05840691179037094\n",
            "Epoch : 1629 \t Train loss : 0.024993741884827614 \t Test loss : 0.05832939222455025\n",
            "Epoch : 1630 \t Train loss : 0.02495972439646721 \t Test loss : 0.058251895010471344\n",
            "Epoch : 1631 \t Train loss : 0.024925703182816505 \t Test loss : 0.05817438289523125\n",
            "Epoch : 1632 \t Train loss : 0.024891695007681847 \t Test loss : 0.05809686332941055\n",
            "Epoch : 1633 \t Train loss : 0.02485767751932144 \t Test loss : 0.058019351214170456\n",
            "Epoch : 1634 \t Train loss : 0.024823656305670738 \t Test loss : 0.05794184282422066\n",
            "Epoch : 1635 \t Train loss : 0.02478964254260063 \t Test loss : 0.05786433070898056\n",
            "Epoch : 1636 \t Train loss : 0.024755625054240227 \t Test loss : 0.057786811143159866\n",
            "Epoch : 1637 \t Train loss : 0.02472160942852497 \t Test loss : 0.057709306478500366\n",
            "Epoch : 1638 \t Train loss : 0.024687593802809715 \t Test loss : 0.057631779462099075\n",
            "Epoch : 1639 \t Train loss : 0.024653572589159012 \t Test loss : 0.05755428597331047\n",
            "Epoch : 1640 \t Train loss : 0.024619558826088905 \t Test loss : 0.05747677758336067\n",
            "Epoch : 1641 \t Train loss : 0.0245855450630188 \t Test loss : 0.057399261742830276\n",
            "Epoch : 1642 \t Train loss : 0.024551525712013245 \t Test loss : 0.05732174962759018\n",
            "Epoch : 1643 \t Train loss : 0.024517511948943138 \t Test loss : 0.057244233787059784\n",
            "Epoch : 1644 \t Train loss : 0.024483496323227882 \t Test loss : 0.05716671794652939\n",
            "Epoch : 1645 \t Train loss : 0.024449482560157776 \t Test loss : 0.0570891872048378\n",
            "Epoch : 1646 \t Train loss : 0.02441546693444252 \t Test loss : 0.05701170116662979\n",
            "Epoch : 1647 \t Train loss : 0.024381447583436966 \t Test loss : 0.05693419650197029\n",
            "Epoch : 1648 \t Train loss : 0.02434743009507656 \t Test loss : 0.056856680661439896\n",
            "Epoch : 1649 \t Train loss : 0.024313410744071007 \t Test loss : 0.0567791573703289\n",
            "Epoch : 1650 \t Train loss : 0.024279393255710602 \t Test loss : 0.05670164152979851\n",
            "Epoch : 1651 \t Train loss : 0.024245381355285645 \t Test loss : 0.05662413313984871\n",
            "Epoch : 1652 \t Train loss : 0.02421136200428009 \t Test loss : 0.05654662847518921\n",
            "Epoch : 1653 \t Train loss : 0.024177346378564835 \t Test loss : 0.05646910518407822\n",
            "Epoch : 1654 \t Train loss : 0.024143332615494728 \t Test loss : 0.05639160796999931\n",
            "Epoch : 1655 \t Train loss : 0.024109311401844025 \t Test loss : 0.056314099580049515\n",
            "Epoch : 1656 \t Train loss : 0.02407529577612877 \t Test loss : 0.05623658746480942\n",
            "Epoch : 1657 \t Train loss : 0.02404128387570381 \t Test loss : 0.056159067898988724\n",
            "Epoch : 1658 \t Train loss : 0.024007266387343407 \t Test loss : 0.05608155578374863\n",
            "Epoch : 1659 \t Train loss : 0.02397325076162815 \t Test loss : 0.056004054844379425\n",
            "Epoch : 1660 \t Train loss : 0.023939231410622597 \t Test loss : 0.05592653900384903\n",
            "Epoch : 1661 \t Train loss : 0.023905228823423386 \t Test loss : 0.055849023163318634\n",
            "Epoch : 1662 \t Train loss : 0.023871202021837234 \t Test loss : 0.05577149987220764\n",
            "Epoch : 1663 \t Train loss : 0.02383718453347683 \t Test loss : 0.05569400638341904\n",
            "Epoch : 1664 \t Train loss : 0.023803172633051872 \t Test loss : 0.05561648681759834\n",
            "Epoch : 1665 \t Train loss : 0.02376914955675602 \t Test loss : 0.055538952350616455\n",
            "Epoch : 1666 \t Train loss : 0.023735132068395615 \t Test loss : 0.05546145513653755\n",
            "Epoch : 1667 \t Train loss : 0.023701120167970657 \t Test loss : 0.05538395047187805\n",
            "Epoch : 1668 \t Train loss : 0.023667100816965103 \t Test loss : 0.055306434631347656\n",
            "Epoch : 1669 \t Train loss : 0.02363308146595955 \t Test loss : 0.05522892624139786\n",
            "Epoch : 1670 \t Train loss : 0.023599063977599144 \t Test loss : 0.055151425302028656\n",
            "Epoch : 1671 \t Train loss : 0.023565052077174187 \t Test loss : 0.055073898285627365\n",
            "Epoch : 1672 \t Train loss : 0.02353104017674923 \t Test loss : 0.054996393620967865\n",
            "Epoch : 1673 \t Train loss : 0.023497018963098526 \t Test loss : 0.05491887405514717\n",
            "Epoch : 1674 \t Train loss : 0.02346300333738327 \t Test loss : 0.054841361939907074\n",
            "Epoch : 1675 \t Train loss : 0.023428987711668015 \t Test loss : 0.05476389080286026\n",
            "Epoch : 1676 \t Train loss : 0.023394957184791565 \t Test loss : 0.05468635633587837\n",
            "Epoch : 1677 \t Train loss : 0.023360952734947205 \t Test loss : 0.054608821868896484\n",
            "Epoch : 1678 \t Train loss : 0.023326942697167397 \t Test loss : 0.054531317204236984\n",
            "Epoch : 1679 \t Train loss : 0.023292917758226395 \t Test loss : 0.05445381999015808\n",
            "Epoch : 1680 \t Train loss : 0.023258911445736885 \t Test loss : 0.05437628552317619\n",
            "Epoch : 1681 \t Train loss : 0.02322489023208618 \t Test loss : 0.0542987696826458\n",
            "Epoch : 1682 \t Train loss : 0.02319086715579033 \t Test loss : 0.0542212650179863\n",
            "Epoch : 1683 \t Train loss : 0.02315685525536537 \t Test loss : 0.054143764078617096\n",
            "Epoch : 1684 \t Train loss : 0.023122841492295265 \t Test loss : 0.054066240787506104\n",
            "Epoch : 1685 \t Train loss : 0.02308882214128971 \t Test loss : 0.053988732397556305\n",
            "Epoch : 1686 \t Train loss : 0.023054806515574455 \t Test loss : 0.05391122028231621\n",
            "Epoch : 1687 \t Train loss : 0.0230207871645689 \t Test loss : 0.05383371189236641\n",
            "Epoch : 1688 \t Train loss : 0.022986775264143944 \t Test loss : 0.053756196051836014\n",
            "Epoch : 1689 \t Train loss : 0.022952761501073837 \t Test loss : 0.05367868393659592\n",
            "Epoch : 1690 \t Train loss : 0.022918744012713432 \t Test loss : 0.053601156920194626\n",
            "Epoch : 1691 \t Train loss : 0.02288472279906273 \t Test loss : 0.05352365970611572\n",
            "Epoch : 1692 \t Train loss : 0.02285071089863777 \t Test loss : 0.053446151316165924\n",
            "Epoch : 1693 \t Train loss : 0.022816695272922516 \t Test loss : 0.05336863920092583\n",
            "Epoch : 1694 \t Train loss : 0.02278267964720726 \t Test loss : 0.05329111963510513\n",
            "Epoch : 1695 \t Train loss : 0.022748660296201706 \t Test loss : 0.05321360379457474\n",
            "Epoch : 1696 \t Train loss : 0.02271464467048645 \t Test loss : 0.05313609912991524\n",
            "Epoch : 1697 \t Train loss : 0.022680629044771194 \t Test loss : 0.05305858328938484\n",
            "Epoch : 1698 \t Train loss : 0.022646604105830193 \t Test loss : 0.05298108607530594\n",
            "Epoch : 1699 \t Train loss : 0.022612594068050385 \t Test loss : 0.05290357023477554\n",
            "Epoch : 1700 \t Train loss : 0.022578582167625427 \t Test loss : 0.05282604694366455\n",
            "Epoch : 1701 \t Train loss : 0.022544562816619873 \t Test loss : 0.05274853855371475\n",
            "Epoch : 1702 \t Train loss : 0.022510549053549767 \t Test loss : 0.052671026438474655\n",
            "Epoch : 1703 \t Train loss : 0.02247653342783451 \t Test loss : 0.05259351804852486\n",
            "Epoch : 1704 \t Train loss : 0.022442512214183807 \t Test loss : 0.052515994757413864\n",
            "Epoch : 1705 \t Train loss : 0.0224084984511137 \t Test loss : 0.05243850499391556\n",
            "Epoch : 1706 \t Train loss : 0.022374477237462997 \t Test loss : 0.05236098915338516\n",
            "Epoch : 1707 \t Train loss : 0.02234046161174774 \t Test loss : 0.052283477038145065\n",
            "Epoch : 1708 \t Train loss : 0.022306444123387337 \t Test loss : 0.05220595747232437\n",
            "Epoch : 1709 \t Train loss : 0.02227243408560753 \t Test loss : 0.052128445357084274\n",
            "Epoch : 1710 \t Train loss : 0.022238418459892273 \t Test loss : 0.05205092951655388\n",
            "Epoch : 1711 \t Train loss : 0.022204402834177017 \t Test loss : 0.05197342112660408\n",
            "Epoch : 1712 \t Train loss : 0.022170385345816612 \t Test loss : 0.05189589411020279\n",
            "Epoch : 1713 \t Train loss : 0.022136371582746506 \t Test loss : 0.051818400621414185\n",
            "Epoch : 1714 \t Train loss : 0.0221023540943861 \t Test loss : 0.05174088478088379\n",
            "Epoch : 1715 \t Train loss : 0.022068340331315994 \t Test loss : 0.05166337639093399\n",
            "Epoch : 1716 \t Train loss : 0.02203432098031044 \t Test loss : 0.051585860550403595\n",
            "Epoch : 1717 \t Train loss : 0.022000301629304886 \t Test loss : 0.0515083447098732\n",
            "Epoch : 1718 \t Train loss : 0.02196628227829933 \t Test loss : 0.051430851221084595\n",
            "Epoch : 1719 \t Train loss : 0.021932266652584076 \t Test loss : 0.0513533353805542\n",
            "Epoch : 1720 \t Train loss : 0.02189825102686882 \t Test loss : 0.05127581208944321\n",
            "Epoch : 1721 \t Train loss : 0.021864240989089012 \t Test loss : 0.05119829624891281\n",
            "Epoch : 1722 \t Train loss : 0.021830221638083458 \t Test loss : 0.05112078785896301\n",
            "Epoch : 1723 \t Train loss : 0.021796202287077904 \t Test loss : 0.051043279469013214\n",
            "Epoch : 1724 \t Train loss : 0.021762188524007797 \t Test loss : 0.05096576362848282\n",
            "Epoch : 1725 \t Train loss : 0.021728171035647392 \t Test loss : 0.05088825151324272\n",
            "Epoch : 1726 \t Train loss : 0.021694157272577286 \t Test loss : 0.05081074312329292\n",
            "Epoch : 1727 \t Train loss : 0.02166013978421688 \t Test loss : 0.05073323845863342\n",
            "Epoch : 1728 \t Train loss : 0.021626124158501625 \t Test loss : 0.05065572261810303\n",
            "Epoch : 1729 \t Train loss : 0.021592101082205772 \t Test loss : 0.05057820677757263\n",
            "Epoch : 1730 \t Train loss : 0.021558087319135666 \t Test loss : 0.05050069838762283\n",
            "Epoch : 1731 \t Train loss : 0.02152407541871071 \t Test loss : 0.05042318254709244\n",
            "Epoch : 1732 \t Train loss : 0.021490052342414856 \t Test loss : 0.05034567043185234\n",
            "Epoch : 1733 \t Train loss : 0.0214560404419899 \t Test loss : 0.050268154591321945\n",
            "Epoch : 1734 \t Train loss : 0.021422024816274643 \t Test loss : 0.05019066482782364\n",
            "Epoch : 1735 \t Train loss : 0.021388014778494835 \t Test loss : 0.050113141536712646\n",
            "Epoch : 1736 \t Train loss : 0.021353989839553833 \t Test loss : 0.05003562569618225\n",
            "Epoch : 1737 \t Train loss : 0.021319974213838577 \t Test loss : 0.049958109855651855\n",
            "Epoch : 1738 \t Train loss : 0.021285956725478172 \t Test loss : 0.04988060146570206\n",
            "Epoch : 1739 \t Train loss : 0.021251942962408066 \t Test loss : 0.04980310797691345\n",
            "Epoch : 1740 \t Train loss : 0.021217919886112213 \t Test loss : 0.049725573509931564\n",
            "Epoch : 1741 \t Train loss : 0.021183911710977554 \t Test loss : 0.04964805766940117\n",
            "Epoch : 1742 \t Train loss : 0.02114989422261715 \t Test loss : 0.04957054182887077\n",
            "Epoch : 1743 \t Train loss : 0.021115874871611595 \t Test loss : 0.04949304461479187\n",
            "Epoch : 1744 \t Train loss : 0.021081868559122086 \t Test loss : 0.04941552132368088\n",
            "Epoch : 1745 \t Train loss : 0.021047841757535934 \t Test loss : 0.04933801293373108\n",
            "Epoch : 1746 \t Train loss : 0.021013829857110977 \t Test loss : 0.049260497093200684\n",
            "Epoch : 1747 \t Train loss : 0.020979810506105423 \t Test loss : 0.049182988703250885\n",
            "Epoch : 1748 \t Train loss : 0.020945794880390167 \t Test loss : 0.04910548776388168\n",
            "Epoch : 1749 \t Train loss : 0.020911775529384613 \t Test loss : 0.0490279421210289\n",
            "Epoch : 1750 \t Train loss : 0.020877769216895103 \t Test loss : 0.04895044490695\n",
            "Epoch : 1751 \t Train loss : 0.02084374986588955 \t Test loss : 0.0488729402422905\n",
            "Epoch : 1752 \t Train loss : 0.020809734240174294 \t Test loss : 0.0487954318523407\n",
            "Epoch : 1753 \t Train loss : 0.02077571488916874 \t Test loss : 0.0487179160118103\n",
            "Epoch : 1754 \t Train loss : 0.020741697400808334 \t Test loss : 0.0486404225230217\n",
            "Epoch : 1755 \t Train loss : 0.02070768177509308 \t Test loss : 0.04856289178133011\n",
            "Epoch : 1756 \t Train loss : 0.020673666149377823 \t Test loss : 0.04848539084196091\n",
            "Epoch : 1757 \t Train loss : 0.020639648661017418 \t Test loss : 0.04840787127614021\n",
            "Epoch : 1758 \t Train loss : 0.020605631172657013 \t Test loss : 0.04833035543560982\n",
            "Epoch : 1759 \t Train loss : 0.020571613684296608 \t Test loss : 0.04825283959507942\n",
            "Epoch : 1760 \t Train loss : 0.0205375999212265 \t Test loss : 0.04817534238100052\n",
            "Epoch : 1761 \t Train loss : 0.020503584295511246 \t Test loss : 0.048097819089889526\n",
            "Epoch : 1762 \t Train loss : 0.02046956494450569 \t Test loss : 0.048020314425230026\n",
            "Epoch : 1763 \t Train loss : 0.020435551181435585 \t Test loss : 0.04794280603528023\n",
            "Epoch : 1764 \t Train loss : 0.02040152996778488 \t Test loss : 0.04786530137062073\n",
            "Epoch : 1765 \t Train loss : 0.020367518067359924 \t Test loss : 0.04778777435421944\n",
            "Epoch : 1766 \t Train loss : 0.02033349871635437 \t Test loss : 0.047710269689559937\n",
            "Epoch : 1767 \t Train loss : 0.020299483090639114 \t Test loss : 0.04763275384902954\n",
            "Epoch : 1768 \t Train loss : 0.020265469327569008 \t Test loss : 0.047555238008499146\n",
            "Epoch : 1769 \t Train loss : 0.020231453701853752 \t Test loss : 0.04747772216796875\n",
            "Epoch : 1770 \t Train loss : 0.020197436213493347 \t Test loss : 0.04740021750330925\n",
            "Epoch : 1771 \t Train loss : 0.020163413137197495 \t Test loss : 0.04732268303632736\n",
            "Epoch : 1772 \t Train loss : 0.020129412412643433 \t Test loss : 0.047245193272829056\n",
            "Epoch : 1773 \t Train loss : 0.020095381885766983 \t Test loss : 0.04716767743229866\n",
            "Epoch : 1774 \t Train loss : 0.020061371847987175 \t Test loss : 0.04709017276763916\n",
            "Epoch : 1775 \t Train loss : 0.02002735622227192 \t Test loss : 0.04701264575123787\n",
            "Epoch : 1776 \t Train loss : 0.019993338733911514 \t Test loss : 0.04693514108657837\n",
            "Epoch : 1777 \t Train loss : 0.01995931938290596 \t Test loss : 0.04685763642191887\n",
            "Epoch : 1778 \t Train loss : 0.019925305619835854 \t Test loss : 0.046780139207839966\n",
            "Epoch : 1779 \t Train loss : 0.01989128813147545 \t Test loss : 0.04670260474085808\n",
            "Epoch : 1780 \t Train loss : 0.01985727623105049 \t Test loss : 0.04662509635090828\n",
            "Epoch : 1781 \t Train loss : 0.019823256880044937 \t Test loss : 0.04654758423566818\n",
            "Epoch : 1782 \t Train loss : 0.01978924497961998 \t Test loss : 0.04647006466984749\n",
            "Epoch : 1783 \t Train loss : 0.019755231216549873 \t Test loss : 0.046392567455768585\n",
            "Epoch : 1784 \t Train loss : 0.019721204414963722 \t Test loss : 0.04631505161523819\n",
            "Epoch : 1785 \t Train loss : 0.019687190651893616 \t Test loss : 0.046237535774707794\n",
            "Epoch : 1786 \t Train loss : 0.01965317502617836 \t Test loss : 0.0461600199341774\n",
            "Epoch : 1787 \t Train loss : 0.01961914822459221 \t Test loss : 0.046082526445388794\n",
            "Epoch : 1788 \t Train loss : 0.0195851381868124 \t Test loss : 0.046004991978406906\n",
            "Epoch : 1789 \t Train loss : 0.019551124423742294 \t Test loss : 0.045927487313747406\n",
            "Epoch : 1790 \t Train loss : 0.019517112523317337 \t Test loss : 0.04584997147321701\n",
            "Epoch : 1791 \t Train loss : 0.019483093172311783 \t Test loss : 0.04577247425913811\n",
            "Epoch : 1792 \t Train loss : 0.019449079409241676 \t Test loss : 0.04569495841860771\n",
            "Epoch : 1793 \t Train loss : 0.01941506192088127 \t Test loss : 0.04561743885278702\n",
            "Epoch : 1794 \t Train loss : 0.019381044432520866 \t Test loss : 0.04553992301225662\n",
            "Epoch : 1795 \t Train loss : 0.01934702694416046 \t Test loss : 0.04546242207288742\n",
            "Epoch : 1796 \t Train loss : 0.019313011318445206 \t Test loss : 0.04538490250706673\n",
            "Epoch : 1797 \t Train loss : 0.0192789938300848 \t Test loss : 0.04530739039182663\n",
            "Epoch : 1798 \t Train loss : 0.019244980067014694 \t Test loss : 0.04522988200187683\n",
            "Epoch : 1799 \t Train loss : 0.01921096071600914 \t Test loss : 0.045152366161346436\n",
            "Epoch : 1800 \t Train loss : 0.019176948815584183 \t Test loss : 0.04507485777139664\n",
            "Epoch : 1801 \t Train loss : 0.01914292946457863 \t Test loss : 0.04499735310673714\n",
            "Epoch : 1802 \t Train loss : 0.019108915701508522 \t Test loss : 0.044919829815626144\n",
            "Epoch : 1803 \t Train loss : 0.019074898213148117 \t Test loss : 0.044842321425676346\n",
            "Epoch : 1804 \t Train loss : 0.019040878862142563 \t Test loss : 0.04476480558514595\n",
            "Epoch : 1805 \t Train loss : 0.019006861373782158 \t Test loss : 0.04468729346990585\n",
            "Epoch : 1806 \t Train loss : 0.01897284761071205 \t Test loss : 0.04460979253053665\n",
            "Epoch : 1807 \t Train loss : 0.018938828259706497 \t Test loss : 0.04453226923942566\n",
            "Epoch : 1808 \t Train loss : 0.01890481449663639 \t Test loss : 0.04445476457476616\n",
            "Epoch : 1809 \t Train loss : 0.018870791420340538 \t Test loss : 0.044377248734235764\n",
            "Epoch : 1810 \t Train loss : 0.01883678138256073 \t Test loss : 0.044299740344285965\n",
            "Epoch : 1811 \t Train loss : 0.018802760168910027 \t Test loss : 0.04422222450375557\n",
            "Epoch : 1812 \t Train loss : 0.01876874826848507 \t Test loss : 0.04414471983909607\n",
            "Epoch : 1813 \t Train loss : 0.018734734505414963 \t Test loss : 0.04406719654798508\n",
            "Epoch : 1814 \t Train loss : 0.018700717017054558 \t Test loss : 0.04398968815803528\n",
            "Epoch : 1815 \t Train loss : 0.018666699528694153 \t Test loss : 0.04391217976808548\n",
            "Epoch : 1816 \t Train loss : 0.018632683902978897 \t Test loss : 0.043834663927555084\n",
            "Epoch : 1817 \t Train loss : 0.018598666414618492 \t Test loss : 0.04375715181231499\n",
            "Epoch : 1818 \t Train loss : 0.018564648926258087 \t Test loss : 0.043679624795913696\n",
            "Epoch : 1819 \t Train loss : 0.018530631437897682 \t Test loss : 0.04360213130712509\n",
            "Epoch : 1820 \t Train loss : 0.018496617674827576 \t Test loss : 0.043524615466594696\n",
            "Epoch : 1821 \t Train loss : 0.01846260204911232 \t Test loss : 0.0434471070766449\n",
            "Epoch : 1822 \t Train loss : 0.018428584560751915 \t Test loss : 0.0433695986866951\n",
            "Epoch : 1823 \t Train loss : 0.01839456893503666 \t Test loss : 0.04329214245080948\n",
            "Epoch : 1824 \t Train loss : 0.018360571935772896 \t Test loss : 0.043214570730924606\n",
            "Epoch : 1825 \t Train loss : 0.0183265320956707 \t Test loss : 0.043137066066265106\n",
            "Epoch : 1826 \t Train loss : 0.018292520195245743 \t Test loss : 0.043059539049863815\n",
            "Epoch : 1827 \t Train loss : 0.018258502706885338 \t Test loss : 0.04298204183578491\n",
            "Epoch : 1828 \t Train loss : 0.01822448894381523 \t Test loss : 0.04290451481938362\n",
            "Epoch : 1829 \t Train loss : 0.01819046400487423 \t Test loss : 0.04282699152827263\n",
            "Epoch : 1830 \t Train loss : 0.018156452104449272 \t Test loss : 0.042749494314193726\n",
            "Epoch : 1831 \t Train loss : 0.018122438341379166 \t Test loss : 0.042671989649534225\n",
            "Epoch : 1832 \t Train loss : 0.01808842457830906 \t Test loss : 0.04259448125958443\n",
            "Epoch : 1833 \t Train loss : 0.018054407089948654 \t Test loss : 0.04251696541905403\n",
            "Epoch : 1834 \t Train loss : 0.0180203877389431 \t Test loss : 0.042439453303813934\n",
            "Epoch : 1835 \t Train loss : 0.017986373975872993 \t Test loss : 0.04236193373799324\n",
            "Epoch : 1836 \t Train loss : 0.01795235648751259 \t Test loss : 0.04228442162275314\n",
            "Epoch : 1837 \t Train loss : 0.01791834458708763 \t Test loss : 0.04220690205693245\n",
            "Epoch : 1838 \t Train loss : 0.017884325236082077 \t Test loss : 0.04212937504053116\n",
            "Epoch : 1839 \t Train loss : 0.017850296571850777 \t Test loss : 0.04205189272761345\n",
            "Epoch : 1840 \t Train loss : 0.017816290259361267 \t Test loss : 0.041974376887083054\n",
            "Epoch : 1841 \t Train loss : 0.01778227649629116 \t Test loss : 0.04189686104655266\n",
            "Epoch : 1842 \t Train loss : 0.017748259007930756 \t Test loss : 0.04181935265660286\n",
            "Epoch : 1843 \t Train loss : 0.01771424524486065 \t Test loss : 0.04174182936549187\n",
            "Epoch : 1844 \t Train loss : 0.017680229619145393 \t Test loss : 0.041664332151412964\n",
            "Epoch : 1845 \t Train loss : 0.01764621213078499 \t Test loss : 0.04158680513501167\n",
            "Epoch : 1846 \t Train loss : 0.017612189054489136 \t Test loss : 0.04150931164622307\n",
            "Epoch : 1847 \t Train loss : 0.01757817529141903 \t Test loss : 0.04143179580569267\n",
            "Epoch : 1848 \t Train loss : 0.017544161528348923 \t Test loss : 0.041354287415742874\n",
            "Epoch : 1849 \t Train loss : 0.01751014217734337 \t Test loss : 0.04127676412463188\n",
            "Epoch : 1850 \t Train loss : 0.017476120963692665 \t Test loss : 0.04119925945997238\n",
            "Epoch : 1851 \t Train loss : 0.017442110925912857 \t Test loss : 0.041121743619441986\n",
            "Epoch : 1852 \t Train loss : 0.01740809716284275 \t Test loss : 0.04104423522949219\n",
            "Epoch : 1853 \t Train loss : 0.017374074086546898 \t Test loss : 0.04096672683954239\n",
            "Epoch : 1854 \t Train loss : 0.01734006032347679 \t Test loss : 0.04088921472430229\n",
            "Epoch : 1855 \t Train loss : 0.017306044697761536 \t Test loss : 0.0408116951584816\n",
            "Epoch : 1856 \t Train loss : 0.01727202907204628 \t Test loss : 0.040734194219112396\n",
            "Epoch : 1857 \t Train loss : 0.017238011583685875 \t Test loss : 0.040656678378582\n",
            "Epoch : 1858 \t Train loss : 0.017204001545906067 \t Test loss : 0.0405791699886322\n",
            "Epoch : 1859 \t Train loss : 0.017169978469610214 \t Test loss : 0.04050165414810181\n",
            "Epoch : 1860 \t Train loss : 0.01713596284389496 \t Test loss : 0.04042414575815201\n",
            "Epoch : 1861 \t Train loss : 0.017101960256695747 \t Test loss : 0.04034662991762161\n",
            "Epoch : 1862 \t Train loss : 0.017067935317754745 \t Test loss : 0.04026911407709122\n",
            "Epoch : 1863 \t Train loss : 0.01703391596674919 \t Test loss : 0.04019160196185112\n",
            "Epoch : 1864 \t Train loss : 0.016999898478388786 \t Test loss : 0.040114086121320724\n",
            "Epoch : 1865 \t Train loss : 0.01696588099002838 \t Test loss : 0.04003657028079033\n",
            "Epoch : 1866 \t Train loss : 0.016931861639022827 \t Test loss : 0.039959073066711426\n",
            "Epoch : 1867 \t Train loss : 0.01689785346388817 \t Test loss : 0.03988156467676163\n",
            "Epoch : 1868 \t Train loss : 0.016863830387592316 \t Test loss : 0.039804041385650635\n",
            "Epoch : 1869 \t Train loss : 0.01682981476187706 \t Test loss : 0.03972652554512024\n",
            "Epoch : 1870 \t Train loss : 0.016795799136161804 \t Test loss : 0.039649009704589844\n",
            "Epoch : 1871 \t Train loss : 0.016761787235736847 \t Test loss : 0.03957151249051094\n",
            "Epoch : 1872 \t Train loss : 0.01672777161002159 \t Test loss : 0.039494000375270844\n",
            "Epoch : 1873 \t Train loss : 0.016693752259016037 \t Test loss : 0.03941648080945015\n",
            "Epoch : 1874 \t Train loss : 0.01665973849594593 \t Test loss : 0.03933896869421005\n",
            "Epoch : 1875 \t Train loss : 0.016625728458166122 \t Test loss : 0.039226748049259186\n",
            "Epoch : 1876 \t Train loss : 0.016591768711805344 \t Test loss : 0.039149247109889984\n",
            "Epoch : 1877 \t Train loss : 0.016557743772864342 \t Test loss : 0.03907172754406929\n",
            "Epoch : 1878 \t Train loss : 0.016523731872439384 \t Test loss : 0.0389942042529583\n",
            "Epoch : 1879 \t Train loss : 0.016489718109369278 \t Test loss : 0.0389166995882988\n",
            "Epoch : 1880 \t Train loss : 0.016455700621008873 \t Test loss : 0.0388391837477684\n",
            "Epoch : 1881 \t Train loss : 0.016421684995293617 \t Test loss : 0.0387616865336895\n",
            "Epoch : 1882 \t Train loss : 0.016387667506933212 \t Test loss : 0.038684166967868805\n",
            "Epoch : 1883 \t Train loss : 0.016353648155927658 \t Test loss : 0.0386066809296608\n",
            "Epoch : 1884 \t Train loss : 0.01631963811814785 \t Test loss : 0.038529135286808014\n",
            "Epoch : 1885 \t Train loss : 0.016285613179206848 \t Test loss : 0.038451630622148514\n",
            "Epoch : 1886 \t Train loss : 0.01625160314142704 \t Test loss : 0.03837411850690842\n",
            "Epoch : 1887 \t Train loss : 0.016217608004808426 \t Test loss : 0.03826189786195755\n",
            "Epoch : 1888 \t Train loss : 0.016183629631996155 \t Test loss : 0.038184382021427155\n",
            "Epoch : 1889 \t Train loss : 0.0161496102809906 \t Test loss : 0.03810686618089676\n",
            "Epoch : 1890 \t Train loss : 0.016115598380565643 \t Test loss : 0.03802936151623726\n",
            "Epoch : 1891 \t Train loss : 0.01608158089220524 \t Test loss : 0.037951864302158356\n",
            "Epoch : 1892 \t Train loss : 0.016047557815909386 \t Test loss : 0.03787432983517647\n",
            "Epoch : 1893 \t Train loss : 0.01601354405283928 \t Test loss : 0.03779681771993637\n",
            "Epoch : 1894 \t Train loss : 0.015979530289769173 \t Test loss : 0.03771931678056717\n",
            "Epoch : 1895 \t Train loss : 0.015945512801408768 \t Test loss : 0.03764181211590767\n",
            "Epoch : 1896 \t Train loss : 0.015911497175693512 \t Test loss : 0.03756428882479668\n",
            "Epoch : 1897 \t Train loss : 0.015877481549978256 \t Test loss : 0.03748678043484688\n",
            "Epoch : 1898 \t Train loss : 0.0158434696495533 \t Test loss : 0.037409257143735886\n",
            "Epoch : 1899 \t Train loss : 0.015809496864676476 \t Test loss : 0.037297047674655914\n",
            "Epoch : 1900 \t Train loss : 0.015775498002767563 \t Test loss : 0.03721952438354492\n",
            "Epoch : 1901 \t Train loss : 0.01574147306382656 \t Test loss : 0.03714201599359512\n",
            "Epoch : 1902 \t Train loss : 0.015707457438111305 \t Test loss : 0.03706451132893562\n",
            "Epoch : 1903 \t Train loss : 0.0156734436750412 \t Test loss : 0.03698698803782463\n",
            "Epoch : 1904 \t Train loss : 0.015639424324035645 \t Test loss : 0.03690948337316513\n",
            "Epoch : 1905 \t Train loss : 0.015605410560965538 \t Test loss : 0.036831967532634735\n",
            "Epoch : 1906 \t Train loss : 0.015571394935250282 \t Test loss : 0.036754459142684937\n",
            "Epoch : 1907 \t Train loss : 0.01553737185895443 \t Test loss : 0.036676932126283646\n",
            "Epoch : 1908 \t Train loss : 0.015503361821174622 \t Test loss : 0.03659942001104355\n",
            "Epoch : 1909 \t Train loss : 0.01546934712678194 \t Test loss : 0.03652191907167435\n",
            "Epoch : 1910 \t Train loss : 0.015435328707098961 \t Test loss : 0.03644440695643425\n",
            "Epoch : 1911 \t Train loss : 0.015401373617351055 \t Test loss : 0.036332182586193085\n",
            "Epoch : 1912 \t Train loss : 0.015367358922958374 \t Test loss : 0.03625468164682388\n",
            "Epoch : 1913 \t Train loss : 0.015333334915339947 \t Test loss : 0.03617715835571289\n",
            "Epoch : 1914 \t Train loss : 0.015299324877560139 \t Test loss : 0.03609964996576309\n",
            "Epoch : 1915 \t Train loss : 0.015265306457877159 \t Test loss : 0.0360221341252327\n",
            "Epoch : 1916 \t Train loss : 0.015231299214065075 \t Test loss : 0.035944610834121704\n",
            "Epoch : 1917 \t Train loss : 0.015197277069091797 \t Test loss : 0.0358671136200428\n",
            "Epoch : 1918 \t Train loss : 0.015163259580731392 \t Test loss : 0.035789601504802704\n",
            "Epoch : 1919 \t Train loss : 0.015129243023693562 \t Test loss : 0.03571208193898201\n",
            "Epoch : 1920 \t Train loss : 0.015095224604010582 \t Test loss : 0.035634588450193405\n",
            "Epoch : 1921 \t Train loss : 0.015061208978295326 \t Test loss : 0.03555706888437271\n",
            "Epoch : 1922 \t Train loss : 0.01502720732241869 \t Test loss : 0.03544483706355095\n",
            "Epoch : 1923 \t Train loss : 0.014993235468864441 \t Test loss : 0.03536734730005264\n",
            "Epoch : 1924 \t Train loss : 0.014959205873310566 \t Test loss : 0.035289812833070755\n",
            "Epoch : 1925 \t Train loss : 0.01492520421743393 \t Test loss : 0.035212308168411255\n",
            "Epoch : 1926 \t Train loss : 0.014891187660396099 \t Test loss : 0.03513479232788086\n",
            "Epoch : 1927 \t Train loss : 0.014857172966003418 \t Test loss : 0.03505728393793106\n",
            "Epoch : 1928 \t Train loss : 0.014823155477643013 \t Test loss : 0.03497976064682007\n",
            "Epoch : 1929 \t Train loss : 0.014789137057960033 \t Test loss : 0.034902263432741165\n",
            "Epoch : 1930 \t Train loss : 0.014755120500922203 \t Test loss : 0.03482475131750107\n",
            "Epoch : 1931 \t Train loss : 0.014721098355948925 \t Test loss : 0.03474723547697067\n",
            "Epoch : 1932 \t Train loss : 0.014687093906104565 \t Test loss : 0.03466973453760147\n",
            "Epoch : 1933 \t Train loss : 0.01465307455509901 \t Test loss : 0.03459221124649048\n",
            "Epoch : 1934 \t Train loss : 0.014619085006415844 \t Test loss : 0.03447999805212021\n",
            "Epoch : 1935 \t Train loss : 0.014585104770958424 \t Test loss : 0.034402478486299515\n",
            "Epoch : 1936 \t Train loss : 0.014551082625985146 \t Test loss : 0.03432493656873703\n",
            "Epoch : 1937 \t Train loss : 0.014517066068947315 \t Test loss : 0.034247446805238724\n",
            "Epoch : 1938 \t Train loss : 0.014483054168522358 \t Test loss : 0.034169942140579224\n",
            "Epoch : 1939 \t Train loss : 0.014449035748839378 \t Test loss : 0.03409242630004883\n",
            "Epoch : 1940 \t Train loss : 0.01441501546651125 \t Test loss : 0.03401491791009903\n",
            "Epoch : 1941 \t Train loss : 0.01438099704682827 \t Test loss : 0.03393741697072983\n",
            "Epoch : 1942 \t Train loss : 0.014346984215080738 \t Test loss : 0.03385988995432854\n",
            "Epoch : 1943 \t Train loss : 0.014312973245978355 \t Test loss : 0.03378238528966904\n",
            "Epoch : 1944 \t Train loss : 0.014278952963650227 \t Test loss : 0.03370486572384834\n",
            "Epoch : 1945 \t Train loss : 0.014244938269257545 \t Test loss : 0.033627353608608246\n",
            "Epoch : 1946 \t Train loss : 0.014210973866283894 \t Test loss : 0.03351513296365738\n",
            "Epoch : 1947 \t Train loss : 0.014176967553794384 \t Test loss : 0.03343762084841728\n",
            "Epoch : 1948 \t Train loss : 0.014142943546175957 \t Test loss : 0.03336011618375778\n",
            "Epoch : 1949 \t Train loss : 0.014108928851783276 \t Test loss : 0.03328261524438858\n",
            "Epoch : 1950 \t Train loss : 0.014074921607971191 \t Test loss : 0.03320508450269699\n",
            "Epoch : 1951 \t Train loss : 0.014040900394320488 \t Test loss : 0.033127568662166595\n",
            "Epoch : 1952 \t Train loss : 0.014006885699927807 \t Test loss : 0.033050067722797394\n",
            "Epoch : 1953 \t Train loss : 0.013972866348922253 \t Test loss : 0.032972551882267\n",
            "Epoch : 1954 \t Train loss : 0.01393885351717472 \t Test loss : 0.03289506584405899\n",
            "Epoch : 1955 \t Train loss : 0.013904839754104614 \t Test loss : 0.0328175313770771\n",
            "Epoch : 1956 \t Train loss : 0.013870817609131336 \t Test loss : 0.03274001553654671\n",
            "Epoch : 1957 \t Train loss : 0.013836808502674103 \t Test loss : 0.03262779861688614\n",
            "Epoch : 1958 \t Train loss : 0.013802844099700451 \t Test loss : 0.03255028277635574\n",
            "Epoch : 1959 \t Train loss : 0.013768831267952919 \t Test loss : 0.03247276693582535\n",
            "Epoch : 1960 \t Train loss : 0.013734808191657066 \t Test loss : 0.03239525482058525\n",
            "Epoch : 1961 \t Train loss : 0.013700795359909534 \t Test loss : 0.032317738980054855\n",
            "Epoch : 1962 \t Train loss : 0.013666781596839428 \t Test loss : 0.03224024921655655\n",
            "Epoch : 1963 \t Train loss : 0.013632768765091896 \t Test loss : 0.03216272592544556\n",
            "Epoch : 1964 \t Train loss : 0.013598745688796043 \t Test loss : 0.03208521008491516\n",
            "Epoch : 1965 \t Train loss : 0.013564732857048512 \t Test loss : 0.032007694244384766\n",
            "Epoch : 1966 \t Train loss : 0.013530713506042957 \t Test loss : 0.03193018585443497\n",
            "Epoch : 1967 \t Train loss : 0.013496696949005127 \t Test loss : 0.03185269236564636\n",
            "Epoch : 1968 \t Train loss : 0.013462677597999573 \t Test loss : 0.031775157898664474\n",
            "Epoch : 1969 \t Train loss : 0.01342869084328413 \t Test loss : 0.03166293352842331\n",
            "Epoch : 1970 \t Train loss : 0.01339471060782671 \t Test loss : 0.03158543258905411\n",
            "Epoch : 1971 \t Train loss : 0.013360703364014626 \t Test loss : 0.031507909297943115\n",
            "Epoch : 1972 \t Train loss : 0.013326674699783325 \t Test loss : 0.031430404633283615\n",
            "Epoch : 1973 \t Train loss : 0.013292658142745495 \t Test loss : 0.031352896243333817\n",
            "Epoch : 1974 \t Train loss : 0.013258645310997963 \t Test loss : 0.031275372952222824\n",
            "Epoch : 1975 \t Train loss : 0.013224625959992409 \t Test loss : 0.031197870150208473\n",
            "Epoch : 1976 \t Train loss : 0.013190610334277153 \t Test loss : 0.03112034872174263\n",
            "Epoch : 1977 \t Train loss : 0.013156594708561897 \t Test loss : 0.03104284405708313\n",
            "Epoch : 1978 \t Train loss : 0.013122576288878918 \t Test loss : 0.030965333804488182\n",
            "Epoch : 1979 \t Train loss : 0.013088559731841087 \t Test loss : 0.030887817963957787\n",
            "Epoch : 1980 \t Train loss : 0.013054546900093555 \t Test loss : 0.030810290947556496\n",
            "Epoch : 1981 \t Train loss : 0.013020572252571583 \t Test loss : 0.030698079615831375\n",
            "Epoch : 1982 \t Train loss : 0.01298657339066267 \t Test loss : 0.030620569363236427\n",
            "Epoch : 1983 \t Train loss : 0.012952563352882862 \t Test loss : 0.03054305911064148\n",
            "Epoch : 1984 \t Train loss : 0.012918541207909584 \t Test loss : 0.030465543270111084\n",
            "Epoch : 1985 \t Train loss : 0.012884527444839478 \t Test loss : 0.030388033017516136\n",
            "Epoch : 1986 \t Train loss : 0.012850510887801647 \t Test loss : 0.030310530215501785\n",
            "Epoch : 1987 \t Train loss : 0.012816496193408966 \t Test loss : 0.030233001336455345\n",
            "Epoch : 1988 \t Train loss : 0.012782478705048561 \t Test loss : 0.030155498534440994\n",
            "Epoch : 1989 \t Train loss : 0.012748455628752708 \t Test loss : 0.0300779826939106\n",
            "Epoch : 1990 \t Train loss : 0.012714442797005177 \t Test loss : 0.03000047244131565\n",
            "Epoch : 1991 \t Train loss : 0.012680426239967346 \t Test loss : 0.02992296777665615\n",
            "Epoch : 1992 \t Train loss : 0.012646411545574665 \t Test loss : 0.029845446348190308\n",
            "Epoch : 1993 \t Train loss : 0.012612452730536461 \t Test loss : 0.02973322942852974\n",
            "Epoch : 1994 \t Train loss : 0.012578439898788929 \t Test loss : 0.029655730351805687\n",
            "Epoch : 1995 \t Train loss : 0.012544411234557629 \t Test loss : 0.029578208923339844\n",
            "Epoch : 1996 \t Train loss : 0.012510403990745544 \t Test loss : 0.029500693082809448\n",
            "Epoch : 1997 \t Train loss : 0.012476393021643162 \t Test loss : 0.029423177242279053\n",
            "Epoch : 1998 \t Train loss : 0.012442376464605331 \t Test loss : 0.029345666989684105\n",
            "Epoch : 1999 \t Train loss : 0.012408358976244926 \t Test loss : 0.029268156737089157\n",
            "Epoch : 2000 \t Train loss : 0.012374338693916798 \t Test loss : 0.02919064834713936\n",
            "Epoch : 2001 \t Train loss : 0.012340323999524117 \t Test loss : 0.029113125056028366\n",
            "Epoch : 2002 \t Train loss : 0.012306300923228264 \t Test loss : 0.029035622254014015\n",
            "Epoch : 2003 \t Train loss : 0.012272296473383904 \t Test loss : 0.028958100825548172\n",
            "Epoch : 2004 \t Train loss : 0.012238291092216969 \t Test loss : 0.028845882043242455\n",
            "Epoch : 2005 \t Train loss : 0.012204318307340145 \t Test loss : 0.028768371790647507\n",
            "Epoch : 2006 \t Train loss : 0.012170301750302315 \t Test loss : 0.02869085595011711\n",
            "Epoch : 2007 \t Train loss : 0.01213628426194191 \t Test loss : 0.028613340109586716\n",
            "Epoch : 2008 \t Train loss : 0.012102266773581505 \t Test loss : 0.028535842895507812\n",
            "Epoch : 2009 \t Train loss : 0.012068256735801697 \t Test loss : 0.028458332642912865\n",
            "Epoch : 2010 \t Train loss : 0.01203423272818327 \t Test loss : 0.02838081121444702\n",
            "Epoch : 2011 \t Train loss : 0.01200021617114544 \t Test loss : 0.028303295373916626\n",
            "Epoch : 2012 \t Train loss : 0.011966202408075333 \t Test loss : 0.028225785121321678\n",
            "Epoch : 2013 \t Train loss : 0.011932187713682652 \t Test loss : 0.028148282319307327\n",
            "Epoch : 2014 \t Train loss : 0.01189817301928997 \t Test loss : 0.02807077206671238\n",
            "Epoch : 2015 \t Train loss : 0.011864153668284416 \t Test loss : 0.027993250638246536\n",
            "Epoch : 2016 \t Train loss : 0.011830171570181847 \t Test loss : 0.027881020680069923\n",
            "Epoch : 2017 \t Train loss : 0.01179618202149868 \t Test loss : 0.02780352160334587\n",
            "Epoch : 2018 \t Train loss : 0.011762170121073723 \t Test loss : 0.02772601880133152\n",
            "Epoch : 2019 \t Train loss : 0.01172814704477787 \t Test loss : 0.027648497372865677\n",
            "Epoch : 2020 \t Train loss : 0.011694135144352913 \t Test loss : 0.027570974081754684\n",
            "Epoch : 2021 \t Train loss : 0.011660119518637657 \t Test loss : 0.027493471279740334\n",
            "Epoch : 2022 \t Train loss : 0.011626103892922401 \t Test loss : 0.027415955439209938\n",
            "Epoch : 2023 \t Train loss : 0.011592088267207146 \t Test loss : 0.027338456362485886\n",
            "Epoch : 2024 \t Train loss : 0.01155807077884674 \t Test loss : 0.027260934934020042\n",
            "Epoch : 2025 \t Train loss : 0.011524050496518612 \t Test loss : 0.027183448895812035\n",
            "Epoch : 2026 \t Train loss : 0.011490041390061378 \t Test loss : 0.02710590325295925\n",
            "Epoch : 2027 \t Train loss : 0.011456016451120377 \t Test loss : 0.0270284004509449\n",
            "Epoch : 2028 \t Train loss : 0.011422056704759598 \t Test loss : 0.026916176080703735\n",
            "Epoch : 2029 \t Train loss : 0.011388048529624939 \t Test loss : 0.026838665828108788\n",
            "Epoch : 2030 \t Train loss : 0.011354032903909683 \t Test loss : 0.026761149987578392\n",
            "Epoch : 2031 \t Train loss : 0.01132001169025898 \t Test loss : 0.026683634147047997\n",
            "Epoch : 2032 \t Train loss : 0.011285999789834023 \t Test loss : 0.026606131345033646\n",
            "Epoch : 2033 \t Train loss : 0.011251985095441341 \t Test loss : 0.026528632268309593\n",
            "Epoch : 2034 \t Train loss : 0.01121796015650034 \t Test loss : 0.026451105251908302\n",
            "Epoch : 2035 \t Train loss : 0.011183948256075382 \t Test loss : 0.026373589411377907\n",
            "Epoch : 2036 \t Train loss : 0.011149933561682701 \t Test loss : 0.026296084746718407\n",
            "Epoch : 2037 \t Train loss : 0.011115915141999722 \t Test loss : 0.026218581944704056\n",
            "Epoch : 2038 \t Train loss : 0.011081898584961891 \t Test loss : 0.026141058653593063\n",
            "Epoch : 2039 \t Train loss : 0.011047890409827232 \t Test loss : 0.026028841733932495\n",
            "Epoch : 2040 \t Train loss : 0.01101392786949873 \t Test loss : 0.025951320305466652\n",
            "Epoch : 2041 \t Train loss : 0.010979910381138325 \t Test loss : 0.025873815640807152\n",
            "Epoch : 2042 \t Train loss : 0.010945901274681091 \t Test loss : 0.02579629421234131\n",
            "Epoch : 2043 \t Train loss : 0.010911877267062664 \t Test loss : 0.02571878395974636\n",
            "Epoch : 2044 \t Train loss : 0.010877858847379684 \t Test loss : 0.02564128115773201\n",
            "Epoch : 2045 \t Train loss : 0.010843845084309578 \t Test loss : 0.025563757866621017\n",
            "Epoch : 2046 \t Train loss : 0.010809825733304024 \t Test loss : 0.025486255064606667\n",
            "Epoch : 2047 \t Train loss : 0.010775813832879066 \t Test loss : 0.02540873922407627\n",
            "Epoch : 2048 \t Train loss : 0.010741797275841236 \t Test loss : 0.025331228971481323\n",
            "Epoch : 2049 \t Train loss : 0.010707775130867958 \t Test loss : 0.025253701955080032\n",
            "Epoch : 2050 \t Train loss : 0.01067376509308815 \t Test loss : 0.025176197290420532\n",
            "Epoch : 2051 \t Train loss : 0.010639769956469536 \t Test loss : 0.025063980370759964\n",
            "Epoch : 2052 \t Train loss : 0.010605789721012115 \t Test loss : 0.02498645707964897\n",
            "Epoch : 2053 \t Train loss : 0.010571775957942009 \t Test loss : 0.02490895427763462\n",
            "Epoch : 2054 \t Train loss : 0.010537763126194477 \t Test loss : 0.02483144961297512\n",
            "Epoch : 2055 \t Train loss : 0.0105037372559309 \t Test loss : 0.024753928184509277\n",
            "Epoch : 2056 \t Train loss : 0.010469727218151093 \t Test loss : 0.02467641793191433\n",
            "Epoch : 2057 \t Train loss : 0.010435708798468113 \t Test loss : 0.024598902091383934\n",
            "Epoch : 2058 \t Train loss : 0.010401701554656029 \t Test loss : 0.02452138066291809\n",
            "Epoch : 2059 \t Train loss : 0.01036767940968275 \t Test loss : 0.024443883448839188\n",
            "Epoch : 2060 \t Train loss : 0.010333660989999771 \t Test loss : 0.02436637319624424\n",
            "Epoch : 2061 \t Train loss : 0.010299643501639366 \t Test loss : 0.024288851767778397\n",
            "Epoch : 2062 \t Train loss : 0.010265626944601536 \t Test loss : 0.024211358278989792\n",
            "Epoch : 2063 \t Train loss : 0.010231654159724712 \t Test loss : 0.024099130183458328\n",
            "Epoch : 2064 \t Train loss : 0.010197656229138374 \t Test loss : 0.024021606892347336\n",
            "Epoch : 2065 \t Train loss : 0.010163637809455395 \t Test loss : 0.02394411526620388\n",
            "Epoch : 2066 \t Train loss : 0.01012960635125637 \t Test loss : 0.02386658266186714\n",
            "Epoch : 2067 \t Train loss : 0.010095606558024883 \t Test loss : 0.02378907799720764\n",
            "Epoch : 2068 \t Train loss : 0.010061590932309628 \t Test loss : 0.023711562156677246\n",
            "Epoch : 2069 \t Train loss : 0.010027576237916946 \t Test loss : 0.0236340519040823\n",
            "Epoch : 2070 \t Train loss : 0.009993557818233967 \t Test loss : 0.023556530475616455\n",
            "Epoch : 2071 \t Train loss : 0.009959539398550987 \t Test loss : 0.023479033261537552\n",
            "Epoch : 2072 \t Train loss : 0.009925523772835732 \t Test loss : 0.023401523008942604\n",
            "Epoch : 2073 \t Train loss : 0.009891502559185028 \t Test loss : 0.02332400716841221\n",
            "Epoch : 2074 \t Train loss : 0.009857497178018093 \t Test loss : 0.02324649691581726\n",
            "Epoch : 2075 \t Train loss : 0.00982353650033474 \t Test loss : 0.0231342613697052\n",
            "Epoch : 2076 \t Train loss : 0.00978951994329691 \t Test loss : 0.023056769743561745\n",
            "Epoch : 2077 \t Train loss : 0.009755506180226803 \t Test loss : 0.0229792483150959\n",
            "Epoch : 2078 \t Train loss : 0.009721484035253525 \t Test loss : 0.022901708260178566\n",
            "Epoch : 2079 \t Train loss : 0.009687469340860844 \t Test loss : 0.02282421663403511\n",
            "Epoch : 2080 \t Train loss : 0.009653453715145588 \t Test loss : 0.02274671196937561\n",
            "Epoch : 2081 \t Train loss : 0.009619438089430332 \t Test loss : 0.022669196128845215\n",
            "Epoch : 2082 \t Train loss : 0.009585417807102203 \t Test loss : 0.022591685876250267\n",
            "Epoch : 2083 \t Train loss : 0.009551400318741798 \t Test loss : 0.022514188662171364\n",
            "Epoch : 2084 \t Train loss : 0.009517387486994267 \t Test loss : 0.022436659783124924\n",
            "Epoch : 2085 \t Train loss : 0.009483376517891884 \t Test loss : 0.022359156981110573\n",
            "Epoch : 2086 \t Train loss : 0.0094493692740798 \t Test loss : 0.022246921434998512\n",
            "Epoch : 2087 \t Train loss : 0.00941540114581585 \t Test loss : 0.022169411182403564\n",
            "Epoch : 2088 \t Train loss : 0.00938138272613287 \t Test loss : 0.022091906517744064\n",
            "Epoch : 2089 \t Train loss : 0.009347368031740189 \t Test loss : 0.02201439067721367\n",
            "Epoch : 2090 \t Train loss : 0.009313346818089485 \t Test loss : 0.021936887875199318\n",
            "Epoch : 2091 \t Train loss : 0.009279332123696804 \t Test loss : 0.021859383210539818\n",
            "Epoch : 2092 \t Train loss : 0.009245323948562145 \t Test loss : 0.021781856194138527\n",
            "Epoch : 2093 \t Train loss : 0.009211303666234016 \t Test loss : 0.02170434035360813\n",
            "Epoch : 2094 \t Train loss : 0.00917728804051876 \t Test loss : 0.02162683568894863\n",
            "Epoch : 2095 \t Train loss : 0.009143268689513206 \t Test loss : 0.021549319848418236\n",
            "Epoch : 2096 \t Train loss : 0.00910925678908825 \t Test loss : 0.021471833810210228\n",
            "Epoch : 2097 \t Train loss : 0.009075242094695568 \t Test loss : 0.02139430120587349\n",
            "Epoch : 2098 \t Train loss : 0.009041251614689827 \t Test loss : 0.021282076835632324\n",
            "Epoch : 2099 \t Train loss : 0.00900726206600666 \t Test loss : 0.021204566583037376\n",
            "Epoch : 2100 \t Train loss : 0.008973246440291405 \t Test loss : 0.02112705074250698\n",
            "Epoch : 2101 \t Train loss : 0.008939234539866447 \t Test loss : 0.021049534901976585\n",
            "Epoch : 2102 \t Train loss : 0.008905211463570595 \t Test loss : 0.020972024649381638\n",
            "Epoch : 2103 \t Train loss : 0.008871197700500488 \t Test loss : 0.020894508808851242\n",
            "Epoch : 2104 \t Train loss : 0.008837184868752956 \t Test loss : 0.020817017182707787\n",
            "Epoch : 2105 \t Train loss : 0.008803170174360275 \t Test loss : 0.020739495754241943\n",
            "Epoch : 2106 \t Train loss : 0.008769148960709572 \t Test loss : 0.0206619743257761\n",
            "Epoch : 2107 \t Train loss : 0.008735135197639465 \t Test loss : 0.020584464073181152\n",
            "Epoch : 2108 \t Train loss : 0.008701116777956486 \t Test loss : 0.020506953820586205\n",
            "Epoch : 2109 \t Train loss : 0.008667098358273506 \t Test loss : 0.02042946219444275\n",
            "Epoch : 2110 \t Train loss : 0.00863313302397728 \t Test loss : 0.02031722106039524\n",
            "Epoch : 2111 \t Train loss : 0.008599129505455494 \t Test loss : 0.020239705219864845\n",
            "Epoch : 2112 \t Train loss : 0.008565112948417664 \t Test loss : 0.020162200555205345\n",
            "Epoch : 2113 \t Train loss : 0.00853110384196043 \t Test loss : 0.020084679126739502\n",
            "Epoch : 2114 \t Train loss : 0.008497078903019428 \t Test loss : 0.020007174462080002\n",
            "Epoch : 2115 \t Train loss : 0.008463061414659023 \t Test loss : 0.019929666072130203\n",
            "Epoch : 2116 \t Train loss : 0.008429046720266342 \t Test loss : 0.01985214278101921\n",
            "Epoch : 2117 \t Train loss : 0.008395027369260788 \t Test loss : 0.01977463997900486\n",
            "Epoch : 2118 \t Train loss : 0.008361012674868107 \t Test loss : 0.019697118550539017\n",
            "Epoch : 2119 \t Train loss : 0.008326997049152851 \t Test loss : 0.019619613885879517\n",
            "Epoch : 2120 \t Train loss : 0.008292979560792446 \t Test loss : 0.01954210363328457\n",
            "Epoch : 2121 \t Train loss : 0.008258972316980362 \t Test loss : 0.019429868087172508\n",
            "Epoch : 2122 \t Train loss : 0.00822500791400671 \t Test loss : 0.019352365285158157\n",
            "Epoch : 2123 \t Train loss : 0.00819099135696888 \t Test loss : 0.019274849444627762\n",
            "Epoch : 2124 \t Train loss : 0.008156975731253624 \t Test loss : 0.019197339192032814\n",
            "Epoch : 2125 \t Train loss : 0.008122965693473816 \t Test loss : 0.019119828939437866\n",
            "Epoch : 2126 \t Train loss : 0.008088940754532814 \t Test loss : 0.01904231309890747\n",
            "Epoch : 2127 \t Train loss : 0.008054929785430431 \t Test loss : 0.018964802846312523\n",
            "Epoch : 2128 \t Train loss : 0.008020913228392601 \t Test loss : 0.018887300044298172\n",
            "Epoch : 2129 \t Train loss : 0.007986897602677345 \t Test loss : 0.018809771165251732\n",
            "Epoch : 2130 \t Train loss : 0.007952881045639515 \t Test loss : 0.01873226836323738\n",
            "Epoch : 2131 \t Train loss : 0.007918858900666237 \t Test loss : 0.018654752522706985\n",
            "Epoch : 2132 \t Train loss : 0.007884846068918705 \t Test loss : 0.018577242270112038\n",
            "Epoch : 2133 \t Train loss : 0.007850851863622665 \t Test loss : 0.018465017899870872\n",
            "Epoch : 2134 \t Train loss : 0.007816873490810394 \t Test loss : 0.01838751509785652\n",
            "Epoch : 2135 \t Train loss : 0.007782858796417713 \t Test loss : 0.018309999257326126\n",
            "Epoch : 2136 \t Train loss : 0.007748845033347607 \t Test loss : 0.018232500180602074\n",
            "Epoch : 2137 \t Train loss : 0.007714811712503433 \t Test loss : 0.01815497875213623\n",
            "Epoch : 2138 \t Train loss : 0.007680806331336498 \t Test loss : 0.018077462911605835\n",
            "Epoch : 2139 \t Train loss : 0.007646795362234116 \t Test loss : 0.01799994707107544\n",
            "Epoch : 2140 \t Train loss : 0.007612778805196285 \t Test loss : 0.01792243681848049\n",
            "Epoch : 2141 \t Train loss : 0.007578761782497168 \t Test loss : 0.017844926565885544\n",
            "Epoch : 2142 \t Train loss : 0.0075447410345077515 \t Test loss : 0.017767418175935745\n",
            "Epoch : 2143 \t Train loss : 0.00751072634011507 \t Test loss : 0.017689894884824753\n",
            "Epoch : 2144 \t Train loss : 0.007476703729480505 \t Test loss : 0.017612392082810402\n",
            "Epoch : 2145 \t Train loss : 0.007442738860845566 \t Test loss : 0.017500167712569237\n",
            "Epoch : 2146 \t Train loss : 0.007408740930259228 \t Test loss : 0.01742265187203884\n",
            "Epoch : 2147 \t Train loss : 0.007374722510576248 \t Test loss : 0.017345141619443893\n",
            "Epoch : 2148 \t Train loss : 0.007340704556554556 \t Test loss : 0.017267625778913498\n",
            "Epoch : 2149 \t Train loss : 0.007306686602532864 \t Test loss : 0.017190109938383102\n",
            "Epoch : 2150 \t Train loss : 0.007272670511156321 \t Test loss : 0.0171126127243042\n",
            "Epoch : 2151 \t Train loss : 0.007238658610731363 \t Test loss : 0.01703510247170925\n",
            "Epoch : 2152 \t Train loss : 0.007204636000096798 \t Test loss : 0.016957581043243408\n",
            "Epoch : 2153 \t Train loss : 0.00717061897739768 \t Test loss : 0.016880065202713013\n",
            "Epoch : 2154 \t Train loss : 0.0071366047486662865 \t Test loss : 0.016802554950118065\n",
            "Epoch : 2155 \t Train loss : 0.007102590054273605 \t Test loss : 0.016725052148103714\n",
            "Epoch : 2156 \t Train loss : 0.0070685758255422115 \t Test loss : 0.016647541895508766\n",
            "Epoch : 2157 \t Train loss : 0.007034613750874996 \t Test loss : 0.016535306349396706\n",
            "Epoch : 2158 \t Train loss : 0.007000601384788752 \t Test loss : 0.01645779050886631\n",
            "Epoch : 2159 \t Train loss : 0.006966585759073496 \t Test loss : 0.016380291432142258\n",
            "Epoch : 2160 \t Train loss : 0.0069325705990195274 \t Test loss : 0.016302788630127907\n",
            "Epoch : 2161 \t Train loss : 0.006898549385368824 \t Test loss : 0.016225267201662064\n",
            "Epoch : 2162 \t Train loss : 0.006864537950605154 \t Test loss : 0.01614774391055107\n",
            "Epoch : 2163 \t Train loss : 0.006830522324889898 \t Test loss : 0.01607024110853672\n",
            "Epoch : 2164 \t Train loss : 0.0067965066991746426 \t Test loss : 0.015992725268006325\n",
            "Epoch : 2165 \t Train loss : 0.00676248874515295 \t Test loss : 0.015915226191282272\n",
            "Epoch : 2166 \t Train loss : 0.006728474982082844 \t Test loss : 0.01583770476281643\n",
            "Epoch : 2167 \t Train loss : 0.006694454699754715 \t Test loss : 0.01576021872460842\n",
            "Epoch : 2168 \t Train loss : 0.006660458631813526 \t Test loss : 0.01564798317849636\n",
            "Epoch : 2169 \t Train loss : 0.006626482121646404 \t Test loss : 0.015570467337965965\n",
            "Epoch : 2170 \t Train loss : 0.006592464633285999 \t Test loss : 0.015492945909500122\n",
            "Epoch : 2171 \t Train loss : 0.006558450870215893 \t Test loss : 0.015415435656905174\n",
            "Epoch : 2172 \t Train loss : 0.006524435244500637 \t Test loss : 0.015337919816374779\n",
            "Epoch : 2173 \t Train loss : 0.006490415893495083 \t Test loss : 0.015260403975844383\n",
            "Epoch : 2174 \t Train loss : 0.006456402130424976 \t Test loss : 0.015182900242507458\n",
            "Epoch : 2175 \t Train loss : 0.0064223879016935825 \t Test loss : 0.01510540209710598\n",
            "Epoch : 2176 \t Train loss : 0.006388363894075155 \t Test loss : 0.015027875080704689\n",
            "Epoch : 2177 \t Train loss : 0.006354350596666336 \t Test loss : 0.014950359240174294\n",
            "Epoch : 2178 \t Train loss : 0.006320336367934942 \t Test loss : 0.014872854575514793\n",
            "Epoch : 2179 \t Train loss : 0.006286317948251963 \t Test loss : 0.014795350842177868\n",
            "Epoch : 2180 \t Train loss : 0.006252334453165531 \t Test loss : 0.01468310970813036\n",
            "Epoch : 2181 \t Train loss : 0.006218344904482365 \t Test loss : 0.014605611562728882\n",
            "Epoch : 2182 \t Train loss : 0.00618432741612196 \t Test loss : 0.014528090134263039\n",
            "Epoch : 2183 \t Train loss : 0.0061503127217292786 \t Test loss : 0.014450585469603539\n",
            "Epoch : 2184 \t Train loss : 0.006116300821304321 \t Test loss : 0.014373064041137695\n",
            "Epoch : 2185 \t Train loss : 0.006082280073314905 \t Test loss : 0.014295553788542747\n",
            "Epoch : 2186 \t Train loss : 0.006048261187970638 \t Test loss : 0.014218050055205822\n",
            "Epoch : 2187 \t Train loss : 0.006014247890561819 \t Test loss : 0.014140528626739979\n",
            "Epoch : 2188 \t Train loss : 0.005980228539556265 \t Test loss : 0.014063024893403053\n",
            "Epoch : 2189 \t Train loss : 0.005946216639131308 \t Test loss : 0.013985509052872658\n",
            "Epoch : 2190 \t Train loss : 0.00591219961643219 \t Test loss : 0.01390799880027771\n",
            "Epoch : 2191 \t Train loss : 0.005878177937120199 \t Test loss : 0.013830470852553844\n",
            "Epoch : 2192 \t Train loss : 0.00584421819075942 \t Test loss : 0.013718253001570702\n",
            "Epoch : 2193 \t Train loss : 0.005810212343931198 \t Test loss : 0.013640749268233776\n",
            "Epoch : 2194 \t Train loss : 0.005776192061603069 \t Test loss : 0.013563227839767933\n",
            "Epoch : 2195 \t Train loss : 0.005742178298532963 \t Test loss : 0.013485724106431007\n",
            "Epoch : 2196 \t Train loss : 0.005708165466785431 \t Test loss : 0.013408219441771507\n",
            "Epoch : 2197 \t Train loss : 0.005674140993505716 \t Test loss : 0.013330698013305664\n",
            "Epoch : 2198 \t Train loss : 0.005640129558742046 \t Test loss : 0.013253187760710716\n",
            "Epoch : 2199 \t Train loss : 0.005606111139059067 \t Test loss : 0.01317567192018032\n",
            "Epoch : 2200 \t Train loss : 0.005572102032601833 \t Test loss : 0.013098150491714478\n",
            "Epoch : 2201 \t Train loss : 0.005538082215934992 \t Test loss : 0.013020652346313\n",
            "Epoch : 2202 \t Train loss : 0.005504063330590725 \t Test loss : 0.012943143025040627\n",
            "Epoch : 2203 \t Train loss : 0.005470050033181906 \t Test loss : 0.012830918654799461\n",
            "Epoch : 2204 \t Train loss : 0.005436087492853403 \t Test loss : 0.012753409333527088\n",
            "Epoch : 2205 \t Train loss : 0.005402071867138147 \t Test loss : 0.01267589908093214\n",
            "Epoch : 2206 \t Train loss : 0.00536805996671319 \t Test loss : 0.012598377652466297\n",
            "Epoch : 2207 \t Train loss : 0.005334042012691498 \t Test loss : 0.012520885095000267\n",
            "Epoch : 2208 \t Train loss : 0.005300006829202175 \t Test loss : 0.012443351559340954\n",
            "Epoch : 2209 \t Train loss : 0.005266009364277124 \t Test loss : 0.012365847826004028\n",
            "Epoch : 2210 \t Train loss : 0.005231992341578007 \t Test loss : 0.012288331985473633\n",
            "Epoch : 2211 \t Train loss : 0.0051979804411530495 \t Test loss : 0.012210821732878685\n",
            "Epoch : 2212 \t Train loss : 0.005163960158824921 \t Test loss : 0.012133300304412842\n",
            "Epoch : 2213 \t Train loss : 0.005129942204803228 \t Test loss : 0.012055802159011364\n",
            "Epoch : 2214 \t Train loss : 0.005095926113426685 \t Test loss : 0.01197829283773899\n",
            "Epoch : 2215 \t Train loss : 0.005061929114162922 \t Test loss : 0.011866068467497826\n",
            "Epoch : 2216 \t Train loss : 0.005027966108173132 \t Test loss : 0.01178855262696743\n",
            "Epoch : 2217 \t Train loss : 0.004993939306586981 \t Test loss : 0.011711031198501587\n",
            "Epoch : 2218 \t Train loss : 0.004959923680871725 \t Test loss : 0.011633539572358131\n",
            "Epoch : 2219 \t Train loss : 0.004925908986479044 \t Test loss : 0.011556017212569714\n",
            "Epoch : 2220 \t Train loss : 0.004891886375844479 \t Test loss : 0.011478478088974953\n",
            "Epoch : 2221 \t Train loss : 0.0048578716814517975 \t Test loss : 0.011400985531508923\n",
            "Epoch : 2222 \t Train loss : 0.004823856987059116 \t Test loss : 0.011323481798171997\n",
            "Epoch : 2223 \t Train loss : 0.004789840430021286 \t Test loss : 0.011245965957641602\n",
            "Epoch : 2224 \t Train loss : 0.004755820147693157 \t Test loss : 0.011168455705046654\n",
            "Epoch : 2225 \t Train loss : 0.0047218031249940395 \t Test loss : 0.01109095849096775\n",
            "Epoch : 2226 \t Train loss : 0.00468778982758522 \t Test loss : 0.011013430543243885\n",
            "Epoch : 2227 \t Train loss : 0.004653820302337408 \t Test loss : 0.010901207104325294\n",
            "Epoch : 2228 \t Train loss : 0.004619821906089783 \t Test loss : 0.010823691263794899\n",
            "Epoch : 2229 \t Train loss : 0.0045858039520680904 \t Test loss : 0.010746181011199951\n",
            "Epoch : 2230 \t Train loss : 0.004551785532385111 \t Test loss : 0.010668677277863026\n",
            "Epoch : 2231 \t Train loss : 0.004517770372331142 \t Test loss : 0.01059116143733263\n",
            "Epoch : 2232 \t Train loss : 0.004483750555664301 \t Test loss : 0.010513657703995705\n",
            "Epoch : 2233 \t Train loss : 0.004449734929949045 \t Test loss : 0.010436153039336205\n",
            "Epoch : 2234 \t Train loss : 0.004415726754814386 \t Test loss : 0.010358626022934914\n",
            "Epoch : 2235 \t Train loss : 0.004381707403808832 \t Test loss : 0.010281110182404518\n",
            "Epoch : 2236 \t Train loss : 0.004347690846771002 \t Test loss : 0.010203605517745018\n",
            "Epoch : 2237 \t Train loss : 0.004313671495765448 \t Test loss : 0.010126089677214622\n",
            "Epoch : 2238 \t Train loss : 0.004279658198356628 \t Test loss : 0.010048603639006615\n",
            "Epoch : 2239 \t Train loss : 0.004245704505592585 \t Test loss : 0.009936362504959106\n",
            "Epoch : 2240 \t Train loss : 0.004211687482893467 \t Test loss : 0.009858846664428711\n",
            "Epoch : 2241 \t Train loss : 0.00417766347527504 \t Test loss : 0.009781336411833763\n",
            "Epoch : 2242 \t Train loss : 0.004143652506172657 \t Test loss : 0.009703820571303368\n",
            "Epoch : 2243 \t Train loss : 0.004109637346118689 \t Test loss : 0.009626304730772972\n",
            "Epoch : 2244 \t Train loss : 0.004075615666806698 \t Test loss : 0.009548795409500599\n",
            "Epoch : 2245 \t Train loss : 0.004041600041091442 \t Test loss : 0.009471279568970203\n",
            "Epoch : 2246 \t Train loss : 0.004007586743682623 \t Test loss : 0.009393787011504173\n",
            "Epoch : 2247 \t Train loss : 0.003973572514951229 \t Test loss : 0.00931626558303833\n",
            "Epoch : 2248 \t Train loss : 0.003939552698284388 \t Test loss : 0.009238744154572487\n",
            "Epoch : 2249 \t Train loss : 0.0039055361412465572 \t Test loss : 0.009161233901977539\n",
            "Epoch : 2250 \t Train loss : 0.0038715326227247715 \t Test loss : 0.009049003943800926\n",
            "Epoch : 2251 \t Train loss : 0.0038375600706785917 \t Test loss : 0.008971500210464\n",
            "Epoch : 2252 \t Train loss : 0.0038035460747778416 \t Test loss : 0.008893990889191628\n",
            "Epoch : 2253 \t Train loss : 0.003769531147554517 \t Test loss : 0.008816475048661232\n",
            "Epoch : 2254 \t Train loss : 0.00373551482334733 \t Test loss : 0.008738970384001732\n",
            "Epoch : 2255 \t Train loss : 0.0037015036214143038 \t Test loss : 0.008661448955535889\n",
            "Epoch : 2256 \t Train loss : 0.003667481942102313 \t Test loss : 0.008583945222198963\n",
            "Epoch : 2257 \t Train loss : 0.0036334642209112644 \t Test loss : 0.008506434969604015\n",
            "Epoch : 2258 \t Train loss : 0.0035994499921798706 \t Test loss : 0.008428913541138172\n",
            "Epoch : 2259 \t Train loss : 0.0035654299426823854 \t Test loss : 0.008351415395736694\n",
            "Epoch : 2260 \t Train loss : 0.0035314150154590607 \t Test loss : 0.008273887448012829\n",
            "Epoch : 2261 \t Train loss : 0.003497400088235736 \t Test loss : 0.008196383714675903\n",
            "Epoch : 2262 \t Train loss : 0.0034634165931493044 \t Test loss : 0.00808415375649929\n",
            "Epoch : 2263 \t Train loss : 0.0034294291399419308 \t Test loss : 0.008006637915968895\n",
            "Epoch : 2264 \t Train loss : 0.0033954097889363766 \t Test loss : 0.00792913418263197\n",
            "Epoch : 2265 \t Train loss : 0.00336139346472919 \t Test loss : 0.007851618342101574\n",
            "Epoch : 2266 \t Train loss : 0.003327378537505865 \t Test loss : 0.0077741085551679134\n",
            "Epoch : 2267 \t Train loss : 0.0032933682668954134 \t Test loss : 0.007696598768234253\n",
            "Epoch : 2268 \t Train loss : 0.003259343560785055 \t Test loss : 0.007619082927703857\n",
            "Epoch : 2269 \t Train loss : 0.003225330263376236 \t Test loss : 0.007541573140770197\n",
            "Epoch : 2270 \t Train loss : 0.003191315336152911 \t Test loss : 0.007464068941771984\n",
            "Epoch : 2271 \t Train loss : 0.003157302038744092 \t Test loss : 0.007386541459709406\n",
            "Epoch : 2272 \t Train loss : 0.0031232833862304688 \t Test loss : 0.007309037260711193\n",
            "Epoch : 2273 \t Train loss : 0.0030892626382410526 \t Test loss : 0.007231521420180798\n",
            "Epoch : 2274 \t Train loss : 0.0030552975367754698 \t Test loss : 0.007119304034858942\n",
            "Epoch : 2275 \t Train loss : 0.003021290060132742 \t Test loss : 0.0070417881943285465\n",
            "Epoch : 2276 \t Train loss : 0.002987275365740061 \t Test loss : 0.006964283995330334\n",
            "Epoch : 2277 \t Train loss : 0.0029532581102102995 \t Test loss : 0.006886768154799938\n",
            "Epoch : 2278 \t Train loss : 0.0029192448128014803 \t Test loss : 0.006809270475059748\n",
            "Epoch : 2279 \t Train loss : 0.002885211957618594 \t Test loss : 0.006731748580932617\n",
            "Epoch : 2280 \t Train loss : 0.002851207507774234 \t Test loss : 0.006654232740402222\n",
            "Epoch : 2281 \t Train loss : 0.002817197237163782 \t Test loss : 0.006576716899871826\n",
            "Epoch : 2282 \t Train loss : 0.0027831816114485264 \t Test loss : 0.006499207112938166\n",
            "Epoch : 2283 \t Train loss : 0.002749162958934903 \t Test loss : 0.006421697326004505\n",
            "Epoch : 2284 \t Train loss : 0.002715142909437418 \t Test loss : 0.006344187073409557\n",
            "Epoch : 2285 \t Train loss : 0.002681134734302759 \t Test loss : 0.006231963634490967\n",
            "Epoch : 2286 \t Train loss : 0.002647171961143613 \t Test loss : 0.006154453847557306\n",
            "Epoch : 2287 \t Train loss : 0.0026131668128073215 \t Test loss : 0.006076938007026911\n",
            "Epoch : 2288 \t Train loss : 0.0025791421066969633 \t Test loss : 0.005999422166496515\n",
            "Epoch : 2289 \t Train loss : 0.0025451243855059147 \t Test loss : 0.005921912379562855\n",
            "Epoch : 2290 \t Train loss : 0.0025111071299761534 \t Test loss : 0.005844396539032459\n",
            "Epoch : 2291 \t Train loss : 0.00247708847746253 \t Test loss : 0.005766880698502064\n",
            "Epoch : 2292 \t Train loss : 0.0024430728517472744 \t Test loss : 0.005689382553100586\n",
            "Epoch : 2293 \t Train loss : 0.002409060951322317 \t Test loss : 0.005611872766166925\n",
            "Epoch : 2294 \t Train loss : 0.0023750378750264645 \t Test loss : 0.005534350872039795\n",
            "Epoch : 2295 \t Train loss : 0.0023410238791257143 \t Test loss : 0.005456835031509399\n",
            "Epoch : 2296 \t Train loss : 0.0023070082534104586 \t Test loss : 0.005379325244575739\n",
            "Epoch : 2297 \t Train loss : 0.002273018006235361 \t Test loss : 0.005267101339995861\n",
            "Epoch : 2298 \t Train loss : 0.0022390373051166534 \t Test loss : 0.0051895915530622005\n",
            "Epoch : 2299 \t Train loss : 0.002205019351094961 \t Test loss : 0.005112075712531805\n",
            "Epoch : 2300 \t Train loss : 0.0021710037253797054 \t Test loss : 0.0050345598720014095\n",
            "Epoch : 2301 \t Train loss : 0.0021369867026805878 \t Test loss : 0.004957062192261219\n",
            "Epoch : 2302 \t Train loss : 0.002102972473949194 \t Test loss : 0.004879557993263006\n",
            "Epoch : 2303 \t Train loss : 0.0020689531229436398 \t Test loss : 0.004802036099135876\n",
            "Epoch : 2304 \t Train loss : 0.0020349412225186825 \t Test loss : 0.0047245146706700325\n",
            "Epoch : 2305 \t Train loss : 0.002000923501327634 \t Test loss : 0.00464701047167182\n",
            "Epoch : 2306 \t Train loss : 0.001966909971088171 \t Test loss : 0.004569494631141424\n",
            "Epoch : 2307 \t Train loss : 0.0019328907364979386 \t Test loss : 0.004491996951401234\n",
            "Epoch : 2308 \t Train loss : 0.0018988758092746139 \t Test loss : 0.004414475057274103\n",
            "Epoch : 2309 \t Train loss : 0.0018648982513695955 \t Test loss : 0.00430225720629096\n",
            "Epoch : 2310 \t Train loss : 0.0018309041624888778 \t Test loss : 0.004224753472954035\n",
            "Epoch : 2311 \t Train loss : 0.0017968856263905764 \t Test loss : 0.004147237632423639\n",
            "Epoch : 2312 \t Train loss : 0.0017628669738769531 \t Test loss : 0.004069715738296509\n",
            "Epoch : 2313 \t Train loss : 0.001728853560052812 \t Test loss : 0.003992205951362848\n",
            "Epoch : 2314 \t Train loss : 0.0016948372358456254 \t Test loss : 0.003914690110832453\n",
            "Epoch : 2315 \t Train loss : 0.0016608185833320022 \t Test loss : 0.0038371800910681486\n",
            "Epoch : 2316 \t Train loss : 0.0016268029576167464 \t Test loss : 0.003759664250537753\n",
            "Epoch : 2317 \t Train loss : 0.0015927873319014907 \t Test loss : 0.003682172391563654\n",
            "Epoch : 2318 \t Train loss : 0.0015587679808959365 \t Test loss : 0.0036046444438397884\n",
            "Epoch : 2319 \t Train loss : 0.0015247538685798645 \t Test loss : 0.003527128603309393\n",
            "Epoch : 2320 \t Train loss : 0.0014907375443726778 \t Test loss : 0.0034496248699724674\n",
            "Epoch : 2321 \t Train loss : 0.0014567800099030137 \t Test loss : 0.0033373951446264982\n",
            "Epoch : 2322 \t Train loss : 0.0014227628707885742 \t Test loss : 0.0032598793040961027\n",
            "Epoch : 2323 \t Train loss : 0.0013887472450733185 \t Test loss : 0.0031823813915252686\n",
            "Epoch : 2324 \t Train loss : 0.0013547309208661318 \t Test loss : 0.003104865550994873\n",
            "Epoch : 2325 \t Train loss : 0.0013207129668444395 \t Test loss : 0.0030273557640612125\n",
            "Epoch : 2326 \t Train loss : 0.0012867033947259188 \t Test loss : 0.002949833869934082\n",
            "Epoch : 2327 \t Train loss : 0.0012526832288131118 \t Test loss : 0.0028723240830004215\n",
            "Epoch : 2328 \t Train loss : 0.0012186638778075576 \t Test loss : 0.0027948201168328524\n",
            "Epoch : 2329 \t Train loss : 0.0011846512788906693 \t Test loss : 0.002717298222705722\n",
            "Epoch : 2330 \t Train loss : 0.0011506311129778624 \t Test loss : 0.0026397942565381527\n",
            "Epoch : 2331 \t Train loss : 0.001116619212552905 \t Test loss : 0.002562278416007757\n",
            "Epoch : 2332 \t Train loss : 0.0010826177895069122 \t Test loss : 0.002450060797855258\n",
            "Epoch : 2333 \t Train loss : 0.0010486438404768705 \t Test loss : 0.002372539136558771\n",
            "Epoch : 2334 \t Train loss : 0.0010146297281607985 \t Test loss : 0.002295029116794467\n",
            "Epoch : 2335 \t Train loss : 0.0009806141024455428 \t Test loss : 0.0022175193298608065\n",
            "Epoch : 2336 \t Train loss : 0.0009465947514399886 \t Test loss : 0.002139997435733676\n",
            "Epoch : 2337 \t Train loss : 0.0009125813958235085 \t Test loss : 0.002062493469566107\n",
            "Epoch : 2338 \t Train loss : 0.0008785679819993675 \t Test loss : 0.0019849897362291813\n",
            "Epoch : 2339 \t Train loss : 0.000844543450511992 \t Test loss : 0.001907473779283464\n",
            "Epoch : 2340 \t Train loss : 0.000810530036687851 \t Test loss : 0.0018299579387530684\n",
            "Epoch : 2341 \t Train loss : 0.0007765136542730033 \t Test loss : 0.001752442098222673\n",
            "Epoch : 2342 \t Train loss : 0.0007425062358379364 \t Test loss : 0.0016749203205108643\n",
            "Epoch : 2343 \t Train loss : 0.0007084846729412675 \t Test loss : 0.00159742240794003\n",
            "Epoch : 2344 \t Train loss : 0.0006744995480403304 \t Test loss : 0.0014851808082312346\n",
            "Epoch : 2345 \t Train loss : 0.0006405093008652329 \t Test loss : 0.0014076888328418136\n",
            "Epoch : 2346 \t Train loss : 0.0006064899498596787 \t Test loss : 0.0013301849830895662\n",
            "Epoch : 2347 \t Train loss : 0.000572475022636354 \t Test loss : 0.0012526691425591707\n",
            "Epoch : 2348 \t Train loss : 0.000538460910320282 \t Test loss : 0.0011751472484320402\n",
            "Epoch : 2349 \t Train loss : 0.0005044445279054344 \t Test loss : 0.0010976552730426192\n",
            "Epoch : 2350 \t Train loss : 0.00047040879144333303 \t Test loss : 0.0010201216209679842\n",
            "Epoch : 2351 \t Train loss : 0.000436408823588863 \t Test loss : 0.000942617654800415\n",
            "Epoch : 2352 \t Train loss : 0.0004023939254693687 \t Test loss : 0.0008651018142700195\n",
            "Epoch : 2353 \t Train loss : 0.0003683827817440033 \t Test loss : 0.0007875919109210372\n",
            "Epoch : 2354 \t Train loss : 0.00033436118974350393 \t Test loss : 0.0007100701332092285\n",
            "Epoch : 2355 \t Train loss : 0.0003003440797328949 \t Test loss : 0.0006325662252493203\n",
            "Epoch : 2356 \t Train loss : 0.0002663806080818176 \t Test loss : 0.0005203544860705733\n",
            "Epoch : 2357 \t Train loss : 0.0002327591209905222 \t Test loss : 0.0005129873752593994\n",
            "Epoch : 2358 \t Train loss : 0.00020056217908859253 \t Test loss : 0.00029798148898407817\n",
            "Epoch : 2359 \t Train loss : 0.00017352774739265442 \t Test loss : 0.0005432128673419356\n",
            "Epoch : 2360 \t Train loss : 0.00020012185268569738 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2361 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2362 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2363 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2364 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2365 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2366 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2367 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2368 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2369 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2370 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2371 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2372 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2373 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2374 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2375 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2376 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2377 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2378 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2379 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2380 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2381 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2382 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2383 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2384 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2385 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2386 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2387 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2388 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2389 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2390 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2391 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2392 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2393 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2394 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2395 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2396 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2397 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2398 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2399 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2400 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2401 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2402 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2403 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2404 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2405 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2406 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2407 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2408 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2409 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2410 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2411 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2412 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2413 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2414 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2415 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2416 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2417 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2418 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2419 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2420 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2421 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2422 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2423 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2424 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2425 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2426 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2427 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2428 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2429 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2430 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2431 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2432 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2433 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2434 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2435 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2436 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2437 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2438 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2439 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2440 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2441 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2442 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2443 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2444 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2445 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2446 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2447 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2448 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2449 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2450 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2451 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2452 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2453 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2454 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2455 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2456 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2457 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2458 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2459 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2460 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2461 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2462 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2463 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2464 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2465 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2466 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2467 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2468 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2469 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2470 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2471 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2472 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2473 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2474 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2475 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2476 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2477 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2478 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2479 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2480 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2481 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2482 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2483 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2484 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2485 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2486 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2487 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2488 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2489 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2490 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2491 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2492 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2493 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2494 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2495 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2496 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2497 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2498 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n",
            "Epoch : 2499 \t Train loss : 0.0005727432435378432 \t Test loss : 0.0009740114328451455\n",
            "Epoch : 2500 \t Train loss : 0.0005912728374823928 \t Test loss : 0.00039252042188309133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14, 9))\n",
        "plt.plot(epoch_counts, train_loss_vals, c=\"y\", label=\"Train loss\")\n",
        "plt.plot(epoch_counts, test_loss_vals, c=\"b\", label=\"Test loss\")\n",
        "plt.legend(prop={\"size\": \"14\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        },
        "id": "lprTrm6dZBD0",
        "outputId": "37d39fa1-fe54-4f11-de20-e6fa008adf5c"
      },
      "execution_count": 3345,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x781d2765b890>"
            ]
          },
          "metadata": {},
          "execution_count": 3345
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x900 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Testing model"
      ],
      "metadata": {
        "id": "hVdPD6kkaBfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "  y_preds = model(X_test)\n",
        "\n",
        "y_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b00fk4YNaEiq",
        "outputId": "4afb92a3-cb3d-4696-9f5b-7a47898f888d"
      },
      "execution_count": 3346,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.1426],\n",
              "        [1.1456],\n",
              "        [1.1486],\n",
              "        [1.1516],\n",
              "        [1.1546],\n",
              "        [1.1576],\n",
              "        [1.1606],\n",
              "        [1.1636],\n",
              "        [1.1666],\n",
              "        [1.1696],\n",
              "        [1.1726],\n",
              "        [1.1756],\n",
              "        [1.1786],\n",
              "        [1.1816],\n",
              "        [1.1846],\n",
              "        [1.1876],\n",
              "        [1.1906],\n",
              "        [1.1936],\n",
              "        [1.1966],\n",
              "        [1.1996]])"
            ]
          },
          "metadata": {},
          "execution_count": 3346
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93epCs0LaLnj",
        "outputId": "11a3456c-c928-4285-cad9-0041acbf7e06"
      },
      "execution_count": 3347,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.1430],\n",
              "        [1.1460],\n",
              "        [1.1490],\n",
              "        [1.1520],\n",
              "        [1.1550],\n",
              "        [1.1580],\n",
              "        [1.1610],\n",
              "        [1.1640],\n",
              "        [1.1670],\n",
              "        [1.1700],\n",
              "        [1.1730],\n",
              "        [1.1760],\n",
              "        [1.1790],\n",
              "        [1.1820],\n",
              "        [1.1850],\n",
              "        [1.1880],\n",
              "        [1.1910],\n",
              "        [1.1940],\n",
              "        [1.1970],\n",
              "        [1.2000]])"
            ]
          },
          "metadata": {},
          "execution_count": 3347
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfHVIFn1aM5k",
        "outputId": "ac07fdb6-5fca-4b06-ea64-8e9c5bb7dfee"
      },
      "execution_count": 3348,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weight', tensor([0.3004])), ('bias', tensor([0.8993]))])"
            ]
          },
          "metadata": {},
          "execution_count": 3348
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_FzLjFbaOWw",
        "outputId": "6f0a760a-4852-4e0a-b7d9-39f5b26b9eb8"
      },
      "execution_count": 3349,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3"
            ]
          },
          "metadata": {},
          "execution_count": 3349
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKLMp_W7aO5h",
        "outputId": "148d8395-93fc-42b9-b200-b24df44e37c7"
      },
      "execution_count": 3350,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9"
            ]
          },
          "metadata": {},
          "execution_count": 3350
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_data(y_preds=y_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "2nDe8M5Mc-gJ",
        "outputId": "aee2d2c7-aac7-4180-dd49-0709485d0c47"
      },
      "execution_count": 3351,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Saving model"
      ],
      "metadata": {
        "id": "KDRO4ie6tpKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH = \"models/linear-reg_ex1_model.pth\"\n",
        "\n",
        "torch.save(model.state_dict(), MODEL_PATH)"
      ],
      "metadata": {
        "id": "uzzj9x0EtrGr"
      },
      "execution_count": 3353,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Load and use the model"
      ],
      "metadata": {
        "id": "CYHQCLVRt-Hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_model = LinearRegressionModel()\n",
        "\n",
        "load_model.load_state_dict(torch.load(MODEL_PATH))\n",
        "\n",
        "load_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2htWUOut_V6",
        "outputId": "f0724b22-2647-4e9a-fe65-f105df5bf407"
      },
      "execution_count": 3354,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegressionModel()"
            ]
          },
          "metadata": {},
          "execution_count": 3354
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_X = torch.tensor([[101.0]], dtype=torch.float32) / 100.0\n",
        "\n",
        "with torch.inference_mode():\n",
        "  y_pred = load_model(new_X)\n",
        "\n",
        "print(f\"Prediction for new_X ({new_X.item()}): {y_pred.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nN2dl_8kuSjL",
        "outputId": "3f8fcaea-acf8-4865-e3fe-44709a59f39d"
      },
      "execution_count": 3357,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for new_X (1.0099999904632568): 1.2026453018188477\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGxS_ZETudmM",
        "outputId": "07a9b512-720f-4aba-bad6-0074b7ec0f2c"
      },
      "execution_count": 3358,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.2026]])"
            ]
          },
          "metadata": {},
          "execution_count": 3358
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2Vdw9hXuk_7",
        "outputId": "1836c922-d523-4c48-a7e0-f59ab75c8d0d"
      },
      "execution_count": 3359,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0100]])"
            ]
          },
          "metadata": {},
          "execution_count": 3359
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(new_X * weight) + bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-cp9tuFulbx",
        "outputId": "e600c1cb-2529-4ce7-d5d3-5eb38280dae0"
      },
      "execution_count": 3360,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.2030]])"
            ]
          },
          "metadata": {},
          "execution_count": 3360
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, raw_X_value: float):\n",
        "  scaled_X = torch.tensor([[raw_X_value]], dtype=torch.float32) / 100.0\n",
        "\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    y_pred_scaled = model(scaled_X)\n",
        "\n",
        "  y_pred_original = (y_pred_scaled - 0.9) * 100 + 0.9\n",
        "\n",
        "  print(f\"Prediction for X = {raw_X_value:.2f} → y = {y_pred_original.item():.4f}\")\n",
        "  return y_pred_original"
      ],
      "metadata": {
        "id": "uSpaF0yCuwmQ"
      },
      "execution_count": 3378,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(load_model, 200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USJaA-pgu4Lh",
        "outputId": "c8ebd914-be59-4854-f051-698a9502ec8f"
      },
      "execution_count": 3379,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for X = 200.00 → y = 60.9002\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[60.9002]])"
            ]
          },
          "metadata": {},
          "execution_count": 3379
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(200 * weight) + bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leYSlpeIu6l6",
        "outputId": "1415699b-07c5-492f-fd7e-1987a571dc6d"
      },
      "execution_count": 3380,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60.9"
            ]
          },
          "metadata": {},
          "execution_count": 3380
        }
      ]
    }
  ]
}